<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/7b0d10d140b8a0a5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/7b0d10d140b8a0a5.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-59c5c889f52620d6.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-f11614d8aa7ee555.js" defer=""></script><script src="/_next/static/chunks/pages/_app-694ee0f2821639fc.js" defer=""></script><script src="/_next/static/chunks/996-eeb5175dbd5dba8f.js" defer=""></script><script src="/_next/static/chunks/36-94b5e24e03efc6db.js" defer=""></script><script src="/_next/static/chunks/pages/%5B...slug%5D-af748dcc13a25fcb.js" defer=""></script><script src="/_next/static/HEkhRyO68Nq4toHtepe11/_buildManifest.js" defer=""></script><script src="/_next/static/HEkhRyO68Nq4toHtepe11/_ssgManifest.js" defer=""></script></head><body><div id="__next"></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"creator":"Bahareh Heravi","title":"How to preserve data journalism","link":"https://datajournalism.com/read/longreads/how-to-save-data-journalism","pubDate":"Mon, 18 Jul 2022 00:00:00 +0200","author":"Bahareh Heravi","content":"\n                                                                        \u003cp\u003eNews organisations have longstanding practices for archiving and preserving their content. The emerging practice of data journalism has led to the creation of complex new outputs, including dynamic data visualisations that rely on distributed digital infrastructures. \u003c/p\u003e\n\u003cp\u003eTraditional news archiving does not yet have systems in place for preserving these outputs, which means that we risk losing this crucial part of reporting and news history. \u003c/p\u003e\n\u003cp\u003eTaking a systematic approach to studying the literature in this area, along with experts in digital archiving preservation, Kathryn Cassidy, Edie Davis, and Natalie Harrower, I studied the implications of the new types of content as the output of data journalism with respect to archiving and preservation of these content, and looked into potential solutions that we could borrow from other more established disciplines such as data and digital archiving, software and game preservation and so on. \u003c/p\u003e\n\u003cp\u003eIn \u003ca href=\"https://www.tandfonline.com/doi/full/10.1080/17512786.2021.1903972\"\u003ea journal paper we published\u003c/a\u003e, we identify the challenges and sticking points in relation to preservation of dynamic interactive visualisations, and provided a set of recommendations for the adoption of long-term preservation of dynamic data visualisations as part of the news publication workflow, as well as identifying concrete actions that data journalists can take immediately to ensure that these visualisations are not lost. Here I take you through some of the problems we identified in our study and the recommendations for preventing further and permanent loss of content.\u003c/p\u003e\n\n                                                                                                \u003cblockquote\u003e\u003cp\u003eEvolving technology threatens preservation of new forms of content in different ways.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n                                                                     \n                            \u003cp\u003eTraditional journalistic outputs were usually published in text and audiovisual format, with news organisations having a longstanding history of archiving and preserving these outputs on various media.\u003c/p\u003e\n\u003cp\u003eThis included paper, tape, or hard disc drives, depending on the historical time period and the original format of the output. Similarly, institutions such as national libraries and archives generally hold large and long standing newspaper archives.\u003c/p\u003e\n\u003cp\u003eData journalism and its enthusiastic uptake in the past decade, however, has opened up a new set of challenges for preservation and demands for new guidelines and practices. The output of data driven journalism still includes traditional text and audiovisual formats, but also it includes data visualisations and/or news applications.  \u003c/p\u003e\n\u003cp\u003eMany of these visual elements rely on digital infrastructures that are not being systematically preserved and sustained as traditional news archiving has not accounted for these dynamic and interactive narratives. \u003c/p\u003e\n\u003cp\u003eThese visualisations communicate key aspects of the story, and without them, in many cases the story is either incomplete, or entirely missing, and so is a part of history. \u003c/p\u003e\n\u003cp\u003eAt the same time, an increasing number of such new, complex outputs are being generated in newsrooms across the world every day, and it is expected that this trend will continue to grow. Without intervention, we will lose a crucial part of reporting and news history.\u003c/p\u003e\n\n                                                                      \n                            \n                                                                      \n                            \u003ch3\u003eWhere is the problem coming from?\u003c/h3\u003e\n\u003cp\u003eData visualisations are one of the core outputs of data journalism. They could be in the form of static image files (e.g. jpeg, gif, png, etc.), but in many cases they are dynamically generated at the time of viewing, by computer code. \u003c/p\u003e\n\u003cp\u003eFor example, many of interactive data visualisations these days are JavaScript based, such as those made using \u003ca href=\"https://d3js.org/\"\u003eD3.js libraries\u003c/a\u003e, or online and/or interactive data visualisation tools that are written on top of JavaScript libraries, such as \u003cstrong\u003eDatawrapper, Flourish, Charticulator, Carto, Mapbox\u003c/strong\u003e and so on. \u003c/p\u003e\n\u003cp\u003eThese data visualisations are hosted on online web servers and possibly outside of the news organisation. If the code behind the visualisation breaks, the server goes offline, or the link between the publication website and the server hosting the visualisation breaks, then the visualisation disappears or renders an error. \u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWe consider any visualisation beyond a simple image to be a dynamic data visualisation.\u003c/strong\u003e As such, all interactive data visualisations are considered dynamic. Such dynamic content cannot be captured by existing tools and methods of archiving, such as tools for archiving web pages or images and videos, and consequently are being lost. \u003c/p\u003e\n\u003cp\u003eDynamic data visualisations are essentially software, and their preservation therefore should include methods suited for software preservation.\u003c/p\u003e\n\u003cp\u003eMy colleagues in the preservation domain consider these dynamic data visualisations as ‘complex digital objects’. \u003c/p\u003e\n\u003cp\u003eThese are distinguished from ‘simple’ or ‘flat’ objects such as image and video files, as they are more challenging to maintain and preserve for long term and sustained access, because they rely on complex digital infrastructures that contain a series of technical (inter)dependencies, where each part of the infrastructure must function in order to deliver the final output. \u003c/p\u003e\n\u003cp\u003eSimple objects are more likely to be maintained long term, because they fall under existing preservation methods used within news organisations since the beginning of the 20th century. \u003c/p\u003e\n\u003cp\u003eIn contrast, the many infrastructures that support ongoing access to dynamic visualisations are not being systematically sustained or preserved in a way that would ensure access to data journalism outputs. \u003c/p\u003e\n\u003cp\u003eIn many cases, the organisation that creates the visualisation, and holds an interest in its preservation (the news organisation), is not usually the same organisation that holds the key to that visualisation’s sustainable accessibility.\u003c/p\u003e\n\n                                                                                                 \u003cblockquote\u003e\u003cp\u003eWithout intervention, we will lose a crucial part of reporting and news history.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n                                                                     \n                            \u003cp\u003eEvolving technology threatens preservation of new forms of content in different ways. Here, I list the four primary factors that we identified in our research to endanger the preservation of data journalism outputs:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eThird-party services:\u003c/strong\u003e Many data visualisations make use of third-party data visualisation tools, such as \u003cstrong\u003eDatawrapper\u003c/strong\u003e and \u003cstrong\u003eFlourish\u003c/strong\u003e, which provide useful and often sophisticated assistance \u003ca href=\"http://bit.ly/BHddjtools\"\u003ein creating visualisations\u003c/a\u003e. \u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHowever, the use of these tools creates risk because of dependencies on the tool provider: the tool may not be maintained by the provider, changes made to their underlying technologies may ‘break’ the connection to published visualisation on a news site, or the service might disappear altogether. \u003c/p\u003e\n\u003cp\u003eThis has already come to pass with the \u003ca href=\"https://blog.silk.co/post/167155630197/its-time-to-say-goodbye\"\u003eshutdown of Silk.co\u003c/a\u003e and \u003ca href=\"https://support.google.com/fusiontables/answer/9185417\"\u003eGoogle Fusion Tables\u003c/a\u003e, both data visualisation services once popular with data journalists. \u003c/p\u003e\n\u003cp\u003eIn the case of Silk.co the website closed on short notice, ceasing access to any data visualisations that had not been exported or migrated by creators prior to the shutdown.\u003c/p\u003e\n\n                                                                                                 \u003cblockquote\u003e\u003cp\u003eDynamic data visualisations are essentially software, and their preservation therefore should include methods suited for software preservation.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n                                                                     \n                            \u003cp\u003eA similar scenario happened a year later in December 2018 when Google announced that they would retire their Fusion Tables service.\u003c/p\u003e\n\u003cp\u003eFusion Tables were one of the tools behind many early examples of Data Journalism, such as the \u003ca href=\"https://www.theguardian.com/news/datablog/2010/oct/23/wikileaks-iraq-data-journalism\"\u003eWikileaks’ Iraq war logs\u003c/a\u003e or the \u003ca href=\"https://www.theguardian.com/news/datablog/interactive/2011/aug/09/uk-riots-incident-map\"\u003eUK Riots in 2011\u003c/a\u003e, published by the Guardian.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eFIGURE 2. Screenshot from October 2021 of \u003ca href=\"https://www.theguardian.com/news/datablog/interactive/2011/aug/09/uk-riots-incident-map\"\u003eThe Guardian story\u003c/a\u003e, depicting how the content gets lost when the third-party services are not maintained.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eFIGURE 1. Screenshots from the Guardian story, depicting how the content gets lost when the third-party services are not maintained: www.theguardian.com/news/datablog/2010/oct/23/wikileaks-iraq-data-journalism. Screenshot taken on 5th August 2020.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eBoth stories were early exemplars of Data Journalism as we know now, and manifested in many talks, tutorials and introductions to Data Journalism, including \u003cstrong\u003eSimon Roger’s TEDx Talk\u003c/strong\u003e on \u003ca href=\"https://www.youtube.com/watch?v=h2zbvmXskSE\"\u003e‘Data-journalists are the new punks’\u003c/a\u003e. I still play the video of his talk in my classes, but none of the maps, the core of these stories, are there.\u003c/p\u003e\n\n                                                                                                                  \n                            \u003cp\u003e\u003cstrong\u003eGoogle Fusion Tables\u003c/strong\u003e was switched off at the end of 2019, and much of the associated content disappeared. The Guardian examples mentioned are only two of many stories with missing visualisations across news organisations in the past number of years.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003e\u003cstrong\u003e2. In-house tools:\u003c/strong\u003e  \u003c/p\u003e\n\u003cp\u003eWhile many workflows rely on third-party apps, some organisations have also designed in-house tools. \u003c/p\u003e\n\u003cp\u003eThese may afford greater control over the tool and its integration with internal technologies, but often these tools have been designed for specific purposes, such as to communicate the data behind a given data-driven piece. \u003c/p\u003e\n\u003cp\u003eThe longer-term use of the tool or its maintenance may not have been considered during the design process, or no strategy has been put in place to track, archive and preserve the output of such tools.\u003c/p\u003e\n\u003cp\u003eAdditionally, these tools are often developed by a small number of (if not one) interested news nerds in the organisation, who may not stay in the same organisation for long, and the continued usage or maintenance may completely vanish with the departure of individual(s) involved.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003e\u003cstrong\u003e3. Content Management Systems:\u003c/strong\u003e \u003c/p\u003e\n\u003cp\u003eThe public-facing website of a news organisation is usually fed by a backend Content Management System (CMS), which itself is regularly maintained, updated, and periodically replaced by new platforms. \u003c/p\u003e\n\u003cp\u003eThrough these changes, the embedding functionality that connects the visualisation to the CMS can be broken or rendered incompatible. In this case, the visualisation and/or the tool remain intact, but the visualisation is not fetched or displayed properly on the news organisation website. \u003c/p\u003e\n\u003cp\u003eFor example iFrames have been one of the common ways to embed data visualisations created with external online tools into stories. An iFrame essentially creates an opening on an HTML page, which can pull content from external websites, including visualisations created in a range of external websites, such as Datawrapper and Flourish, or the above Google Fusion Tables in the Guardian stories. \u003c/p\u003e\n\u003cp\u003eMost online data visualisation tools provide iFrame embed codes, which the journalist can simply copy and paste to their organisational CMS. \u003c/p\u003e\n\u003cp\u003eThe smallest change in the iFrame or embed code management in the CMS could break this link. In such a case, the content remains hosted externally, but the content will not be shown on the publisher website.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003e\u003cstrong\u003e4. Myriad of other technologies:\u003c/strong\u003e \u003c/p\u003e\n\u003cp\u003eWhile the above risks point to significant changes in known aspects of the technology chain, there are other dependencies that underpin visualisations, such as particular programming languages, libraries, databases, hosting platforms and tools. \u003c/p\u003e\n\u003cp\u003eThese change over time – by the news organisation, the tool provider, or globally – and changes can cause the data visualisation itself to no longer be accessible or viewable. \u003c/p\u003e\n\u003cp\u003eAn example of technological change can be seen in the consequences of Adobe’s decision to \u003ca href=\"https://www.adobe.com/products/flashplayer/end-of-life-alternative.html\"\u003eretire Flash\u003c/a\u003e. In countless stories published around and before 2010, such as \u003ca href=\"https://www.theguardian.com/world/interactive/2008/jan/23/earthquakes\"\u003eThe Guardian’s articles on Earthquakes\u003c/a\u003e, or \u003ca href=\"https://www.ig-legacy.ft.com/content/4ce7a094-1c9e-11df-8456-00144feab49a#axzz6murklxLF\"\u003eThe Financial Times’ Banks’ Earnings\u003c/a\u003e, the visualisation itself was the article. \u003c/p\u003e\n\u003cp\u003eSo their disappearance due to the deprecation of Flash resulted in empty pages, with the now-useless suggestions to download or update Flash Player as shown in the images below.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eFIGURE 3.1. Screenshot taken in February 2021 from \u003ca href=\"https://www.theguardian.com/world/interactive/2008/jan/23/earthquakes\"\u003eThe Guardian story\u003c/a\u003e, depicting the disappearance of the full story due to the deprecation of Flash.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eFIGURE 3.2 A screenshot taken in February 2021 from \u003ca href=\"https://www.ig-legacy.ft.com/content/4ce7a094-1c9e-11df-8456-00144feab49a#axzz6murklxLF\"\u003eThe Financial Times\u003c/a\u003e\u003c/p\u003e\n\n                                                                      \n                            \n                                                                      \n                            \u003cp\u003e\u003ca href=\"http://vis.stanford.edu/files/2010-Narrative-InfoVis.pdf\"\u003eA 2010 paper by Edward Segel and Jeffrey Heer\u003c/a\u003e studied 58 visual stories from several publishing houses in their research on narrative visualisation. \u003c/p\u003e\n\u003cp\u003eUnrelated to their findings, I note that most of the visualisations they studied are no longer accessible. It happens that at the time of their research, Flash was the go-to technology for creating interactive visualisations. \u003c/p\u003e\n\u003cp\u003eJust 10 years after this study, Flash Player was deprecated and consequently very few of the visualisations remain accessible. Flash will not be the only casualty, as preferred apps and scripts continue to change over time.\u003c/p\u003e\n\u003cp\u003eIn addition to the large-scale failures, all digital objects, simple or complex, are in danger of degradation or loss over time, due to factors such as data corruption (bit rot) – the obsolescence of file formats, software and hardware – and the limited lifespan of storage media. \u003c/p\u003e\n\u003cp\u003eFor all of these reasons, it is imperative that news media prioritise digital preservation.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eA screenshot image of a message from Adobe explaining support for Flash Player ended in December 2020.\u003c/p\u003e\n\n                                                                      \n                            \u003ch3\u003eHow to tackle these problems\u003c/h3\u003e\n\u003cp\u003eThe findings in our study identified several obstacles, ranging from specific technical challenges to broader social and organisational issues. \u003ca href=\"https://www.tandfonline.com/doi/full/10.1080/17512786.2021.1903972\"\u003eYou can read about the details of it here\u003c/a\u003e. \u003c/p\u003e\n\u003cp\u003eBut in short, two broad approaches emerged from the preservation methods: \u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1) Preservation of visualisations in their original working form\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis approach entails keeping a working version of the visualisation available through methods such as emulation, migration, and virtual machines.\u003c/p\u003e\n\u003cp\u003eAn important category emerged with respect to this approach includes the discussion of specific tools for preservation. The tools used for this purpose mentioned included \u003ca href=\"https://www.reprozip.org/_\"\u003eReproZip\u003c/a\u003e, which is primarily aimed at reproducible scientific research, and provides functionalities for collecting the code, data and server environment used in computational science experiments. \u003c/p\u003e\n\u003cp\u003eOther well-developed tools exist to capture entire webpages or websites. Examples are \u003ca href=\"https://webrecorder.net/\"\u003eWebRecorder\u003c/a\u003e and the \u003ca href=\"http://www.ariadne.ac.uk/issue/52/pope-beresford/\"\u003eInternational Internet Preservation Consortium (IIPC) Toolset\u003c/a\u003e, comprising the Web Curator Tool and the well known and open source Wayback Machine. \u003c/p\u003e\n\u003cp\u003eIn the \u003ca href=\"https://datajournalism.com/read/handbook/two/organising-data-journalism/archiving-data-journalism\"\u003eData Journalism Handbook 2\u003c/a\u003e, \u003cstrong\u003eMeredith Broussard\u003c/strong\u003e proposes that ReproZip could be used in conjunction with \u003ca href=\"https://www.tandfonline.com/doi/abs/10.1080/21670811.2018.1505437\"\u003eBroussard \u0026amp; Boss, 2018's article\u003c/a\u003e, a web archiving and emulation tool for preserving news apps.\u003c/p\u003e\n\u003cp\u003eWhile the web archiving tools may provide a useful starting point for preserving dynamic data visualisations, they are not always able to capture highly interactive data visualisations or those embedded which rely on server-side applications and data, such as those embedded via iFrame or other embedding features. \u003c/p\u003e\n\u003cp\u003eThis is because the code is actually sitting somewhere outside of the current webpage. Furthermore, capturing the web through this method (which creates Web Archiving – WARC files) is difficult and complex and not likely to be implemented as part of journalistic workflows. \u003c/p\u003e\n\u003cp\u003eAdditionally, there are a variety of other preservation, workflow management and configuration management tools according to articles by \u003ca href=\"https://www.researchgate.net/publication/325407286_Using_ReproZip_for_Reproducibility_and_Library_Services\"\u003eChirigati et al., 2016; Steeves et al., 2017\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWhile the existing approaches towards keeping a working version of the visualisation in its original form through available web and software archiving, emulation, migration, and virtual machines are not specifically aimed at archiving dynamic data visualisations, have mixed results when capturing interactive content, and are complex and expensive to implement and maintain, they could shed a light on tools necessary for archiving data visualisation.\u003c/p\u003e\n\u003cp\u003eThey could also provide valuable directions for future preservation of dynamic and interactive data visualisations in data journalism.\u003c/p\u003e\n\n                                                                                                 \u003cblockquote\u003e\u003cp\u003eThe user interaction and experience may be key to the meaning and value of a given data visualisation.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n                                                                     \n                            \u003cp\u003e\u003cstrong\u003e2) Flattening the visual\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe second approach attempts to capture a “flat” or simplified version of the visualisation via methods such as snapshots, documentation, and metadata. \u003c/p\u003e\n\u003cp\u003eA flat or simplified version, considered in digital preservation language under the category of ‘surrogates’, essentially turns dynamic visualisations from complex digital objects into simple objects, such as images, GIF animations or videos, which are more easily preserved. \u003c/p\u003e\n\u003cp\u003eThe dynamism is not maintained, but an effort is made to capture a sense of the original visualisation to preserve at least some part of it from total loss.\u003c/p\u003e\n\n                                                                      \n                            \n                                                                      \n                            \u003ch3\u003eHow to choose? Significant Properties\u003c/h3\u003e\n\u003cp\u003eIn choosing which of these approaches is most suitable for a given dynamic data visualisation or a given story in news and journalism, in our paper we draw on the concept of \u003cstrong\u003e‘Significant Properties’\u003c/strong\u003e of digital objects, originally proposed by \u003ca href=\"https://ils.unc.edu/callee/sigprops_dlm2002.pdf\"\u003eMargaret Hedstrom \u0026amp; Christopher A. Lee\u003c/a\u003e in 2002 as their response to archiving of digital items in relation to their original physical object, such as a physical book being archived in digital format (on microfilm!), or when digital objects were converted from one format to another. \u003c/p\u003e\n\u003cp\u003eThe idea was that the digitised version of a book, for example, may not be capable of preserving all of the properties of the original hard copy materials, such as accurate colour representation or the exact physical dimensions of the originals. \u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSignificant Properties\u003c/strong\u003e are those properties of digital objects that affect their \u003cstrong\u003equality, usability, rendering,\u003c/strong\u003e and \u003cstrong\u003ebehaviour\u003c/strong\u003e. These are typically technical or behavioural characteristics of the digital objects, which need to remain unchanged when the file is accessed in the future, in order for the file to fulfil its original purpose. \u003c/p\u003e\n\u003cp\u003eIn the case of image files this might include aspects such as the height, width and colour depth of the image, while for video content it could include aspects such as the playback length and frame rate.\u003c/p\u003e\n\n                                                                                                 \u003cblockquote\u003e\u003cp\u003eIt may not be necessary to preserve the entire interactive data visualisation in a working form.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n                                                                     \n                            \u003cp\u003eSoftware and other interactive digital objects tend to have more complicated significant properties relating to their behaviour and the types of possible user interaction. \u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eComputer games\u003c/strong\u003e, for example, are inherently experiential: \u003cstrong\u003ethe experience of the game is a significant property of the application\u003c/strong\u003e. This can also be the case with data visualisations. The user interaction and experience may be key to the meaning and \u003cstrong\u003evalue of a given data visualisation\u003c/strong\u003e. \u003c/p\u003e\n\u003cp\u003eOn the other end of the spectrum, interactivity may not provide vital value to the visualisation, rather the information conveyed through different interactive elements may be considered the significant properties. Or it could be somewhere in the middle.\u003c/p\u003e\n\u003cp\u003eThe first step here therefore for us would be to identify these significant properties, put next to the time and resources available, and go forward in relation to our preservation methods accordingly.\u003c/p\u003e\n\u003cp\u003eWhere interactivity is a Significant Property, an approach using techniques such as emulation or migration may be indicated, as this preserves a working version of the original visualisation, and is thus more likely to preserve all of the significant properties of the object.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eThis approach would be in line with recent recommendations by the \u003ca href=\"https://www.dpconline.org/docs/technology-watch-reports/2312-preserving-software-motivations-challenges-and-approaches/file\"\u003eDigital Preservation Coalition on preserving Software\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOn the other hand, for some interactive data visualisations, dynamism and interactivity are not significant properties of the object, and much of the message is communicated without these aspects. \u003c/p\u003e\n\u003cp\u003eIn such cases, it may not be necessary to preserve the entire interactive data visualisation in a working form, as an approach using snapshots and documentation as surrogates for the original may satisfactorily retain the significant properties. \t\u003c/p\u003e\n\u003cp\u003eIdentifying whether and to what extent these are significant properties of a visualisation can help in selecting which approach to take in its preservation. But these must be considered alongside other \u003cstrong\u003eresource and workflow requirements\u003c/strong\u003e and \u003cstrong\u003elimitations for preservation\u003c/strong\u003e.\u003c/p\u003e\n\n                                                                      \n                            \n                                                                      \n                            \u003ch3\u003eNon-technical challenges\u003c/h3\u003e\n\u003cp\u003eRegardless of the technical approach taken to preservation, several systemic methods could be drawn on from recognised topics in the digital preservation domain. Overall, our research indicates that the complexity of the task of preservation is the biggest obstacle to preserving these objects.This complexity is not limited to technical aspects. \u003c/p\u003e\n\u003cp\u003eRather, it is in part attributable to the wider cultural or organisational challenge of digital preservation, where resources - financial and human - are limited, preservation is not embedded in publication workflows, and advocates for preservation are few and far between. \u003c/p\u003e\n\u003cp\u003eFurthermore, the responsibility for these actions must be \u003cstrong\u003eidentified\u003c/strong\u003e and \u003cstrong\u003epursued systematically\u003c/strong\u003e. Awareness-building around preservation, guidelines for preserving visualisations, and training on how to integrate preservation into workflows can assist with these larger social or organisational challenges.\u003c/p\u003e\n\n                                                                      \n                            \n                                                                      \n                            \u003ch3\u003eRecommendations for going forward\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eRecommendations for Immediate and Practical Interventions by Data Journalists\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eHere I start with a set of immediate and simple actions that could be taken by data journalists to ensure partial preservation of the content they are producing now, in lieu of more robust approaches to be developed and implemented widely in the future.\u003c/p\u003e\n\u003cp\u003eThese are approaches that assume limited time and resources combining a basic identification of significant properties, along with the creation of surrogate output of types that are easily preservable using current technologies, such as images and audiovisual formats. \u003c/p\u003e\n\u003cp\u003eThis is essentially a basic form of the snapshot method identified in the literature.\u003c/p\u003e\n\n                                                                                                 \u003cblockquote\u003e\u003cp\u003eIf a number of screen grabs in the form of a GIF animation cannot do justice to the visualisation, then consider creating a video cast of the data visualisation in use.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n                                                                     \n                            \u003cp\u003eWe propose that for every dynamic data visualisation included in a story, the journalist should:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003eIdentify the significant properties of the data visualisation, in terms of the importance of the story at hand. \u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eIf an image screenshot of the data visualisation could represent these properties to a satisfactory degree, then take a screenshot of the visualisation, and store it with other archived audiovisual content.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eScreenshots have been used by some news organisations in their archiving practices. Figure 4.1 and 4.2 depicts two examples from The Washington Post and the New York Times, where the story is missing due to the issue of Flash, but the organisations offer access to alternate archived content.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eFIGURE 4.1. Screenshots taken in February 2021 from \u003ca href=\"www.washingtonpost.com/wp-srv/politics/pioneers/pioneers_spheres.html\"\u003eThe Washington Post\u003c/a\u003e depicting the disappearance of the stories due to the deprecation of Flash, as well as the accessible screenshots through their archives.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eFIGURE 4.2. Screenshots taken in February 2021 from \u003ca href=\"https://www.archive.nytimes.com/www.nytimes.com/interactive/2010/02/20/sports/olympics/downhill-overview.html\"\u003eThe New York Times\u003c/a\u003e, showing the inability to read a story thanks to its reliance on Flash.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eFollowing the link in \u003ca href=\"www.washingtonpost.com/wp-srv/politics/pioneers/pioneers_spheres.html\"\u003eThe Washington Post story\u003c/a\u003e retrieves a PDF, which had been previously generated for the print version of the story. \u003c/p\u003e\n\u003cp\u003eClearly, this conveys an acceptable degree of the original story’s intention. However, the link in \u003ca href=\"https://archive.nytimes.com/www.nytimes.com/interactive/2010/02/20/sports/olympics/downhill-overview.html\"\u003eThe New York Times story\u003c/a\u003e retrieves a \u003ca href=\"https://archive.nytimes.com/screenshots/www.nytimes.com/interactive/2010/02/20/sports/olympics/downhill-overview.jpg\"\u003escreenshot\u003c/a\u003e that only shows the first slide of a multi-slide story, which means a significant part is missing. \u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIf an image screen grab cannot capture the story to a satisfactory level, then we propose two alternatives:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003ea. If a small number of screen grabs can tell the story, then create a GIF animation that includes these in sequence, and archive as above. GIF animations allow limited animation but are nonetheless relatively simple image files which are straightforward to preserve. \u003c/p\u003e\n\u003cp\u003eMany news organisations already create animated GIFs for content promotion on social media and so the tools and expertise are readily available. \u003c/p\u003e\n\u003cp\u003eThe Economist data desk, for example, provided a workshop on From interactive to social media: how to promote data journalism at the 2018 edition of the European Data \u0026amp; Computational Journalism Conference, for which they create GIF animations to promote their interactive data visualisations on social media. \u003c/p\u003e\n\u003cp\u003eThese GIF animations, in essence, capture some part of the significant properties of the original interactive data visualisation.\u003c/p\u003e\n\u003cp\u003eb. If a number of screen grabs in the form of a GIF animation cannot do justice to the visualisation, then consider creating a video cast of the data visualisation in use, highlighting the most important parts. A range of widely-available free tools can be used to create such video content which is also relatively simple to preserve.\u003c/p\u003e\n\u003cp\u003eThese simple surrogate representations must also be linked to the original story to ensure that the reader can find them if the story remains available, but the original visualisation is no longer available.\u003c/p\u003e\n\u003cp\u003eThis linking could be via a structural solution whereby the CMS of the news organisation allows an alternate link to be specified and automatically displays the file behind the link if the main visualisation fails to load. \u003c/p\u003e\n\u003cp\u003eAn alternative or possible interim solution would be to include a link under each visualisation to the surrogate version which invites the user to click on it if the visualisation does not display correctly. An example of how this has worked in practice could be seen in Figure \u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCreating an image, GIF animation or a video of your data visualisation is an uncomplicated solution that enables the capture of significant properties in terms of content as story, providing a stop-gap until more systematic and sophisticated methods for preservation of dynamic data visualisations are in place. In addition to long-term preservation and access, this simple method could also cater for issues associated with loading complex objects across devices.\u003c/li\u003e\n\u003c/ol\u003e\n\n                                                                                                 \u003cblockquote\u003e\u003cp\u003eThese recommendations address the need for an organised and sustainable approach to the long-term digital preservation of data visualisations.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n                                                                     \n                            \u003cp\u003eAs such, we also propose that every provider of data visualisation creation tools should ideally provide GIF animation and video exports, in addition to their current visualisation exports. \u003c/p\u003e\n\u003cp\u003eMany data visualisation providers promise their users that in the case of company closure, users will be given the option to download the code behind the charts. \u003c/p\u003e\n\u003cp\u003eThis is a responsible offer, but most journalists will not have the time or skills to execute that code on a different platform. Nor will they be able to go back to every single story they created to update the server information for where the data visualisation is hosted. \u003c/p\u003e\n\u003cp\u003eHence, it is advisable that journalists create simple exports of their data visualisations at time of publication, and provide the information for how these can be accessed if the original publication fails. \u003c/p\u003e\n\u003cp\u003eBoth data journalists, and the wider digital preservation community, should \u003cstrong\u003eadvocate with vendors\u003c/strong\u003e of these tools to help bring this about.\u003c/p\u003e\n\n                                                                                                 \u003cblockquote\u003e\u003cp\u003eThe preservation of the datasets that underlie data visualisations is also key.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n                                                                     \n                            \u003cp\u003eThese immediate and relatively contained measures could ensure that much of the data journalism currently being produced is not lost entirely, while the newsrooms find ways to implement the recommendations to ensure longer term systematic preservation of such complex objects. \u003c/p\u003e\n\u003cp\u003eIn addition to these, in the paper, my colleagues and I provide a set of recommendations for systematic and more long term interventions. These recommendations draw on the systematic study of the literature in a set of relevant areas such as \u003cstrong\u003eweb archiving, digital preservation, software and game archiving, methods detailed in professional literature\u003c/strong\u003e from the fields of data journalism and digital preservation, as well as our professional expertise as academics and practitioners in these areas. \u003c/p\u003e\n\u003cp\u003eOur recommendations for systematic, organisation and discipline based interventions fall into several categories, including guidance and education, infrastructure and tools, collaboration with trusted, local and national digital repositories and memory institutions, funding and resourcing, and legal frameworks. \u003c/p\u003e\n\u003cp\u003eThese recommendations for long term and systematic interventions address the need for an organised and sustainable approach to the long-term digital preservation of data visualisations. \u003c/p\u003e\n\u003cp\u003eThey aim to ensure that these increasingly important elements of journalistic output are routinely preserved alongside simpler forms of digital news media. \u003c/p\u003e\n\u003cp\u003eThese medium to long-term actions require \u003cstrong\u003echanges to workflows\u003c/strong\u003e and \u003cstrong\u003einvestment into new policies, practices and technical solutions\u003c/strong\u003e. As such, they require an investment of significant effort over time, financial resources, and collaborations that may expand the remit of existing institutions. \u003c/p\u003e\n\u003cp\u003eIf you are interested to read more about these, read the Recommendations part of the paper.\u003c/p\u003e\n\n                                                                                                 \u003cblockquote\u003e\u003cp\u003eDigital preservation is an ongoing process, not simply an endpoint.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n                                                                     \n                            \u003cp\u003eI would like to note here that the scope of this article, and the research paper underlying it, includes works relating to the preservation of dynamic data visualisation and associated software code and dynamic digital objects. \u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe preservation of the datasets that underlie data visualisations is also key\u003c/strong\u003e; in some cases, they are required to make the visualisation function as it is rendered. In any case, the data should be persistently accessible to verify the findings communicated by the visualisation. However, this is a separate, larger issue for digital preservation and is out of the scope of this article. \u003c/p\u003e\n\u003cp\u003eAs a pointer and food for thought, the preservation of research data is being studied by international initiatives such as the \u003ca href=\"https://www.rd-alliance.org/\"\u003eResearch Data Alliance\u003c/a\u003e and the \u003ca href=\"https://codata.org/\"\u003eCODATA\u003c/a\u003e committee of the International Science Council, which could provide valuable input into the preservation of data and code when it comes to data journalism.\u003c/p\u003e\n\u003cp\u003eDigital preservation is an ongoing process, not simply an endpoint. Methods must evolve within and by the communities that are most invested in the \u003cstrong\u003elong-term stewardship of their outputs\u003c/strong\u003e. \u003c/p\u003e\n\u003cp\u003eBecause of journalism’s fundamental and unique contribution to the historical record, it is imperative that preservation is built into the production of data journalism, so that this key element of the record is not lost.\u003c/p\u003e\n\n                                                                      \n                            \n                                                                                                                                 \u003cp\u003eBahareh Heravi is a Data and Computational Journalism researcher, trainer, practitioner and innovator. She is currently a Reader in AI and Media at the Institute for People-Centred AI at the University of Surrey in the UK.  Bahareh is a member of the Irish Open Data Governance Board, and a co-founder and co-chair of the European Data \u0026amp; Computational Journalism Conference. She previously was an Assistant Professor at the School of Information and Communication Studies at UCD, where she led the Data Journalism programme.\u003c/p\u003e\n\n                                                                                                                              \n                            \u003cp\u003eThis article is a shortened and adapted version of an academic paper that Bahareh Heravi co-authored with her colleagues \u003ca href=\"https://www.dri.ie/core-implementation-team/kathryn-cassidy\"\u003eKathryn Cassidy\u003c/a\u003e and \u003ca href=\"https://www.dri.ie/core-implementation-team/natalie-harrower\"\u003eNatalie Harrower\u003c/a\u003e from the \u003ca href=\"https://dri.ie/\"\u003eDigital Repository of Ireland\u003c/a\u003e, and \u003ca href=\"https://libguides.tcd.ie/prf.php?account_id=141278\"\u003eEdie Davis\u003c/a\u003e from the \u003ca href=\"https://www.tcd.ie/library/\"\u003eLibrary of the Trinity College Dublin\u003c/a\u003e. \u003c/p\u003e\n\u003cp\u003eFor the full journal paper, and also for citation and referencing please visit \u003ca href=\"https://www.tandfonline.com/doi/full/10.1080/17512786.2021.1903972\"\u003ethe journal website\u003c/a\u003e.\u003c/p\u003e\n\n                                              \n                ","contentSnippet":"News organisations have longstanding practices for archiving and preserving their content. The emerging practice of data journalism has led to the creation of complex new outputs, including dynamic data visualisations that rely on distributed digital infrastructures. \nTraditional news archiving does not yet have systems in place for preserving these outputs, which means that we risk losing this crucial part of reporting and news history. \nTaking a systematic approach to studying the literature in this area, along with experts in digital archiving preservation, Kathryn Cassidy, Edie Davis, and Natalie Harrower, I studied the implications of the new types of content as the output of data journalism with respect to archiving and preservation of these content, and looked into potential solutions that we could borrow from other more established disciplines such as data and digital archiving, software and game preservation and so on. \nIn a journal paper we published, we identify the challenges and sticking points in relation to preservation of dynamic interactive visualisations, and provided a set of recommendations for the adoption of long-term preservation of dynamic data visualisations as part of the news publication workflow, as well as identifying concrete actions that data journalists can take immediately to ensure that these visualisations are not lost. Here I take you through some of the problems we identified in our study and the recommendations for preventing further and permanent loss of content.\nEvolving technology threatens preservation of new forms of content in different ways.\nTraditional journalistic outputs were usually published in text and audiovisual format, with news organisations having a longstanding history of archiving and preserving these outputs on various media.\nThis included paper, tape, or hard disc drives, depending on the historical time period and the original format of the output. Similarly, institutions such as national libraries and archives generally hold large and long standing newspaper archives.\nData journalism and its enthusiastic uptake in the past decade, however, has opened up a new set of challenges for preservation and demands for new guidelines and practices. The output of data driven journalism still includes traditional text and audiovisual formats, but also it includes data visualisations and/or news applications.  \nMany of these visual elements rely on digital infrastructures that are not being systematically preserved and sustained as traditional news archiving has not accounted for these dynamic and interactive narratives. \nThese visualisations communicate key aspects of the story, and without them, in many cases the story is either incomplete, or entirely missing, and so is a part of history. \nAt the same time, an increasing number of such new, complex outputs are being generated in newsrooms across the world every day, and it is expected that this trend will continue to grow. Without intervention, we will lose a crucial part of reporting and news history.\nWhere is the problem coming from?\nData visualisations are one of the core outputs of data journalism. They could be in the form of static image files (e.g. jpeg, gif, png, etc.), but in many cases they are dynamically generated at the time of viewing, by computer code. \nFor example, many of interactive data visualisations these days are JavaScript based, such as those made using D3.js libraries, or online and/or interactive data visualisation tools that are written on top of JavaScript libraries, such as Datawrapper, Flourish, Charticulator, Carto, Mapbox and so on. \nThese data visualisations are hosted on online web servers and possibly outside of the news organisation. If the code behind the visualisation breaks, the server goes offline, or the link between the publication website and the server hosting the visualisation breaks, then the visualisation disappears or renders an error. \nWe consider any visualisation beyond a simple image to be a dynamic data visualisation. As such, all interactive data visualisations are considered dynamic. Such dynamic content cannot be captured by existing tools and methods of archiving, such as tools for archiving web pages or images and videos, and consequently are being lost. \nDynamic data visualisations are essentially software, and their preservation therefore should include methods suited for software preservation.\nMy colleagues in the preservation domain consider these dynamic data visualisations as ‘complex digital objects’. \nThese are distinguished from ‘simple’ or ‘flat’ objects such as image and video files, as they are more challenging to maintain and preserve for long term and sustained access, because they rely on complex digital infrastructures that contain a series of technical (inter)dependencies, where each part of the infrastructure must function in order to deliver the final output. \nSimple objects are more likely to be maintained long term, because they fall under existing preservation methods used within news organisations since the beginning of the 20th century. \nIn contrast, the many infrastructures that support ongoing access to dynamic visualisations are not being systematically sustained or preserved in a way that would ensure access to data journalism outputs. \nIn many cases, the organisation that creates the visualisation, and holds an interest in its preservation (the news organisation), is not usually the same organisation that holds the key to that visualisation’s sustainable accessibility.\nWithout intervention, we will lose a crucial part of reporting and news history.\nEvolving technology threatens preservation of new forms of content in different ways. Here, I list the four primary factors that we identified in our research to endanger the preservation of data journalism outputs:\nThird-party services: Many data visualisations make use of third-party data visualisation tools, such as Datawrapper and Flourish, which provide useful and often sophisticated assistance in creating visualisations. \nHowever, the use of these tools creates risk because of dependencies on the tool provider: the tool may not be maintained by the provider, changes made to their underlying technologies may ‘break’ the connection to published visualisation on a news site, or the service might disappear altogether. \nThis has already come to pass with the shutdown of Silk.co and Google Fusion Tables, both data visualisation services once popular with data journalists. \nIn the case of Silk.co the website closed on short notice, ceasing access to any data visualisations that had not been exported or migrated by creators prior to the shutdown.\nDynamic data visualisations are essentially software, and their preservation therefore should include methods suited for software preservation.\nA similar scenario happened a year later in December 2018 when Google announced that they would retire their Fusion Tables service.\nFusion Tables were one of the tools behind many early examples of Data Journalism, such as the Wikileaks’ Iraq war logs or the UK Riots in 2011, published by the Guardian.\nFIGURE 2. Screenshot from October 2021 of The Guardian story, depicting how the content gets lost when the third-party services are not maintained.\nFIGURE 1. Screenshots from the Guardian story, depicting how the content gets lost when the third-party services are not maintained: www.theguardian.com/news/datablog/2010/oct/23/wikileaks-iraq-data-journalism. Screenshot taken on 5th August 2020.\nBoth stories were early exemplars of Data Journalism as we know now, and manifested in many talks, tutorials and introductions to Data Journalism, including Simon Roger’s TEDx Talk on ‘Data-journalists are the new punks’. I still play the video of his talk in my classes, but none of the maps, the core of these stories, are there.\nGoogle Fusion Tables was switched off at the end of 2019, and much of the associated content disappeared. The Guardian examples mentioned are only two of many stories with missing visualisations across news organisations in the past number of years.\n2. In-house tools:  \nWhile many workflows rely on third-party apps, some organisations have also designed in-house tools. \nThese may afford greater control over the tool and its integration with internal technologies, but often these tools have been designed for specific purposes, such as to communicate the data behind a given data-driven piece. \nThe longer-term use of the tool or its maintenance may not have been considered during the design process, or no strategy has been put in place to track, archive and preserve the output of such tools.\nAdditionally, these tools are often developed by a small number of (if not one) interested news nerds in the organisation, who may not stay in the same organisation for long, and the continued usage or maintenance may completely vanish with the departure of individual(s) involved.\n3. Content Management Systems: \nThe public-facing website of a news organisation is usually fed by a backend Content Management System (CMS), which itself is regularly maintained, updated, and periodically replaced by new platforms. \nThrough these changes, the embedding functionality that connects the visualisation to the CMS can be broken or rendered incompatible. In this case, the visualisation and/or the tool remain intact, but the visualisation is not fetched or displayed properly on the news organisation website. \nFor example iFrames have been one of the common ways to embed data visualisations created with external online tools into stories. An iFrame essentially creates an opening on an HTML page, which can pull content from external websites, including visualisations created in a range of external websites, such as Datawrapper and Flourish, or the above Google Fusion Tables in the Guardian stories. \nMost online data visualisation tools provide iFrame embed codes, which the journalist can simply copy and paste to their organisational CMS. \nThe smallest change in the iFrame or embed code management in the CMS could break this link. In such a case, the content remains hosted externally, but the content will not be shown on the publisher website.\n4. Myriad of other technologies: \nWhile the above risks point to significant changes in known aspects of the technology chain, there are other dependencies that underpin visualisations, such as particular programming languages, libraries, databases, hosting platforms and tools. \nThese change over time – by the news organisation, the tool provider, or globally – and changes can cause the data visualisation itself to no longer be accessible or viewable. \nAn example of technological change can be seen in the consequences of Adobe’s decision to retire Flash. In countless stories published around and before 2010, such as The Guardian’s articles on Earthquakes, or The Financial Times’ Banks’ Earnings, the visualisation itself was the article. \nSo their disappearance due to the deprecation of Flash resulted in empty pages, with the now-useless suggestions to download or update Flash Player as shown in the images below.\nFIGURE 3.1. Screenshot taken in February 2021 from The Guardian story, depicting the disappearance of the full story due to the deprecation of Flash.\nFIGURE 3.2 A screenshot taken in February 2021 from The Financial Times\nA 2010 paper by Edward Segel and Jeffrey Heer studied 58 visual stories from several publishing houses in their research on narrative visualisation. \nUnrelated to their findings, I note that most of the visualisations they studied are no longer accessible. It happens that at the time of their research, Flash was the go-to technology for creating interactive visualisations. \nJust 10 years after this study, Flash Player was deprecated and consequently very few of the visualisations remain accessible. Flash will not be the only casualty, as preferred apps and scripts continue to change over time.\nIn addition to the large-scale failures, all digital objects, simple or complex, are in danger of degradation or loss over time, due to factors such as data corruption (bit rot) – the obsolescence of file formats, software and hardware – and the limited lifespan of storage media. \nFor all of these reasons, it is imperative that news media prioritise digital preservation.\nA screenshot image of a message from Adobe explaining support for Flash Player ended in December 2020.\nHow to tackle these problems\nThe findings in our study identified several obstacles, ranging from specific technical challenges to broader social and organisational issues. You can read about the details of it here. \nBut in short, two broad approaches emerged from the preservation methods: \n1) Preservation of visualisations in their original working form\nThis approach entails keeping a working version of the visualisation available through methods such as emulation, migration, and virtual machines.\nAn important category emerged with respect to this approach includes the discussion of specific tools for preservation. The tools used for this purpose mentioned included ReproZip, which is primarily aimed at reproducible scientific research, and provides functionalities for collecting the code, data and server environment used in computational science experiments. \nOther well-developed tools exist to capture entire webpages or websites. Examples are WebRecorder and the International Internet Preservation Consortium (IIPC) Toolset, comprising the Web Curator Tool and the well known and open source Wayback Machine. \nIn the Data Journalism Handbook 2, Meredith Broussard proposes that ReproZip could be used in conjunction with Broussard \u0026 Boss, 2018's article, a web archiving and emulation tool for preserving news apps.\nWhile the web archiving tools may provide a useful starting point for preserving dynamic data visualisations, they are not always able to capture highly interactive data visualisations or those embedded which rely on server-side applications and data, such as those embedded via iFrame or other embedding features. \nThis is because the code is actually sitting somewhere outside of the current webpage. Furthermore, capturing the web through this method (which creates Web Archiving – WARC files) is difficult and complex and not likely to be implemented as part of journalistic workflows. \nAdditionally, there are a variety of other preservation, workflow management and configuration management tools according to articles by Chirigati et al., 2016; Steeves et al., 2017.\nWhile the existing approaches towards keeping a working version of the visualisation in its original form through available web and software archiving, emulation, migration, and virtual machines are not specifically aimed at archiving dynamic data visualisations, have mixed results when capturing interactive content, and are complex and expensive to implement and maintain, they could shed a light on tools necessary for archiving data visualisation.\nThey could also provide valuable directions for future preservation of dynamic and interactive data visualisations in data journalism.\nThe user interaction and experience may be key to the meaning and value of a given data visualisation.\n2) Flattening the visual\nThe second approach attempts to capture a “flat” or simplified version of the visualisation via methods such as snapshots, documentation, and metadata. \nA flat or simplified version, considered in digital preservation language under the category of ‘surrogates’, essentially turns dynamic visualisations from complex digital objects into simple objects, such as images, GIF animations or videos, which are more easily preserved. \nThe dynamism is not maintained, but an effort is made to capture a sense of the original visualisation to preserve at least some part of it from total loss.\nHow to choose? Significant Properties\nIn choosing which of these approaches is most suitable for a given dynamic data visualisation or a given story in news and journalism, in our paper we draw on the concept of ‘Significant Properties’ of digital objects, originally proposed by Margaret Hedstrom \u0026 Christopher A. Lee in 2002 as their response to archiving of digital items in relation to their original physical object, such as a physical book being archived in digital format (on microfilm!), or when digital objects were converted from one format to another. \nThe idea was that the digitised version of a book, for example, may not be capable of preserving all of the properties of the original hard copy materials, such as accurate colour representation or the exact physical dimensions of the originals. \nSignificant Properties are those properties of digital objects that affect their quality, usability, rendering, and behaviour. These are typically technical or behavioural characteristics of the digital objects, which need to remain unchanged when the file is accessed in the future, in order for the file to fulfil its original purpose. \nIn the case of image files this might include aspects such as the height, width and colour depth of the image, while for video content it could include aspects such as the playback length and frame rate.\nIt may not be necessary to preserve the entire interactive data visualisation in a working form.\nSoftware and other interactive digital objects tend to have more complicated significant properties relating to their behaviour and the types of possible user interaction. \nComputer games, for example, are inherently experiential: the experience of the game is a significant property of the application. This can also be the case with data visualisations. The user interaction and experience may be key to the meaning and value of a given data visualisation. \nOn the other end of the spectrum, interactivity may not provide vital value to the visualisation, rather the information conveyed through different interactive elements may be considered the significant properties. Or it could be somewhere in the middle.\nThe first step here therefore for us would be to identify these significant properties, put next to the time and resources available, and go forward in relation to our preservation methods accordingly.\nWhere interactivity is a Significant Property, an approach using techniques such as emulation or migration may be indicated, as this preserves a working version of the original visualisation, and is thus more likely to preserve all of the significant properties of the object.\nThis approach would be in line with recent recommendations by the Digital Preservation Coalition on preserving Software.\nOn the other hand, for some interactive data visualisations, dynamism and interactivity are not significant properties of the object, and much of the message is communicated without these aspects. \nIn such cases, it may not be necessary to preserve the entire interactive data visualisation in a working form, as an approach using snapshots and documentation as surrogates for the original may satisfactorily retain the significant properties. \t\nIdentifying whether and to what extent these are significant properties of a visualisation can help in selecting which approach to take in its preservation. But these must be considered alongside other resource and workflow requirements and limitations for preservation.\nNon-technical challenges\nRegardless of the technical approach taken to preservation, several systemic methods could be drawn on from recognised topics in the digital preservation domain. Overall, our research indicates that the complexity of the task of preservation is the biggest obstacle to preserving these objects.This complexity is not limited to technical aspects. \nRather, it is in part attributable to the wider cultural or organisational challenge of digital preservation, where resources - financial and human - are limited, preservation is not embedded in publication workflows, and advocates for preservation are few and far between. \nFurthermore, the responsibility for these actions must be identified and pursued systematically. Awareness-building around preservation, guidelines for preserving visualisations, and training on how to integrate preservation into workflows can assist with these larger social or organisational challenges.\nRecommendations for going forward\nRecommendations for Immediate and Practical Interventions by Data Journalists\nHere I start with a set of immediate and simple actions that could be taken by data journalists to ensure partial preservation of the content they are producing now, in lieu of more robust approaches to be developed and implemented widely in the future.\nThese are approaches that assume limited time and resources combining a basic identification of significant properties, along with the creation of surrogate output of types that are easily preservable using current technologies, such as images and audiovisual formats. \nThis is essentially a basic form of the snapshot method identified in the literature.\nIf a number of screen grabs in the form of a GIF animation cannot do justice to the visualisation, then consider creating a video cast of the data visualisation in use.\nWe propose that for every dynamic data visualisation included in a story, the journalist should:\nIdentify the significant properties of the data visualisation, in terms of the importance of the story at hand. \nIf an image screenshot of the data visualisation could represent these properties to a satisfactory degree, then take a screenshot of the visualisation, and store it with other archived audiovisual content.\nScreenshots have been used by some news organisations in their archiving practices. Figure 4.1 and 4.2 depicts two examples from The Washington Post and the New York Times, where the story is missing due to the issue of Flash, but the organisations offer access to alternate archived content.\nFIGURE 4.1. Screenshots taken in February 2021 from The Washington Post depicting the disappearance of the stories due to the deprecation of Flash, as well as the accessible screenshots through their archives.\nFIGURE 4.2. Screenshots taken in February 2021 from The New York Times, showing the inability to read a story thanks to its reliance on Flash.\nFollowing the link in The Washington Post story retrieves a PDF, which had been previously generated for the print version of the story. \nClearly, this conveys an acceptable degree of the original story’s intention. However, the link in The New York Times story retrieves a screenshot that only shows the first slide of a multi-slide story, which means a significant part is missing. \nIf an image screen grab cannot capture the story to a satisfactory level, then we propose two alternatives:\na. If a small number of screen grabs can tell the story, then create a GIF animation that includes these in sequence, and archive as above. GIF animations allow limited animation but are nonetheless relatively simple image files which are straightforward to preserve. \nMany news organisations already create animated GIFs for content promotion on social media and so the tools and expertise are readily available. \nThe Economist data desk, for example, provided a workshop on From interactive to social media: how to promote data journalism at the 2018 edition of the European Data \u0026 Computational Journalism Conference, for which they create GIF animations to promote their interactive data visualisations on social media. \nThese GIF animations, in essence, capture some part of the significant properties of the original interactive data visualisation.\nb. If a number of screen grabs in the form of a GIF animation cannot do justice to the visualisation, then consider creating a video cast of the data visualisation in use, highlighting the most important parts. A range of widely-available free tools can be used to create such video content which is also relatively simple to preserve.\nThese simple surrogate representations must also be linked to the original story to ensure that the reader can find them if the story remains available, but the original visualisation is no longer available.\nThis linking could be via a structural solution whereby the CMS of the news organisation allows an alternate link to be specified and automatically displays the file behind the link if the main visualisation fails to load. \nAn alternative or possible interim solution would be to include a link under each visualisation to the surrogate version which invites the user to click on it if the visualisation does not display correctly. An example of how this has worked in practice could be seen in Figure \nCreating an image, GIF animation or a video of your data visualisation is an uncomplicated solution that enables the capture of significant properties in terms of content as story, providing a stop-gap until more systematic and sophisticated methods for preservation of dynamic data visualisations are in place. In addition to long-term preservation and access, this simple method could also cater for issues associated with loading complex objects across devices.\nThese recommendations address the need for an organised and sustainable approach to the long-term digital preservation of data visualisations.\nAs such, we also propose that every provider of data visualisation creation tools should ideally provide GIF animation and video exports, in addition to their current visualisation exports. \nMany data visualisation providers promise their users that in the case of company closure, users will be given the option to download the code behind the charts. \nThis is a responsible offer, but most journalists will not have the time or skills to execute that code on a different platform. Nor will they be able to go back to every single story they created to update the server information for where the data visualisation is hosted. \nHence, it is advisable that journalists create simple exports of their data visualisations at time of publication, and provide the information for how these can be accessed if the original publication fails. \nBoth data journalists, and the wider digital preservation community, should advocate with vendors of these tools to help bring this about.\nThe preservation of the datasets that underlie data visualisations is also key.\nThese immediate and relatively contained measures could ensure that much of the data journalism currently being produced is not lost entirely, while the newsrooms find ways to implement the recommendations to ensure longer term systematic preservation of such complex objects. \nIn addition to these, in the paper, my colleagues and I provide a set of recommendations for systematic and more long term interventions. These recommendations draw on the systematic study of the literature in a set of relevant areas such as web archiving, digital preservation, software and game archiving, methods detailed in professional literature from the fields of data journalism and digital preservation, as well as our professional expertise as academics and practitioners in these areas. \nOur recommendations for systematic, organisation and discipline based interventions fall into several categories, including guidance and education, infrastructure and tools, collaboration with trusted, local and national digital repositories and memory institutions, funding and resourcing, and legal frameworks. \nThese recommendations for long term and systematic interventions address the need for an organised and sustainable approach to the long-term digital preservation of data visualisations. \nThey aim to ensure that these increasingly important elements of journalistic output are routinely preserved alongside simpler forms of digital news media. \nThese medium to long-term actions require changes to workflows and investment into new policies, practices and technical solutions. As such, they require an investment of significant effort over time, financial resources, and collaborations that may expand the remit of existing institutions. \nIf you are interested to read more about these, read the Recommendations part of the paper.\nDigital preservation is an ongoing process, not simply an endpoint.\nI would like to note here that the scope of this article, and the research paper underlying it, includes works relating to the preservation of dynamic data visualisation and associated software code and dynamic digital objects. \nThe preservation of the datasets that underlie data visualisations is also key; in some cases, they are required to make the visualisation function as it is rendered. In any case, the data should be persistently accessible to verify the findings communicated by the visualisation. However, this is a separate, larger issue for digital preservation and is out of the scope of this article. \nAs a pointer and food for thought, the preservation of research data is being studied by international initiatives such as the Research Data Alliance and the CODATA committee of the International Science Council, which could provide valuable input into the preservation of data and code when it comes to data journalism.\nDigital preservation is an ongoing process, not simply an endpoint. Methods must evolve within and by the communities that are most invested in the long-term stewardship of their outputs. \nBecause of journalism’s fundamental and unique contribution to the historical record, it is imperative that preservation is built into the production of data journalism, so that this key element of the record is not lost.\nBahareh Heravi is a Data and Computational Journalism researcher, trainer, practitioner and innovator. She is currently a Reader in AI and Media at the Institute for People-Centred AI at the University of Surrey in the UK.  Bahareh is a member of the Irish Open Data Governance Board, and a co-founder and co-chair of the European Data \u0026 Computational Journalism Conference. She previously was an Assistant Professor at the School of Information and Communication Studies at UCD, where she led the Data Journalism programme.\nThis article is a shortened and adapted version of an academic paper that Bahareh Heravi co-authored with her colleagues Kathryn Cassidy and Natalie Harrower from the Digital Repository of Ireland, and Edie Davis from the Library of the Trinity College Dublin. \nFor the full journal paper, and also for citation and referencing please visit the journal website.","guid":"https://datajournalism.com/read/longreads/how-to-save-data-journalism","isoDate":"2022-07-17T22:00:00.000Z","blogTitle":"DataJournalism.com"}},"__N_SSG":true},"page":"/[...slug]","query":{"slug":["how-to-preserve-data-journalism"]},"buildId":"HEkhRyO68Nq4toHtepe11","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>