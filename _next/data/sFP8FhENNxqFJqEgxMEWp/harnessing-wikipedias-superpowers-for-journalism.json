{"pageProps":{"post":{"creator":"Monika Sengul-Jones","title":"Harnessing Wikipedia's superpowers for journalism","link":"https://datajournalism.com/read/longreads/harnessing-wikipedias-superpowers-for-journalism","pubDate":"Wed, 02 Dec 2020 07:00:00 +0100","author":"Monika Sengul-Jones","content":"\n                                                                                                                    <p>Orientations to Wikipedia often begin with its enormity. And it is enormous. The encyclopedia will be 20 years old in January 2021 and has more than 53 million articles in <a href=\"https://meta.wikimedia.org/wiki/List_of_Wikipedias\">314 languages</a>. <a href=\"https://en.wikipedia.org/wiki/Special:Statistics\">Six million are in English</a>. According to Alexa.com, <a href=\"https://www.alexa.com/topsites/countries/US\">Wikipedia is the 8th most-visited web domain in the United States</a>, and the <a href=\"https://www.alexa.com/topsites\">13th globally</a>; it’s the only non-profit in the top-100 domains. In November 2020, more than 1.7 billion unique devices from around the world accessed Wikipedia articles. Average monthly pageviews surpass 20 billion. </p>\n<p>Beyond reach, there’s the data. All data on and about all Wikipedias—from pageview statistics, <a href=\"https://diff.wikimedia.org/2018/04/05/ten-most-cited-sources-wikipedia/\">most-frequently cited references</a>, to access to every version ever written and all the editors who have ever contributed to it—is <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Copyrights\">freely available</a>. Entire version histories are available at <a href=\"https://dumps.wikimedia.org/\">dumps.wikimedia.org</a>.</p>\n\n                                                                     \n                            <p>Twitter bots that share edited Wikipedia entries text from high impact IP addresses, such as the White House, which is covered by the @whitehouseedits bot, pictured above, can help data journalists track malfeasance. But there’s evidence the bots can be manipulated. Image credit: <a href=\"https://twitter.com/whitehousedits/status/1293889545805672451/photo/1\">Twitter @Whitehousedits</a></p>\n\n                                                                      \n                            <p>Thanks to the free and open access to billions of human and machine-readable data, corporations and research centres have been leveraging Wikipedia for research for years. <a href=\"https://com.uw.edu/people/faculty/benjamin-mako-hill/\">Benjamin Mako Hill</a>, assistant professor of communication at the <a href=\"https://www.washington.edu/\">University of Washington</a> and <a href=\"https://communication.northwestern.edu/faculty/aaron-shaw\">Aaron Shaw</a>, associate professor of communication at <a href=\"https://www.northwestern.edu/\">Northwestern University</a>, describe Wikipedia as the “most important laboratory for social scientific and computing research in history” in <a href=\"https://wikipedia20.pubpub.org/pub/fgas2h4l/release/2\">their chapter</a> in <a href=\"https://wikipedia20.pubpub.org/\">\"Wikipedia@20\"</a>, a new book on Wikipedia published by MIT Press, edited by Joseph Reagle and Jackie Koerner. </p>\n<p>“Wikipedia has become part of the mainstream of every social and computational research field we know of,” Hill and Shaw write. Google’s knowledge graph and smart AI technologies, such as Amazon’s Alexa and Google Home, are based on metadata from <a href=\"https://meta.wikimedia.org/wiki/Wikimedia_projects\">Wikimedia projects</a>, of which Wikipedia is the best-known. Significant for data journalists is how Wikipedia’s influence has already surpassed clicks to article pages; in a way, the internet is already Wikipedia’s world, we’re just living in it.</p>\n\n                                                                      \n                            <p>But journalists know well that ubiquity shouldn’t stand in for universality. We should be mindful that indiscriminate use of “big data” without acknowledging context reproduces what <a href=\"https://www.media.mit.edu/people/joyab/overview/\">Joy Buolamwini</a>, founder of the <a href=\"https://www.ajl.org/\">Algorithmic Justice League</a>, calls the <a href=\"https://www.youtube.com/watch?v=ZSJXKoD6mA8\">“coded gaze” of white data</a>. <a href=\"https://safiyaunoble.com/\">Safiya Umoja Noble</a>, a critical information studies expert and associate professor at <a href=\"https://is.gseis.ucla.edu/\">UCLA</a>, challenges the acceptance of invisible values that normalise algorithmic hierarchies. </p>\n<p>Internet search results, which often prioritise Wikipedia articles in addition to using Wikipedia’s infobox data or structured data in sidebars, “feign impartiality and objectivity in the process of displaying results” Noble writes in <a href=\"http://algorithmsofoppression.com/\">\"Algorithms of Oppression: How Search Engines Reinforce Racism\"</a>. </p>\n<p>Systemic biases on Wikipedia, including <a href=\"https://meta.wikimedia.org/wiki/Research:Knowledge_Gaps_Index/Taxonomy/Full_paper#Sociodemographics_Gaps\">well-documented “gaps” in coverage, readership, and source</a>, are cause for pause. Globally, volunteer contributors are predominately white males from the northern hemisphere. On English Wikipedia, less than 20% of editors self-identify as female. Asymmetries in participation have impacted the editorial processes and content. Editors who <a href=\"https://upload.wikimedia.org/wikipedia/commons/7/77/The_Heart_Work_of_Wikipedia_Gendered,_Emotional_Labor_in_the_World%27s_Largest_Online_Encyclopedia.pdf\">self-identify as women often perform “emotional work”</a> to justify their contributions. Women and nonbinary users on Wikipedia may encounter <a href=\"https://meta.wikimedia.org/wiki/Research:Communicating_on_Wikipedia_while_female\">hostile, violent language</a> and some have experienced harassment and doxing. Then there are the asymmetries in the breadth and depth of coverage; only approximately 17% of biographies on English Wikipedia are about women.</p>\n\n                                                                                                 <p><strong> How to contribute to Wikipedia</strong></p>\n<p>Anyone can edit Wikipedia, but there is an editorial pecking order and policies to keep in mind. Tips for success:</p>\n<ol>\n<li><p>Assuming you have <a href=\"https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&amp;returnto=Wikipedia:Why_create_an_account%3F\">created an account</a>, be sure to include a bio on your <a href=\"https://en.wikipedia.org/wiki/Wikipedia:User_pages\">user page</a> (you don't need to use your real name, but you can).</p>\n</li>\n<li><p>Improve existing articles to begin, you can create new articles once your account is four days old and you’ve made ten edits.</p>\n</li>\n<li><p>Include <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Verifiability\">verifiable citations</a> to secondary sources for any new claims--or claims where a citation is needed.</p>\n</li>\n<li><p>Be aware of Wikipedia’s guidelines on <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Conflict_of_interest\">conflicts of interest</a>.</p>\n</li>\n</ol>\n<p>Beyond this, there are many tutorials and videos with various tips and tricks. Among them, <a href=\"https://www.groovypost.com/howto/guide-getting-started-wikipedia-contributor/#:~:text=Creating%20Your%20Registered%20Wikipedia%20Account&amp;text=Select%20%E2%80%9Chelp%20me%20choose%E2%80%9D%20if,and%20now%20a%20Wikipedia%20contributor\">this is a useful high-level summary</a>, while an <a href=\"https://outreachdashboard.wmflabs.org/training/editing-wikipedia/editing-basics\">editing tutorial</a> hosted by the Wikimedia Foundation walks you through nitty-gritty basics.</p>\n\n                                                                     \n                            <p>With this glut of imperfect or missing data, what’s a data journalist to do? Journalists doing internet research might consider that they are already knee-deep in a minefield of constraints. </p>\n<p>“The reality for journalists working on the internet is fraught,” said Hill. “Most internet data sets are controlled by commercial companies. That means there’s never going to be a full data set and what’s available has been—or is being—manipulated. Wikipedia is different. It’s free, it’s accessible, and it’s from a public service organisation.” Like any institution, as <a href=\"https://dusp.mit.edu/faculty/catherine-dignazio\">Catherine D’Ignazio</a> has pointed out <a href=\"https://datajournalism.com/read/longreads/putting-data-back-into-context\">in this publication</a>, context may be hard to find. On Wikipedia, that’s often due to the decentralised organisation of open source projects; volunteers come and go, rather than intentional obfuscation.  </p>\n<p>Nevertheless, <a href=\"https://www.wired.com/author/noam-cohen/\">Noam Cohen</a>, a journalist for Wired and The New York Times who has written about Wikipedia for nearly two decades, said in a phone interview that journalists should—if they are not already—use Wikipedia’s data, including pageviews and the layers of information found in article pages. But Cohen cautions journalists not to let Wikipedia’s decisions on coverage replace news judgement. “In journalism, word length is often a sign of importance,” Cohen said. “That’s not the case on Wikipedia, there are articles about \"The Simpsons\" or characters on \"Lost\" that are longer than articles about important women scientists or philosophers. But these trends don’t mean there are not rules. There are, the information is changing.”</p>\n\n                                                                                                 <blockquote><p>To leverage Wikipedia’s superpowers for data journalism, it’s best to climb into the belly of the beast.</p>\n</blockquote>\n\n                                                                     \n                            <p>Last year, Cohen’s editor asked him to write about why his Wikipedia biography—which he did not create, there are guidelines barring <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Conflict_of_interest\">“conflict of interest editing”</a>—was deleted. Cohen dug in and <a href=\"https://www.wired.com/story/socked-into-the-puppet-hole-on-wikipedia/\">discovered it was due to “sock-puppetry;</a>” that’s shorthand for editors who use more than one account without disclosure. Later, another editor restored Cohen’s biography. </p>\n<p>Stories like this may give journalists discomfort about the contingencies of the online encyclopedia, and any data sets therein. And for as long as there’s been Wikipedia, there have been editors and professors warning us to stay away. But Cohen suggests thinking otherwise. “The fact that information is slowly being changed and is always saved is Wikipedia’s superpower,” said Cohen. To leverage Wikipedia’s superpowers for data journalism, it’s best to climb into the belly of the beast.</p>\n\n                                                                      \n                            <h2>Understand how Wikipedia’s authority works</h2>\n<p>While one might reasonably guess that <a href=\"https://wikimediafoundation.org/\">The Wikipedia Foundation</a> manages editorial oversight, that’s not the case. All content decisions, including developing and managing bots to do tedious, repetitive tasks—fixing redirects or reverting vandalism, as <a href=\"https://en.wikipedia.org/wiki/User:ClueBot_NG\">ClueBot_NG</a> does—are designed and run by volunteers. The Wikipedia community has developed a number of policies and guidelines to govern editing, including a rule about verifiability and a blacklist of publications not allowed to be cited on Wikipedia. Blacklisted publications include spam and publications that do not fact check and circulate conspiracy theories.</p>\n<p>In 2017, Katherine Maher, executive director of The Wikimedia Foundation, <a href=\"https://www.theguardian.com/technology/2017/feb/12/wikipedia-daily-mail-reliability-ban-katherine-maher\">spoke with The Guardian</a> about the volunteer community’s decision to blacklist The Daily Mail as a reliable source. “It’s amazing [Wikipedia] works in practice,” she said, motioning to a concept that academics have called peer-production or crowdsourcing. “Because in theory it is a total disaster.” Wikipedia works in practice, and not in theory. It’s a popular idiom among Wikipedians, as <a href=\"https://www.colorado.edu/cmci/people/information-science/brian-c-keegan\">Brian Keegan</a> writes in <a href=\"https://wikipedia20.pubpub.org/pub/dj6frhgz/release/2\">Wikipedia@20</a>. And it does suggest there’s something magical about the project, where successful shared editing of a single document has been happening long before Google docs.</p>\n<p>There is a logic to Wikipedia—no magic. The free encyclopedia launched in 2001 for “anyone” to edit. This was not an explicit democratic effort to engage portions of the public who have historically been left out of structures of power, though some have championed Wikipedia for getting close to achieving this. Rather, the effort was a wildcard reversal of Wikipedia’s failed predecessor, <a href=\"https://en.wikipedia.org/wiki/Nupedia\">Nupedia</a>, which was designed as a free, peer-reviewed encyclopedia edited by recognised experts. When shifted from experts to “anyone”—that is, people who happened to have computers, internet connections, a penchant for online debate and were familiar with <a href=\"https://www.mediawiki.org/wiki/MediaWiki\">MediaWiki</a>, as opposed to busy academic experts—contributions flowed faster. </p>\n<p>Wikipedia was also a product of its time. It was one of many online encyclopedia projects in the early 2000s. According to the <a href=\"https://www.law.cornell.edu/uscode/text/47/230\">Section 230</a> of the <a href=\"https://www.law.cornell.edu/uscode/text/47/230\">1996 Computer Decency Act in the United States</a>, Wikipedia, like other platforms then and now, has been immune from legal liability for contents. <a href=\"https://www.law.cornell.edu/uscode/text/47/230\">Section 230</a> also gives platforms the legal blessing to govern as they see fit. Jimmy Wales, co-founder of Wikipedia, set up the Wikimedia Foundation to oversee the project and sister platforms in 2005, and it has remained volunteer-run. The Wikimedia Foundation has an endowment of more than 64 million, with tech titans such as <a href=\"https://wikimediafoundation.org/news/2020/09/25/amazon-donates-1-million-gift/\">Amazon pledging millions</a>, and the Foundation supports projects by volunteers and affiliates. English Wikipedia has snowballed in popularity on a commercial internet. Google, for instance, prioritises Wikipedia articles in search results—treats them like “gospel” said Cohen, while the <a href=\"https://firstmonday.org/article/view/2830/2476\">convenience, currency, and comprehensibility</a> of Wikipedia attracts regular readers.</p>\n\n                                                                      \n                            <h2>Using pageviews to tell a story</h2>\n<p>Data journalists can find the granular level of insight about pageviews handy for storytelling. Viewers of Wikipedia come from around the world. The Wikimedia Foundation does not track individual data, but tracks devices across pages. Data about what type of device—mobile app, mobile browser, or desktop browser—are used to access pages. This can give journalists insight into topical and regional access trends. </p>\n<p>More radically, pageviews can reveal kernels of stories yet to be broken. Let’s simulate research using pageviews for a story on the rising COVID-19 case count in light of concerns about circulation of misinformation and disinformation on the virus. Digging into pageview data on COVID-19 articles in English Wikipedia can help to tell this story, and others like it. </p>\n<p>In spring 2020, as unprecedented economic and social changes unfolded across the globe, journalists were at the forefront of providing coverage on this moment. Meanwhile, conspiracy theories were gaining visibility in social media groups, while edit counts and information queries about all articles related to COVID-19 were at their highest to date. </p>\n<p>By mid-November 2020, a new trend. Positive cases of COVID-19 skyrocketed around the globe. Several European countries and U.S. states re-introduced lockdown measures to slow the spread of the virus. But Wikipedia pageviews for articles about COVID-19 were not rising, in fact, they were lower than earlier in the year. The election pageviews on the presidential candidates and their families were cresting with the U.S. election.</p>\n\n                                                                      \n                            <p>A line graph above shows a pageview analysis from Nov 2019 to Oct. 2020 (x axis) depicting pageviews by the thousands (y axis) of four article pages: Donald_Trump, Coronavirus_disease_2019, Joe_Biden, and George_Floyd. Source: <a href=\"https://pageviews.toolforge.org/?project=en.wikipedia.org&amp;platform=all-access&amp;agent=user&amp;redirects=0&amp;start=2019-11&amp;end=2020-10&amp;pages=Donald_Trump|Coronavirus_disease_2019|Joe_Biden|George_Floyd\">pageviews.toolforge.org/</a></p>\n\n                                                                      \n                            <p>Did election coverage distract readers from the pandemic? Spikes in readership on Wikipedia are often the consequence of other media attention or events, which could help to explain for the peaks in views for George Floyd, Donald Trump and Joe Biden. \n<a href=\"https://www.jackiekoerner.com/\">Koerner</a>, who trained as a social scientist, cautions journalists not to make quick deductions about readers' motivations from high-level pageview data. “It’s tricky to say that pageviews are indicative of what people are thinking,” she said. To dig into more granularity, journalists can dig in and compare sets of pageviews using the browser-optimised <a href=\"https://pageviews.toolforge.org/?project=en.wikipedia.org&amp;platform=all-access&amp;agent=user&amp;redirects=0&amp;range=latest-20&amp;pages=Coronavirus_disease_2019|Donald_Trump|Joe_Biden\">pageview visualisation tool</a> available.</p>\n\n                                                                      \n                            <p>Above is a blue bar graph showing pageviews to the Symptoms of COVID-19 article page rising from October to November 22, 2020 (x axis) by the hundreds (y axis). <a href=\"https://pageviews.toolforge.org/?project=en.wikipedia.org&amp;platform=all-access&amp;agent=user&amp;redirects=0&amp;start=2020-10-01&amp;end=2020-11-22&amp;pages=Symptoms_of_COVID-19\">Pageviews to the Symptoms of COVID-19</a> rose by hundreds in under two months.</p>\n\n                                                                      \n                            <p>Meanwhile, pageviews of the COVID-19 general article may have peaked in the spring, but data journalists can note that pageviews of the article “Symptoms of the coronavirus” rose in October, as depicted above, before the peaking case numbers. Incidentally, this correlation could lend credence to the suggestion by a team of epidemiologists in 2014 that <a href=\"https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003581\">high pageview data about influenza-related Wikipedia articles</a> could be used to make predictions about the percentage of Americans with influenza. While it remains to be seen if pageviews can predict illness spikes, the data can offer a wide lens on the zeitgeist.</p>\n\n                                                                      \n                            <p>Above is a list of the top 10 most viewed articles in 2019, in order of popularity, with lists of the number of edits and editors. Avengers: Endgame, Deaths in 2019, Wikipedia, Ted Bundy, Freddie Mercury, Chernobyl Disaster, and List of Highest-grossing films are top seven. Wikimedia Statistics provide high-level data on trends in pageviews, including top-viewed article pages. <a href=\"https://pageviews.toolforge.org/topviews/?project=en.wikipedia.org&amp;platform=all-access&amp;date=2019&amp;excludes=\">The data was accessed at Pageviews.toolforge.org</a>.</p>\n\n                                                                      \n                            <h2>Behind the scenes</h2>\n<p>With approximately 300 edits per minute—which is <a href=\"http://listen.hatnote.com/\">soothing to listen to</a>—Wikipedia is always changing. You may already have edited Wikipedia, the blue “edit” tab is on almost every article page. There are <a href=\"https://en.wikipedia.org/wiki/List_of_languages_by_total_number_of_speakers#cite_ref-6\">more than 1.2 billion speakers</a> of English and over 40 million Wikipedia accounts.</p>\n<p>Maybe you made an account and your changes stuck. Maybe you tried to write an article, only to have it deleted. Or maybe you wondered about how easy it is to add profanity to an article on a popular topic—only to realise that the “Edit” tab is missing. Rather, there’s a lock. Or possibly, a gold star. </p>\n<p>Locks. Gold stars. Deletions. These are subtle signs and signals that can help you understand how the editing community works.</p>\n\n                                                                      \n                            <p>Above is a labelled diagram of the parts of a Wikipedia page using the example of the Black Lives Matter article. While every article page has these features, I've chosen to label the Black Lives Matter article because it is an extensive composite of the movement's history, it's been peer-reviewed by editors and is locked, which makes vandalism more difficult.</p>\n\n                                                                      \n                            <p>Wikipedia’s “best” are marked with green crosses and gold stars, these are Good and Featured content which have undergone “peer review.” They are the minority among Wikipedia's millions, just 0.1%. </p>\n<p>Meanwhile, the active editorial community on English Wikipedia monthly is about 4,000 editors. Fewer are administrators. As of November 2020, approximately 1,100 users have successfully undergone a “request for administratorship” and have been granted additional technical privileges, including the ability to delete and/or protect pages.  Non administrative editors, however, may patrol new pages and rollback recent changes. </p>\n<p>Wikipedia’s editorial judgement can spark justified outrage. </p>\n<p>Journalist Stephen Harrison covered this recently in his <a href=\"https://web.archive.org/web/20201027185627/https:/slate.com/technology/2020/10/theresa-greenfield-iowa-senate-race-wikipedia-page.html\">Slate article on the Theresa Greenfield biography</a>. While archivists, indigenous and feminist communities have noted the reliable source guidelines exclude oral histories, ephemera, and special collections; I am currently co-leading an <a href=\"https://misinfocon.com/reading-together-reliability-and-multilingual-global-communities-3c7e9bc4af03\">Art+Feminism research project</a> on marginalised communities and reliable source guidelines, funded by <a href=\"https://www.wikicred.org/\">WikiCred</a> which supports research, software projects and Wikimedia events on information reliability and credibility. Data journalists can follow debates on-wiki, and note what is absent, by looking at article Talk and View history tabs, and on notice boards for <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Articles_for_deletion\">deletion</a> and <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Reliable_sources/Noticeboard\">reliable sources</a>. </p>\n<p>At the same time, there’s plenty to be discovered with Wikipedia. Article features such as wikilinks, citations, and categories can help data journalists quickly access a living repository of information.</p>\n\n                                                                      \n                            <p>Above is a labelled diagram showing the wikilinks, citations, and categories using the example of the Black Lives Matter article. On Wikipedia, hyperlinks within articles generally lead to other Wikipedia articles, citations are footnotes with references listed at the end of an article. Categories may help journalists find other articles.</p>\n\n                                                                      \n                            <p>In 2011, an editor began a list that documented people killed by law enforcement in the United States, both on duty and off duty. Since 2015, the annual average number of justifiable homicides reported was estimated to be near 930. <a href=\"https://en.wikipedia.org/wiki/Lists_of_killings_by_law_enforcement_officers_in_the_United_States\">Tables about gun violence</a>, have been collected on Wikipedia for nearly a decade.</p>\n\n                                                                      \n                            <p>Above shows a diagram of portions of the article list of killings by law enforcement officers in the United States, including a monthly table from pre-2009 to 2020. This Wikipedia list has amassed data sets from hundreds of sources that verify the killing of humans by law enforcement officers. Between 930 and 1,240 people are killed by police annually in the United States.</p>\n\n                                                                      \n                            <p>The integrity of this list was brought to my attention by <a href=\"http://www.jennifer8lee.com/\">Jennifer 8. Lee</a>, a former New York Times journalist. She expressed surprise that there are not more examples of journalists using Wikipedia’s data. Lee would know, she co-founded the U.S.-based <a href=\"https://credibilitycoalition.org/\">Credibility Coalition</a> and <a href=\"https://misinfocon.com/\">MisinfoCon</a>, and supports <a href=\"https://www.wikicred.org/#-1\">WikiCred</a>, which addresses credibility in online information and includes Wikipedians, technologists, and journalists.</p>\n<p>“[These] are fascinating and useful,” said Lee. “Not automated, this is a hand-written list. It’s all in one place. This is useful for journalists and those of us in the credibility sphere to use it for research.” </p>\n<p><a href=\"https://wikimediafoundation.org/news/author/eerhart/\">Ed Erhart</a>, who works with the Wikimedia Foundation’s audience engagement team, suggests that stories can not only be a repository but fodder for coverage. “I like to say that there is a story in every Wikipedia article,” he wrote by email, drawing my attention to a Featured article about a small town, <a href=\"https://en.wikipedia.org/wiki/Arlington,_Washington\">Arlington, Washington</a>. “Who wrote it? Where are they from? What motivated them? The talk and history tabs on Wikipedia's pages can be the starting point for some truly unique takes on local places and issues.”</p>\n\n                                                                                                 <p><strong>Quick links</strong></p>\n<ul>\n<li><p>Visit <a href=\"https://pageviews.toolforge.org/?project=en.wikipedia.org&amp;platform=all-access&amp;agent=user&amp;redirects=0&amp;range=latest-20&amp;pages=Cat|Dog\">pageview statistics homepage</a></p>\n</li>\n<li><p>More about <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Protection_policy\">page protection</a></p>\n</li>\n<li><p>More about <a href=\"https://en.wikipedia.org/wiki/Wikipedia:User_access_levels\">user access levels</a></p>\n</li>\n<li><p>More about <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Featured_articles\">featured articles</a> and <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Featured_lists\">lists</a></p>\n</li>\n<li><p>Icon for featured articles is a star</p>\n</li>\n<li><p>Icon for good articles is a green circle with a plus sign</p>\n</li>\n</ul>\n\n                                                                     \n                            <h2>Catching malfeasance</h2>\n<p>Data journalists can follow edits to track corporate or governmental malfeasance. Article pages about companies or politicians can be scrubbed to omit negative information. Though editors are required to disclose conflicts of interest on their user page or in the <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Conflict_of_interest#How_to_disclose_a_COI\">Article's talk page</a>.</p>\n\n                                                                      \n                            <p>Users who edit Wikipedia as a part of their paid work are required to disclose conflicts of interest. This image shows an example of a user who has done so. John P. Sadowski edits articles on biomedical topics, including articles related to COVID-19, using resources from his employer, the U.S. Center for Disease Control (CDC). Not all contributors do this.</p>\n\n                                                                      \n                            <p>Not all contributors disclose. <a href=\"https://scholar.google.com/citations?user=oAUwQCEAAAAJ&amp;hl=en\">Kaylea Champion</a>, a doctoral student at the University of Washington, led a large scale research project on IP editing and <a href=\"https://dl.acm.org/doi/10.1145/3359155?cid=81100401804\">discovered systematic deletions to mining articles</a>. Anonymous editors removed information about environmental contamination and abuse. Champion and her co-authors traced the IP addresses that deleted the incriminating information to the headquarters of the mining companies. </p>\n<p>Journalists can do their own large-scale reconstructions of edit histories using data from Wikipedia’s <a href=\"https://dumps.wikimedia.org/\">data dump</a>, or manually browse pages of interest. Historical contributions can all be accessed, even if they are not visible on the live page. As well, journalists can reach out to editors by writing a note on their Talk page with information on how to connect.</p>\n<p>The below GIF demonstrates how to access View History and compare versions of the Black Lives Matter article page, using the Compare Version History tool. Be sure to use the View History tab to compare version histories, which is shown above. You can also click on the timestamp to view an article in full.</p>\n\n                                                                                                                                             <p><strong>Quick links</strong> </p>\n<ul>\n<li><p><a href=\"https://wikimedia.org/api/rest_v1/#/Pageviews_data/get_metrics_\">Wikipedia’s API</a> </p>\n</li>\n<li><p><a href=\"https://dumps.wikimedia.org/\">Data dumps</a> with complete copies of all Wikipedias</p>\n</li>\n</ul>\n\n                                                                     \n                            <h2>Tracking with bots</h2>\n<p>Bots can help with tracking. In 2014, a number of bots were launched by volunteers to track edits made by specific IP ranges and posted the findings to Twitter. <a href=\"https://twitter.com/parliamentedits\">Parliament WikiEdits</a>, one of the first, still regularly tweets edits made to Parliamentary IPs in the UK. Similar efforts have been available for <a href=\"https://twitter.com/whitehousedits\">The White House</a>, <a href=\"https://twitter.com/euroedit\">European Union</a>, <a href=\"https://twitter.com/wikistorting\">Norwegian Parliament</a>, <a href=\"https://twitter.com/bundesedit\">German Parliament</a>, Goldman Sachs and Monsanto Company, though not all are up to date. </p>\n<p>For data journalists interested in setting up a bot that tweets about anonymous Wikipedia edits from particular IP address ranges in their beat, <a href=\"https://github.com/edsu/anon\">the code is available</a> from Ed Summers on GitHub under a CC0 license.</p>\n\n                                                                                                 <blockquote><p>Data journalists should weigh the public benefit of amplifying hate speech, harassment, or vandalism, which could be a form of coded language, with reporting.</p>\n</blockquote>\n\n                                                                     \n                            <h2>Pitfalls to avoid: steering clear of media manipulation</h2>\n<p>Summers created @CongressEdits in 2014, which tweeted IP contributions from U.S. capitol computers. <a href=\"http://thewikipedian.net/2019/01/17/congressedits-twitter-suspended/\">The Wikipedian</a> reported that “Twitter-addicted journalists” soon were mining the bots for story ideas -- some of which did reveal manipulation, such as an attempt to water down <a href=\"https://mashable.com/2014/12/10/senate-wikipedia-torture-report/?europe=true#YgOIf16EJkqN\">the entry on CIA torture</a>. @CongressEdits amassed a growing audience.  Things came to a head in 2018. <a href=\"https://www.washingtonpost.com/local/public-safety/democratic-ex-staffer-contests-charges-he-posted-personal-data-on-gop-senators-threatened-witness-in-doxing/2018/10/04/88842844-c806-11e8-b2b5-79270f9cce17_story.html\">A former Democratic staffer</a> (who was later arrested) with access to the U.S. capitol computers inserted personal information to Wikipedia articles about Republican members of the Senate Judiciary Committee. The Twitter account automatically shared out those details with the large following. <a href=\"http://thewikipedian.net/2019/01/17/congressedits-twitter-suspended/\">Twitter banned the bot as a result</a>.</p>\n<p>People can intentionally game the editorial system or interconnections between Wikipedia and other social media platforms. Data journalists should weigh the public benefit of amplifying hate speech, harassment, or vandalism, which could be a form of coded language, with reporting. “Why are people editing articles to say that the [mainstream political party] is [name of radical, violent party]? They want the screenshot,” Cohen remarked. “The best way to get a lie into [the] mainstream is to edit an article, let Google pick it up, and get reporting on it. It’s probably a thrill to plant them.”</p>\n\n                                                                      \n                            <p>Furthermore, Wikipedia has no “real name” policy for editors. Some choose to disclose personal details on user pages, which can help gain the confidence of other editors, but this is not required. Thus, manipulators can mimic the behaviour patterns of a group to blend in.</p>\n<p><a href=\"https://www.hks.harvard.edu/faculty/joan-donovan\">Joan Donovan</a>, director of Technology and Social Change at <a href=\"https://shorensteincenter.org/\">Harvard Kennedy School’s Shorenstein Center</a>, calls this a <a href=\"https://www.theatlantic.com/ideas/archive/2019/03/extremists-understand-what-tech-platforms-have-built/585136/\">“butterfly attack.”</a> Once the fakes are indistinguishable to outsiders from legitimate accounts, the manipulators push contentious issues to divide and delegitimise the group. Be mindful that you are not also falling for a “butterfly attack”—or perpetuating one by accidentally characterising editors as occupying one particular position over another. Instead, get to know the communities behind the data to minimise harm.  </p>\n<p>If you discover vandalism or hate-speech on a page history, consider the impact of your coverage on a topic that has since disappeared. Be mindful of the extent to which the effort at public service can dually serve as a form of publicity or exposure for people sympathetic with fringe ideologies or violence. Reporters who stumble across data on hate-speech might report on this in aggregate, without identifying particular details, to minimise harm.</p>\n\n                                                                                                 <p><strong>Pro tips for navigating Wikipedia:</strong></p>\n<ul>\n<li><p>Get to know Wikipedia’s editorial process and community before reporting on hate speech or harassment</p>\n</li>\n<li><p>Strongly consider the newsworthiness of articles that might give publicity to fringe ideologies</p>\n</li>\n<li><p>Use data in aggregate to avoid revealing details</p>\n</li>\n</ul>\n\n                                                                     \n                            <h2>Circular reporting</h2>\n<p>In 2007, <a href=\"https://www.independent.co.uk/\">The Independent</a> published an article on <a href=\"https://www.independent.co.uk/arts-entertainment/comedy/news/baron-cohen-comes-out-of-character-to-defend-borat-424656.html\">Sasha Baron Cohen</a> that included a line that he had previously worked as an investment banker. Days earlier, <a href=\"https://en.wikipedia.org/w/index.php?title=Sacha_Baron_Cohen&amp;type=revision&amp;diff=87679263&amp;oldid=87661979\">the claim had appeared in Wikipedia</a>, and was unverified. Later, The Independent’s article became the citation for the erroneous claim. </p>\n<p>None of it was true. And Wikipedia editors call incidents like this “citogenesis,” or circular reporting. There is even a Wikipedia <a href=\"https://en.wikipedia.org/wiki/Circular_reporting#Examples_involving_Wikipedia\">article that compiles known instances</a>. <a href=\"https://techdebug.com/blog/2008/04/19/wikipedia-article-creates-circular-references/\">Techdebug blog</a> depicted the Baron Cohen example with the good advice to “pay attention to timelines” when reviewing sources of claims on Wikipedia. When using facts from Wikipedia, trust but verify.</p>\n<p>With close attention to detail and context, data journalists can use Wikipedia’s trove of data to elucidate stories of the digital landscape. “Wikipedia is more than the sum of its parts” said Cohen. “Random encounters are often more compelling than the articles themselves. The search for information resembles a walk through an overbuilt quarter of an ancient capital. You circle around topics on a path that appears to be shifting. Ultimately, the journey ends and you are not sure how you got there.”</p>\n\n                                                                                                                                 <p>Monika Sengul-Jones, PhD, is a freelance researcher, writer and expert on digital cultures and media industries. She was the OCLC Wikipedian-in-Residence in 2018-19. In 2020, she is co-leading <a href=\"https://artandfeminism.org/initiatives/current-initiatives/reading-together/\">Reading Together: Reliable Sources and Multilingual Communities</a>, an Art+Feminism project on reliable sources and marginalised communities funded by WikiCred. <a href=\"https://twitter.com/monikajones\">@monikajones</a>, <a href=\"https://monikasjones.com\">www.monikasjones.com</a></p>\n\n                                                                                                                              \n                            <p>Thanks to Mohammed Sadat Abdulai (Art+Feminism, Wikimedia Deutschland), Ahmed Median (Hack/Hackers), and Kevin Payravi (WikiCred, Wikimedia D.C.), and for taking time to interview with me for background research for this story.</p>\n\n                                                                      \n                            <ul>\n<li><a href=\"https://datajournalism.com/read/longreads/data-visualisation-by-hand\">Data visualisation by hand: drawing data for your next story</a></li>\n<li><a href=\"https://datajournalism.com/read/longreads/hypothesis-data-journalism\">A data journalists guide to building a hypothesis</a></li>\n<li><a href=\"https://datajournalism.com/read/longreads/the-promise-of-wikidata\">The promise of WikiData as a data source for journalists</a></li>\n<li><a href=\"https://datajournalism.com/read/longreads/data-sonification\">Making numbers louder: telling stories with sound</a></li>\n<li><a href=\"https://datajournalism.com/read/longreads/conflict-reporting-with-data\">Conflict reporting with data</a></li>\n<li><a href=\"https://datajournalism.com/read/longreads/own-your-newsfeed-own-your-data\">Own your newsfeed, own your data</a></li>\n</ul>\n\n                                              \n                ","contentSnippet":"Orientations to Wikipedia often begin with its enormity. And it is enormous. The encyclopedia will be 20 years old in January 2021 and has more than 53 million articles in 314 languages. Six million are in English. According to Alexa.com, Wikipedia is the 8th most-visited web domain in the United States, and the 13th globally; it’s the only non-profit in the top-100 domains. In November 2020, more than 1.7 billion unique devices from around the world accessed Wikipedia articles. Average monthly pageviews surpass 20 billion. \nBeyond reach, there’s the data. All data on and about all Wikipedias—from pageview statistics, most-frequently cited references, to access to every version ever written and all the editors who have ever contributed to it—is freely available. Entire version histories are available at dumps.wikimedia.org.\nTwitter bots that share edited Wikipedia entries text from high impact IP addresses, such as the White House, which is covered by the @whitehouseedits bot, pictured above, can help data journalists track malfeasance. But there’s evidence the bots can be manipulated. Image credit: Twitter @Whitehousedits\nThanks to the free and open access to billions of human and machine-readable data, corporations and research centres have been leveraging Wikipedia for research for years. Benjamin Mako Hill, assistant professor of communication at the University of Washington and Aaron Shaw, associate professor of communication at Northwestern University, describe Wikipedia as the “most important laboratory for social scientific and computing research in history” in their chapter in \"Wikipedia@20\", a new book on Wikipedia published by MIT Press, edited by Joseph Reagle and Jackie Koerner. \n“Wikipedia has become part of the mainstream of every social and computational research field we know of,” Hill and Shaw write. Google’s knowledge graph and smart AI technologies, such as Amazon’s Alexa and Google Home, are based on metadata from Wikimedia projects, of which Wikipedia is the best-known. Significant for data journalists is how Wikipedia’s influence has already surpassed clicks to article pages; in a way, the internet is already Wikipedia’s world, we’re just living in it.\nBut journalists know well that ubiquity shouldn’t stand in for universality. We should be mindful that indiscriminate use of “big data” without acknowledging context reproduces what Joy Buolamwini, founder of the Algorithmic Justice League, calls the “coded gaze” of white data. Safiya Umoja Noble, a critical information studies expert and associate professor at UCLA, challenges the acceptance of invisible values that normalise algorithmic hierarchies. \nInternet search results, which often prioritise Wikipedia articles in addition to using Wikipedia’s infobox data or structured data in sidebars, “feign impartiality and objectivity in the process of displaying results” Noble writes in \"Algorithms of Oppression: How Search Engines Reinforce Racism\". \nSystemic biases on Wikipedia, including well-documented “gaps” in coverage, readership, and source, are cause for pause. Globally, volunteer contributors are predominately white males from the northern hemisphere. On English Wikipedia, less than 20% of editors self-identify as female. Asymmetries in participation have impacted the editorial processes and content. Editors who self-identify as women often perform “emotional work” to justify their contributions. Women and nonbinary users on Wikipedia may encounter hostile, violent language and some have experienced harassment and doxing. Then there are the asymmetries in the breadth and depth of coverage; only approximately 17% of biographies on English Wikipedia are about women.\n How to contribute to Wikipedia\nAnyone can edit Wikipedia, but there is an editorial pecking order and policies to keep in mind. Tips for success:\nAssuming you have created an account, be sure to include a bio on your user page (you don't need to use your real name, but you can).\nImprove existing articles to begin, you can create new articles once your account is four days old and you’ve made ten edits.\nInclude verifiable citations to secondary sources for any new claims--or claims where a citation is needed.\nBe aware of Wikipedia’s guidelines on conflicts of interest.\nBeyond this, there are many tutorials and videos with various tips and tricks. Among them, this is a useful high-level summary, while an editing tutorial hosted by the Wikimedia Foundation walks you through nitty-gritty basics.\nWith this glut of imperfect or missing data, what’s a data journalist to do? Journalists doing internet research might consider that they are already knee-deep in a minefield of constraints. \n“The reality for journalists working on the internet is fraught,” said Hill. “Most internet data sets are controlled by commercial companies. That means there’s never going to be a full data set and what’s available has been—or is being—manipulated. Wikipedia is different. It’s free, it’s accessible, and it’s from a public service organisation.” Like any institution, as Catherine D’Ignazio has pointed out in this publication, context may be hard to find. On Wikipedia, that’s often due to the decentralised organisation of open source projects; volunteers come and go, rather than intentional obfuscation.  \nNevertheless, Noam Cohen, a journalist for Wired and The New York Times who has written about Wikipedia for nearly two decades, said in a phone interview that journalists should—if they are not already—use Wikipedia’s data, including pageviews and the layers of information found in article pages. But Cohen cautions journalists not to let Wikipedia’s decisions on coverage replace news judgement. “In journalism, word length is often a sign of importance,” Cohen said. “That’s not the case on Wikipedia, there are articles about \"The Simpsons\" or characters on \"Lost\" that are longer than articles about important women scientists or philosophers. But these trends don’t mean there are not rules. There are, the information is changing.”\nTo leverage Wikipedia’s superpowers for data journalism, it’s best to climb into the belly of the beast.\nLast year, Cohen’s editor asked him to write about why his Wikipedia biography—which he did not create, there are guidelines barring “conflict of interest editing”—was deleted. Cohen dug in and discovered it was due to “sock-puppetry;” that’s shorthand for editors who use more than one account without disclosure. Later, another editor restored Cohen’s biography. \nStories like this may give journalists discomfort about the contingencies of the online encyclopedia, and any data sets therein. And for as long as there’s been Wikipedia, there have been editors and professors warning us to stay away. But Cohen suggests thinking otherwise. “The fact that information is slowly being changed and is always saved is Wikipedia’s superpower,” said Cohen. To leverage Wikipedia’s superpowers for data journalism, it’s best to climb into the belly of the beast.\nUnderstand how Wikipedia’s authority works\nWhile one might reasonably guess that The Wikipedia Foundation manages editorial oversight, that’s not the case. All content decisions, including developing and managing bots to do tedious, repetitive tasks—fixing redirects or reverting vandalism, as ClueBot_NG does—are designed and run by volunteers. The Wikipedia community has developed a number of policies and guidelines to govern editing, including a rule about verifiability and a blacklist of publications not allowed to be cited on Wikipedia. Blacklisted publications include spam and publications that do not fact check and circulate conspiracy theories.\nIn 2017, Katherine Maher, executive director of The Wikimedia Foundation, spoke with The Guardian about the volunteer community’s decision to blacklist The Daily Mail as a reliable source. “It’s amazing [Wikipedia] works in practice,” she said, motioning to a concept that academics have called peer-production or crowdsourcing. “Because in theory it is a total disaster.” Wikipedia works in practice, and not in theory. It’s a popular idiom among Wikipedians, as Brian Keegan writes in Wikipedia@20. And it does suggest there’s something magical about the project, where successful shared editing of a single document has been happening long before Google docs.\nThere is a logic to Wikipedia—no magic. The free encyclopedia launched in 2001 for “anyone” to edit. This was not an explicit democratic effort to engage portions of the public who have historically been left out of structures of power, though some have championed Wikipedia for getting close to achieving this. Rather, the effort was a wildcard reversal of Wikipedia’s failed predecessor, Nupedia, which was designed as a free, peer-reviewed encyclopedia edited by recognised experts. When shifted from experts to “anyone”—that is, people who happened to have computers, internet connections, a penchant for online debate and were familiar with MediaWiki, as opposed to busy academic experts—contributions flowed faster. \nWikipedia was also a product of its time. It was one of many online encyclopedia projects in the early 2000s. According to the Section 230 of the 1996 Computer Decency Act in the United States, Wikipedia, like other platforms then and now, has been immune from legal liability for contents. Section 230 also gives platforms the legal blessing to govern as they see fit. Jimmy Wales, co-founder of Wikipedia, set up the Wikimedia Foundation to oversee the project and sister platforms in 2005, and it has remained volunteer-run. The Wikimedia Foundation has an endowment of more than 64 million, with tech titans such as Amazon pledging millions, and the Foundation supports projects by volunteers and affiliates. English Wikipedia has snowballed in popularity on a commercial internet. Google, for instance, prioritises Wikipedia articles in search results—treats them like “gospel” said Cohen, while the convenience, currency, and comprehensibility of Wikipedia attracts regular readers.\nUsing pageviews to tell a story\nData journalists can find the granular level of insight about pageviews handy for storytelling. Viewers of Wikipedia come from around the world. The Wikimedia Foundation does not track individual data, but tracks devices across pages. Data about what type of device—mobile app, mobile browser, or desktop browser—are used to access pages. This can give journalists insight into topical and regional access trends. \nMore radically, pageviews can reveal kernels of stories yet to be broken. Let’s simulate research using pageviews for a story on the rising COVID-19 case count in light of concerns about circulation of misinformation and disinformation on the virus. Digging into pageview data on COVID-19 articles in English Wikipedia can help to tell this story, and others like it. \nIn spring 2020, as unprecedented economic and social changes unfolded across the globe, journalists were at the forefront of providing coverage on this moment. Meanwhile, conspiracy theories were gaining visibility in social media groups, while edit counts and information queries about all articles related to COVID-19 were at their highest to date. \nBy mid-November 2020, a new trend. Positive cases of COVID-19 skyrocketed around the globe. Several European countries and U.S. states re-introduced lockdown measures to slow the spread of the virus. But Wikipedia pageviews for articles about COVID-19 were not rising, in fact, they were lower than earlier in the year. The election pageviews on the presidential candidates and their families were cresting with the U.S. election.\nA line graph above shows a pageview analysis from Nov 2019 to Oct. 2020 (x axis) depicting pageviews by the thousands (y axis) of four article pages: Donald_Trump, Coronavirus_disease_2019, Joe_Biden, and George_Floyd. Source: pageviews.toolforge.org/\nDid election coverage distract readers from the pandemic? Spikes in readership on Wikipedia are often the consequence of other media attention or events, which could help to explain for the peaks in views for George Floyd, Donald Trump and Joe Biden. \nKoerner, who trained as a social scientist, cautions journalists not to make quick deductions about readers' motivations from high-level pageview data. “It’s tricky to say that pageviews are indicative of what people are thinking,” she said. To dig into more granularity, journalists can dig in and compare sets of pageviews using the browser-optimised pageview visualisation tool available.\nAbove is a blue bar graph showing pageviews to the Symptoms of COVID-19 article page rising from October to November 22, 2020 (x axis) by the hundreds (y axis). Pageviews to the Symptoms of COVID-19 rose by hundreds in under two months.\nMeanwhile, pageviews of the COVID-19 general article may have peaked in the spring, but data journalists can note that pageviews of the article “Symptoms of the coronavirus” rose in October, as depicted above, before the peaking case numbers. Incidentally, this correlation could lend credence to the suggestion by a team of epidemiologists in 2014 that high pageview data about influenza-related Wikipedia articles could be used to make predictions about the percentage of Americans with influenza. While it remains to be seen if pageviews can predict illness spikes, the data can offer a wide lens on the zeitgeist.\nAbove is a list of the top 10 most viewed articles in 2019, in order of popularity, with lists of the number of edits and editors. Avengers: Endgame, Deaths in 2019, Wikipedia, Ted Bundy, Freddie Mercury, Chernobyl Disaster, and List of Highest-grossing films are top seven. Wikimedia Statistics provide high-level data on trends in pageviews, including top-viewed article pages. The data was accessed at Pageviews.toolforge.org.\nBehind the scenes\nWith approximately 300 edits per minute—which is soothing to listen to—Wikipedia is always changing. You may already have edited Wikipedia, the blue “edit” tab is on almost every article page. There are more than 1.2 billion speakers of English and over 40 million Wikipedia accounts.\nMaybe you made an account and your changes stuck. Maybe you tried to write an article, only to have it deleted. Or maybe you wondered about how easy it is to add profanity to an article on a popular topic—only to realise that the “Edit” tab is missing. Rather, there’s a lock. Or possibly, a gold star. \nLocks. Gold stars. Deletions. These are subtle signs and signals that can help you understand how the editing community works.\nAbove is a labelled diagram of the parts of a Wikipedia page using the example of the Black Lives Matter article. While every article page has these features, I've chosen to label the Black Lives Matter article because it is an extensive composite of the movement's history, it's been peer-reviewed by editors and is locked, which makes vandalism more difficult.\nWikipedia’s “best” are marked with green crosses and gold stars, these are Good and Featured content which have undergone “peer review.” They are the minority among Wikipedia's millions, just 0.1%. \nMeanwhile, the active editorial community on English Wikipedia monthly is about 4,000 editors. Fewer are administrators. As of November 2020, approximately 1,100 users have successfully undergone a “request for administratorship” and have been granted additional technical privileges, including the ability to delete and/or protect pages.  Non administrative editors, however, may patrol new pages and rollback recent changes. \nWikipedia’s editorial judgement can spark justified outrage. \nJournalist Stephen Harrison covered this recently in his Slate article on the Theresa Greenfield biography. While archivists, indigenous and feminist communities have noted the reliable source guidelines exclude oral histories, ephemera, and special collections; I am currently co-leading an Art+Feminism research project on marginalised communities and reliable source guidelines, funded by WikiCred which supports research, software projects and Wikimedia events on information reliability and credibility. Data journalists can follow debates on-wiki, and note what is absent, by looking at article Talk and View history tabs, and on notice boards for deletion and reliable sources. \nAt the same time, there’s plenty to be discovered with Wikipedia. Article features such as wikilinks, citations, and categories can help data journalists quickly access a living repository of information.\nAbove is a labelled diagram showing the wikilinks, citations, and categories using the example of the Black Lives Matter article. On Wikipedia, hyperlinks within articles generally lead to other Wikipedia articles, citations are footnotes with references listed at the end of an article. Categories may help journalists find other articles.\nIn 2011, an editor began a list that documented people killed by law enforcement in the United States, both on duty and off duty. Since 2015, the annual average number of justifiable homicides reported was estimated to be near 930. Tables about gun violence, have been collected on Wikipedia for nearly a decade.\nAbove shows a diagram of portions of the article list of killings by law enforcement officers in the United States, including a monthly table from pre-2009 to 2020. This Wikipedia list has amassed data sets from hundreds of sources that verify the killing of humans by law enforcement officers. Between 930 and 1,240 people are killed by police annually in the United States.\nThe integrity of this list was brought to my attention by Jennifer 8. Lee, a former New York Times journalist. She expressed surprise that there are not more examples of journalists using Wikipedia’s data. Lee would know, she co-founded the U.S.-based Credibility Coalition and MisinfoCon, and supports WikiCred, which addresses credibility in online information and includes Wikipedians, technologists, and journalists.\n“[These] are fascinating and useful,” said Lee. “Not automated, this is a hand-written list. It’s all in one place. This is useful for journalists and those of us in the credibility sphere to use it for research.” \nEd Erhart, who works with the Wikimedia Foundation’s audience engagement team, suggests that stories can not only be a repository but fodder for coverage. “I like to say that there is a story in every Wikipedia article,” he wrote by email, drawing my attention to a Featured article about a small town, Arlington, Washington. “Who wrote it? Where are they from? What motivated them? The talk and history tabs on Wikipedia's pages can be the starting point for some truly unique takes on local places and issues.”\nQuick links\nVisit pageview statistics homepage\nMore about page protection\nMore about user access levels\nMore about featured articles and lists\nIcon for featured articles is a star\nIcon for good articles is a green circle with a plus sign\nCatching malfeasance\nData journalists can follow edits to track corporate or governmental malfeasance. Article pages about companies or politicians can be scrubbed to omit negative information. Though editors are required to disclose conflicts of interest on their user page or in the Article's talk page.\nUsers who edit Wikipedia as a part of their paid work are required to disclose conflicts of interest. This image shows an example of a user who has done so. John P. Sadowski edits articles on biomedical topics, including articles related to COVID-19, using resources from his employer, the U.S. Center for Disease Control (CDC). Not all contributors do this.\nNot all contributors disclose. Kaylea Champion, a doctoral student at the University of Washington, led a large scale research project on IP editing and discovered systematic deletions to mining articles. Anonymous editors removed information about environmental contamination and abuse. Champion and her co-authors traced the IP addresses that deleted the incriminating information to the headquarters of the mining companies. \nJournalists can do their own large-scale reconstructions of edit histories using data from Wikipedia’s data dump, or manually browse pages of interest. Historical contributions can all be accessed, even if they are not visible on the live page. As well, journalists can reach out to editors by writing a note on their Talk page with information on how to connect.\nThe below GIF demonstrates how to access View History and compare versions of the Black Lives Matter article page, using the Compare Version History tool. Be sure to use the View History tab to compare version histories, which is shown above. You can also click on the timestamp to view an article in full.\nQuick links \nWikipedia’s API \nData dumps with complete copies of all Wikipedias\nTracking with bots\nBots can help with tracking. In 2014, a number of bots were launched by volunteers to track edits made by specific IP ranges and posted the findings to Twitter. Parliament WikiEdits, one of the first, still regularly tweets edits made to Parliamentary IPs in the UK. Similar efforts have been available for The White House, European Union, Norwegian Parliament, German Parliament, Goldman Sachs and Monsanto Company, though not all are up to date. \nFor data journalists interested in setting up a bot that tweets about anonymous Wikipedia edits from particular IP address ranges in their beat, the code is available from Ed Summers on GitHub under a CC0 license.\nData journalists should weigh the public benefit of amplifying hate speech, harassment, or vandalism, which could be a form of coded language, with reporting.\nPitfalls to avoid: steering clear of media manipulation\nSummers created @CongressEdits in 2014, which tweeted IP contributions from U.S. capitol computers. The Wikipedian reported that “Twitter-addicted journalists” soon were mining the bots for story ideas -- some of which did reveal manipulation, such as an attempt to water down the entry on CIA torture. @CongressEdits amassed a growing audience.  Things came to a head in 2018. A former Democratic staffer (who was later arrested) with access to the U.S. capitol computers inserted personal information to Wikipedia articles about Republican members of the Senate Judiciary Committee. The Twitter account automatically shared out those details with the large following. Twitter banned the bot as a result.\nPeople can intentionally game the editorial system or interconnections between Wikipedia and other social media platforms. Data journalists should weigh the public benefit of amplifying hate speech, harassment, or vandalism, which could be a form of coded language, with reporting. “Why are people editing articles to say that the [mainstream political party] is [name of radical, violent party]? They want the screenshot,” Cohen remarked. “The best way to get a lie into [the] mainstream is to edit an article, let Google pick it up, and get reporting on it. It’s probably a thrill to plant them.”\nFurthermore, Wikipedia has no “real name” policy for editors. Some choose to disclose personal details on user pages, which can help gain the confidence of other editors, but this is not required. Thus, manipulators can mimic the behaviour patterns of a group to blend in.\nJoan Donovan, director of Technology and Social Change at Harvard Kennedy School’s Shorenstein Center, calls this a “butterfly attack.” Once the fakes are indistinguishable to outsiders from legitimate accounts, the manipulators push contentious issues to divide and delegitimise the group. Be mindful that you are not also falling for a “butterfly attack”—or perpetuating one by accidentally characterising editors as occupying one particular position over another. Instead, get to know the communities behind the data to minimise harm.  \nIf you discover vandalism or hate-speech on a page history, consider the impact of your coverage on a topic that has since disappeared. Be mindful of the extent to which the effort at public service can dually serve as a form of publicity or exposure for people sympathetic with fringe ideologies or violence. Reporters who stumble across data on hate-speech might report on this in aggregate, without identifying particular details, to minimise harm.\nPro tips for navigating Wikipedia:\nGet to know Wikipedia’s editorial process and community before reporting on hate speech or harassment\nStrongly consider the newsworthiness of articles that might give publicity to fringe ideologies\nUse data in aggregate to avoid revealing details\nCircular reporting\nIn 2007, The Independent published an article on Sasha Baron Cohen that included a line that he had previously worked as an investment banker. Days earlier, the claim had appeared in Wikipedia, and was unverified. Later, The Independent’s article became the citation for the erroneous claim. \nNone of it was true. And Wikipedia editors call incidents like this “citogenesis,” or circular reporting. There is even a Wikipedia article that compiles known instances. Techdebug blog depicted the Baron Cohen example with the good advice to “pay attention to timelines” when reviewing sources of claims on Wikipedia. When using facts from Wikipedia, trust but verify.\nWith close attention to detail and context, data journalists can use Wikipedia’s trove of data to elucidate stories of the digital landscape. “Wikipedia is more than the sum of its parts” said Cohen. “Random encounters are often more compelling than the articles themselves. The search for information resembles a walk through an overbuilt quarter of an ancient capital. You circle around topics on a path that appears to be shifting. Ultimately, the journey ends and you are not sure how you got there.”\nMonika Sengul-Jones, PhD, is a freelance researcher, writer and expert on digital cultures and media industries. She was the OCLC Wikipedian-in-Residence in 2018-19. In 2020, she is co-leading Reading Together: Reliable Sources and Multilingual Communities, an Art+Feminism project on reliable sources and marginalised communities funded by WikiCred. @monikajones, www.monikasjones.com\nThanks to Mohammed Sadat Abdulai (Art+Feminism, Wikimedia Deutschland), Ahmed Median (Hack/Hackers), and Kevin Payravi (WikiCred, Wikimedia D.C.), and for taking time to interview with me for background research for this story.\nData visualisation by hand: drawing data for your next story\nA data journalists guide to building a hypothesis\nThe promise of WikiData as a data source for journalists\nMaking numbers louder: telling stories with sound\nConflict reporting with data\nOwn your newsfeed, own your data","guid":"https://datajournalism.com/read/longreads/harnessing-wikipedias-superpowers-for-journalism","isoDate":"2020-12-02T06:00:00.000Z","blogTitle":"DataJournalism.com"}},"__N_SSG":true}