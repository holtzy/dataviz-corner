<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/7b0d10d140b8a0a5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/7b0d10d140b8a0a5.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-59c5c889f52620d6.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-f11614d8aa7ee555.js" defer=""></script><script src="/_next/static/chunks/pages/_app-694ee0f2821639fc.js" defer=""></script><script src="/_next/static/chunks/996-eeb5175dbd5dba8f.js" defer=""></script><script src="/_next/static/chunks/36-94b5e24e03efc6db.js" defer=""></script><script src="/_next/static/chunks/pages/%5B...slug%5D-af748dcc13a25fcb.js" defer=""></script><script src="/_next/static/yP12RsHv5AtYmalih9fUM/_buildManifest.js" defer=""></script><script src="/_next/static/yP12RsHv5AtYmalih9fUM/_ssgManifest.js" defer=""></script></head><body><div id="__next"></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"creator":"Maud Beelman","title":"Data journalism: a guide for editors","link":"https://datajournalism.com/read/longreads/data-journalism-a-guide-for-editors","pubDate":"Wed, 30 Oct 2019 09:56:00 +0100","author":"Maud Beelman","content":"\n                                                                        \u003cp\u003eThe best ‘data stories’ are not obvious. They don’t hit the reader over the head with numbers, at least not initially. But the data is the very foundation on which the story is built, and it can help guide reporters to the best anecdotes or ways to illustrate their findings.\u003c/p\u003e\n\u003cp\u003eGood data editing requires an understanding of all that, along with critical thinking,  project management skills, and a better-than-average understanding of the content, \u003ca href=\"https://datajournalism.com/read/longreads/putting-data-back-into-context\"\u003econtext\u003c/a\u003e, and organisation of the data.\u003c/p\u003e\n\u003cp\u003eJust as all investigations need a process that works for everyone, so do data investigations. Knowing the process can help editors ask the right questions and backstop  reporters. It’s also important (and helpful to the overall story) for all team members to understand the methodology, regardless of whether they’ll be working with the data. \u003c/p\u003e\n\u003cp\u003eGood data projects have good workflows, which depend in part on backout scheduling. Know your publication deadline, and then back up each important prior mark that must be hit. Data-based investigations have more moving parts and more predicates, so it’s vital that everyone knows the order in which the work must get done.\u003c/p\u003e\n\n                                                                     \n                            \u003ch1\u003eGetting started: guidance for journalists and their editors\u003c/h1\u003e\n\u003cp\u003eJournalists, start as you would any other investigation: with initial research and reporting to identify what data and documents exist. If you get stuck, look for examples of similar reporting (these can be found in the \u003ca href=\"https://www.ire.org/resource-center\"\u003eIRE Resource Center\u003c/a\u003e) or search scientific and academic works to identify experts who have done similar work or share the same interest in your topic. Often such sources can help you streamline your preliminary data research.\u003c/p\u003e\n\u003cp\u003eIt’s important to remember that initial story memos or pitches should include information about what data is available and how that data might help you tell the story.\u003c/p\u003e\n\u003cp\u003eOnce the data is identified and acquired or assembled (more on that later), it and any notes should be somewhere the entire team can access during the reporting and editing process. The key is to keep everyone in the loop so there are no surprises. When working on the data analysis, avoid using email to track changes or make updates, which can get lost or be confusing. If you have a project management system in your newsroom, you may want to use that, but tools such as Github or Google docs also work well.\u003c/p\u003e\n\u003cp\u003eIf you’re an editor who doesn’t use programming, you should still make sure scripts contain comments, explaining what the code does, so that another person could follow along and understand your reporter’s data and methodology. \u003c/p\u003e\n\u003cp\u003eYou also need to have an internal process for independently double-checking the data analysis. And keeping a data diary is essential to this end. For example, if you have a large enough team, one reporter could be the backstop for another. You also might ask a trusted colleague outside the organisation to be your backstop.\u003c/p\u003e\n\n                                                                                                  \n                                \u003cp\u003eData diaries can take many forms. Here’s one example.\u003c/p\u003e\n\n                                                                                        \u003cp\u003eData diaries also come in handy for keeping track of file names, code, and syntax, and footnoting and lawyering copy. Mostly, they need to be clear enough that colleagues, including the editor, can make sense of the work.\u003c/p\u003e\n\n                                                                                                  \n                            \u003ch1\u003eBulletproofing the data and its analysis\u003c/h1\u003e\n\u003cp\u003eBulletproofing a data driven investigation begins with bulletproofing the data before starting the analysis. This is important because often the simplest problems get overlooked, such as not having all of the relevant records. Editors who know this can backstop their reporters’ practices and ensure that their stories are set up for success from the get-go.\u003c/p\u003e\n\u003cp\u003eIn addition, you should always work off a copy, instead of the original data, in case something bad happens while you’re doing the analysis, such as your computer dies or you accidentally introduce an error into the data. As you do your checks, always keep notes on what you found -- it will help you later when you do your analysis. Here are the checks that data journalists should  conduct on every dataset, which can also be used by editors to backstop analyses:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCheck that you have all of the relevant records. It’s easy for an agency to accidentally miss some records, either by copying and pasting data or reusing an old query that pulls only certain records. If there is no reference for the exact correct number, use common sense. Would it be reasonable that the United States would have only 80,000 voters?  \u003c/li\u003e\n\u003cli\u003eMake sure all locations, such as cities or counties are included.\u003c/li\u003e\n\u003cli\u003eLook for inconsistencies in key fields. For example, are city names spelled the same way? It’s important because it could affect your results. You can do this check by getting a list of all unique possibilities within a given field and sorting them alphabetically.\u003c/li\u003e\n\u003cli\u003eMake sure that numeric fields are within valid ranges. For example, does your data include dates of birth that would make individuals too young or too old?\u003c/li\u003e\n\u003cli\u003eCheck for missing data or blank fields. Make sure that you did not cause these problems by importing data incorrectly. Look at the file in a text editor to be sure.\u003c/li\u003e\n\u003cli\u003eDouble-check totals or counts against summary reports from the agency.\u003c/li\u003e\n\u003cli\u003eKnow your data. Know what every field means and how the agency uses it. Something that looks boring to you could be critical to your analysis.\u003c/li\u003e\n\u003cli\u003eTalk with the folks who work with the data and ask them about the checks they do. \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOnce you’ve checked your data, you’re ready to do your analysis. Keeping notes about what you do will be crucial at this stage. Those notes will help you write your methodology later and will help you (and your editor) vet the findings. As you go along with your analysis, be sure to regularly back up your data and use a naming convention that makes sense to you and to others who may use the data. Here are a few other tips to keep in mind as you undertake your analysis:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMake sure you’re using the right tool. You may need to do more than counting and sorting.\u003c/li\u003e\n\u003cli\u003eCheck with experts from different sides of the issue about your methods and your findings.\u003c/li\u003e\n\u003cli\u003eBeware of lurking variables. The trend you found could be caused by an underlying variable you haven’t considered.\u003c/li\u003e\n\u003cli\u003eIf you think you’re in over your head, call on an expert to help. Don’t guess or assume.\u003c/li\u003e\n\u003cli\u003eDouble-check surprising results. For example, if citations spiked by 50% in one year, it could be a story or it could (more likely) be an error.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOften data ‘analyses’ are \u003ca href=\"https://datajournalism.com/read/longreads/spreadsheets-for-journalism\"\u003ecounting or summing data\u003c/a\u003e, but if you need to do a more complex analysis, here are some suggestions to help you figure out the best methodology: \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRead research reports. Academic research on your topic might reveal best practices for working with your data.\u003c/li\u003e\n\u003cli\u003eFind an expert to vet your methodology. Many are happy to help, especially once they realise you’re interested in doing a serious analysis. When the Dallas Morning News \u003ca href=\"http://www.jenster.com/juries.pdf\"\u003eexamined\u003c/a\u003e jury strikes, one of the leading experts on bias in jury selection reviewed all of the reporting teams’ findings.\u003c/li\u003e\n\u003cli\u003eShow findings to the targets of the story. We’re not suggesting sharing your story, but you should put together a findings document or presentation that you can share with the targets of the story. This helps bulletproof your methodology by surfacing any problems that may exist (or variables you didn’t consider) before publication. \u003c/li\u003e\n\u003cli\u003eDuplicate your work. To make sure you didn’t mess something up along the way. Don’t just rerun original scripts, recreate them so you know they were done correctly the first time.\u003c/li\u003e\n\u003cli\u003eMaintain a consistent universe of cases. If you have to filter or redefine your universe, be able to explain why you isolated certain records or cases.\u003c/li\u003e\n\u003cli\u003eGive yourself enough time to follow through on collecting information for your database before you start writing. If you’ve \u003ca href=\"https://datajournalism.com/read/handbook/two/experiencing-data/searchable-databases-as-a-journalistic-product\"\u003ebuilt your own database\u003c/a\u003e, where information may need to be updated or will change after additional reporting, set a cut-off date and don’t make any more changes to the database unless the data is inaccurate or the new information will change the meaning of the story.\u003c/li\u003e\n\u003cli\u003eIf you are doing the data entry yourself, make sure at least two people have reviewed every record, or consider hiring a data-entry firm that uses double-entry verification.\u003c/li\u003e\n\u003c/ul\u003e\n\n                                                                      \n                            \u003ch1\u003eBulletproofing the process\u003c/h1\u003e\n\u003cp\u003eEditors of data investigations must ask even more questions than usual and do their own research. In much the same way that the reporter may have identified similar works of journalism or scientific studies, editors should familiarise themselves with those methodologies as well.\u003c/p\u003e\n\u003cp\u003eAlso, it’s essential for editors to know and understand a database’s ‘record layout’, or more simply put, the kind of information that is contained in the data and how is it broken down and organised. If there is a ‘read me’ file that accompanies the data, which often describes known quirks or problems, it’s the editor’s responsibility as much as the reporter’s to read and understand those details.\u003c/p\u003e\n\u003cp\u003eYou should discuss known or suspected problems in the data with your reporters and the whole project team. In fact, even if your reporters don’t bring them to you, ask what the problems are, because data always has problems. Listen to your reporters carefully, and have regular check-ins with them to see what’s worrying them. Look at the data yourself, or if you’re not conversant in the software, ask the reporter to give you a guided tour of the data. Don’t be shy about challenging the data, if there’s anything you don’t understand or that doesn’t pass the sniff test. Encourage creative thinking and brainstorm solutions.\u003c/p\u003e\n\u003cp\u003eFinally, have your reporters write their methodology (or a ‘white paper’ if it’s a long and complicated analysis) before they start drafting any story. Most often, methodologies (aka the ‘nerd box’) are written at the end of a project drafting process. But it’s not at all uncommon -- once a reporter has to explain all the details of how they conducted the analysis -- that the story language needs to change. Once a detailed methodology is written, it’s even possible that you find some misunderstandings amongst the team over what was done and how. It’s better to surface these issues before the writing begins.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eA methodology story by the Associated Press.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eHere are a couple of examples of handling methodology in copy. Aside from including a few paragraphs in your story, which is how simple methodologies can be handled, you can write a separate short \u003ca href=\"https://apnews.com/3af4a442bb5b4249a70abe0b6a6af50a\"\u003estory\u003c/a\u003e on what you did. You can also produce a more detailed \u003ca href=\"https://www.propublica.org/article/how-propublica-analyzed-pardon-data\"\u003ewhite paper\u003c/a\u003e, which allows you to go into great detail on a complex analysis and can have the effect of creating greater confidence and transparency around your work.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eTo sum up, here are 10 questions every editor should ask:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eDoes the data answer our questions? Does it surface other questions? \u003c/li\u003e\n\u003cli\u003eWhere did you find the data? \u003c/li\u003e\n\u003cli\u003eHow did you vet and clean the data? \u003c/li\u003e\n\u003cli\u003eHow did you calculate those numbers?\u003c/li\u003e\n\u003cli\u003eAre you keeping a data diary? \u003c/li\u003e\n\u003cli\u003eDid you replicate your data work? Could someone else? \u003c/li\u003e\n\u003cli\u003eHave you consulted experts or done a scientific literature review?\u003c/li\u003e\n\u003cli\u003eDo we need a white paper?\u003c/li\u003e\n\u003cli\u003eCould you write a nerd graf/story if asked to?\u003c/li\u003e\n\u003cli\u003eWhat is the significance of the data? (Don’t confuse effort with importance.)\u003c/li\u003e\n\u003c/ol\u003e\n\n                                                                      \n                            \u003ch1\u003eWriting the data story\u003c/h1\u003e\n\u003cp\u003eAs we mentioned at the start, the best data stories are not data heavy. They don’t ask readers to ‘do the math’, and they don’t subject the narrative to a lot of numbers. They tell the story or stories that the data has surfaced, through interesting characters or circumstances. As you guide your reporters through the writing phase, consider some of the below examples.\u003c/p\u003e\n\u003cp\u003eTake, for instance, this Associated Press\u003ca href=\"https://www.apnews.com/86ba45b0a4ad443fad1214622d13e6cb\"\u003e story\u003c/a\u003e that was part of a \u003ca href=\"https://www.icij.org/investigations/implant-files/\"\u003eglobal investigation\u003c/a\u003e into medical implants, led in 2018 by the International Consortium of Investigative Journalists.\u003c/p\u003e\n\u003cp\u003eNot until the sixth paragraph do the writers introduce the idea that the topic of the story -- spinal cord stimulators -- is being examined because the devices rank among the top of those causing patient harm. In fact, the sixth and seventh paragraphs help form the nut grafs of the story: often where you find data first appearing in stories that take a narrative approach:\u003c/p\u003e\n\n                                                                                                 \u003cp\u003e\u003cem\u003e“But the stimulators — devices that use electrical currents to block pain signals before they reach the brain — are more dangerous than many patients know, an Associated Press investigation found. They account for the third-highest number of medical device injury reports to the U.S. Food and Drug Administration, with more than 80,000 incidents flagged since 2008.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003ePatients report that they have been shocked or burned or have suffered spinal-cord nerve damage ranging from muscle weakness to paraplegia, FDA data shows. Among the 4,000 types of devices tracked by the FDA, only metal hip replacements and insulin pumps have logged more injury reports.”\u003c/em\u003e\u003c/p\u003e\n\n                                                                     \n                            \u003cp\u003eThis is how The Philadelphia Inquirer started its story on the findings of a year-long investigation into how children in public schools were suffering from environmental poisoning:\u003c/p\u003e\n\n                                                                                                 \u003cp\u003e\u003cem\u003e“Day after day last September, toxic lead paint chips fluttered from the ceiling of a first-grade classroom and landed on the desk of 6-year-old Dean Pagan.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eDean didn’t want his desk to look messy. But he feared that if he got up to toss the paint slivers in the trash, he’d get in trouble.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eSo he put them in his mouth. And swallowed them.”\u003c/em\u003e\u003c/p\u003e\n\n                                                                     \n                            \u003cp\u003eThere’s no indication in the opening paragraphs that these findings are the result of a data analysis until the eighth and ninth paragraphs:\u003c/p\u003e\n\n                                                                                                 \u003cp\u003e“As part of its “Toxic City” series, the Inquirer and Daily News investigated the physical conditions at district-run schools. Reporters examined five years of internal maintenance logs and building records, and interviewed 120 teachers, nurses, parents, students, and experts.*\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eWhen the newspapers analyzed the district records, they identified more than 9,000 environmental problems since September 2015. They reveal filthy schools and unsafe conditions — mold, deteriorated asbestos, and acres of flaking and peeling paint likely containing lead — that put children at risk.”\u003c/em\u003e\u003c/p\u003e\n\n                                                                     \n                            \u003cp\u003eThis is not to say that using the findings of a data analysis in your lead is always a bad idea. Each story, including its methodology and findings, needs to dictate the best approach to follow. Consider these examples:\u003c/p\u003e\n\u003cp\u003eThe Post and Courier in South Carolina won a Pulitzer Prize for its 2014 domestic violence investigation, in which the data analysis was the lead because the numbers were so startling:\u003c/p\u003e\n\n                                                                                                 \u003cp\u003e\u003cem\u003e“More than 300 women were shot, stabbed, strangled, beaten, bludgeoned or burned to death over the past decade by men in South Carolina, dying at a rate of one every 12 days while the state does little to stem the carnage from domestic abuse.”\u003c/em\u003e\u003c/p\u003e\n\n                                                                     \n                            \u003cp\u003eCompared to this narrative-based example from ESPN, on what ends up in some US stadium foods:\u003c/p\u003e\n\n                                                                                                 \u003cp\u003e\u003cem\u003e“Most Cracker Jack boxes come with a surprise inside. At Coors Field in Denver, the molasses-flavored popcorn and peanut snacks came with a live mouse.”\u003c/em\u003e\u003c/p\u003e\n\n                                                                     \n                            \u003cp\u003eIllustrations, graphics, and videos are often your best friends in presenting data stories, as they can do the heavy lifting of the analysis, allowing your reporter’s storytelling (in any format) to flourish in the findings, not drown in the data.\u003c/p\u003e\n\u003cp\u003eSome examples below illustrate how data can achieve storytelling goals: \u003c/p\u003e\n\u003cp\u003eA collaborative investigation into the death toll in Puerto Rico caused by Hurricane Maria, which was named 2019 investigation of the year by the Data Journalism Awards. The powerful interactive embedded in the story showed how the numbers grew beyond initial reports. Greatly. It also included a \u003ca href=\"https://hurricanemariasdead.com/database.html#\"\u003esearchable database\u003c/a\u003e with profiles of the dead.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eQuartz and Puerto Rico’s Center for Investigative Journalism.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eA look at how thin models must be to walk the catwalk by NOS, Netherlands, which uses a combination of video and graphics to illustrate the findings of an analysis into the sizes of 1000+ models.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003e\u003ciframe width=\"877\" height=\"493\" src=\"https://www.youtube-nocookie.com/embed/DWRGqmywNYs\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\u003c/p\u003e\n\n                                                                                                                                                             \u003cp\u003e\u003ca href=\"https://www.reuters.com/investigates/section/ocean-shock/\"\u003eOcean Shock\u003c/a\u003e, a Reuters investigation into the effect of the climate crisis on marine life, and a stunning and engaging visual presentation of data.\u003c/p\u003e\n\u003cp\u003eESPN’s 2018 \u003ca href=\"http://www.espn.com/espn/feature/story/_/id/25316231/health-inspection-reports-find-critical-violations-nfl-nhl-nba-mlb-stadiums-2018-espn-lines\"\u003eanalysis\u003c/a\u003e of food-safety inspection reports for professional sports venues; this powerful data presentation allowed writers to deliver up the mouse lead above. The graphics provide lots of numbers without overwhelming the reader.\u003c/p\u003e\n\n                                                                                                  \n                            \u003ch1\u003eOne last thing...\u003c/h1\u003e\n\u003cp\u003eEarlier, we referenced the possibility of doing an investigation based on data you assemble and analyse yourself. \u003c/p\u003e\n\u003cp\u003eIn many ways, that is the most original form of data investigation because you’re not analysing some other entity’s information but rather doing the ground-up reporting that will give you truly unique findings. \u003c/p\u003e\n\u003cp\u003eThat’s the up side. The downside is that this form of investigative data analysis is extremely labor intensive and fraught with potential methodology questions and errors. It requires more time and greater levels of bulletproofing from reporters and their editors, so plan accordingly if you decide it’s the only way to answer that burning question.\u003c/p\u003e\n\n                                              \n                ","contentSnippet":"The best ‘data stories’ are not obvious. They don’t hit the reader over the head with numbers, at least not initially. But the data is the very foundation on which the story is built, and it can help guide reporters to the best anecdotes or ways to illustrate their findings.\nGood data editing requires an understanding of all that, along with critical thinking,  project management skills, and a better-than-average understanding of the content, context, and organisation of the data.\nJust as all investigations need a process that works for everyone, so do data investigations. Knowing the process can help editors ask the right questions and backstop  reporters. It’s also important (and helpful to the overall story) for all team members to understand the methodology, regardless of whether they’ll be working with the data. \nGood data projects have good workflows, which depend in part on backout scheduling. Know your publication deadline, and then back up each important prior mark that must be hit. Data-based investigations have more moving parts and more predicates, so it’s vital that everyone knows the order in which the work must get done.\nGetting started: guidance for journalists and their editors\nJournalists, start as you would any other investigation: with initial research and reporting to identify what data and documents exist. If you get stuck, look for examples of similar reporting (these can be found in the IRE Resource Center) or search scientific and academic works to identify experts who have done similar work or share the same interest in your topic. Often such sources can help you streamline your preliminary data research.\nIt’s important to remember that initial story memos or pitches should include information about what data is available and how that data might help you tell the story.\nOnce the data is identified and acquired or assembled (more on that later), it and any notes should be somewhere the entire team can access during the reporting and editing process. The key is to keep everyone in the loop so there are no surprises. When working on the data analysis, avoid using email to track changes or make updates, which can get lost or be confusing. If you have a project management system in your newsroom, you may want to use that, but tools such as Github or Google docs also work well.\nIf you’re an editor who doesn’t use programming, you should still make sure scripts contain comments, explaining what the code does, so that another person could follow along and understand your reporter’s data and methodology. \nYou also need to have an internal process for independently double-checking the data analysis. And keeping a data diary is essential to this end. For example, if you have a large enough team, one reporter could be the backstop for another. You also might ask a trusted colleague outside the organisation to be your backstop.\nData diaries can take many forms. Here’s one example.\nData diaries also come in handy for keeping track of file names, code, and syntax, and footnoting and lawyering copy. Mostly, they need to be clear enough that colleagues, including the editor, can make sense of the work.\nBulletproofing the data and its analysis\nBulletproofing a data driven investigation begins with bulletproofing the data before starting the analysis. This is important because often the simplest problems get overlooked, such as not having all of the relevant records. Editors who know this can backstop their reporters’ practices and ensure that their stories are set up for success from the get-go.\nIn addition, you should always work off a copy, instead of the original data, in case something bad happens while you’re doing the analysis, such as your computer dies or you accidentally introduce an error into the data. As you do your checks, always keep notes on what you found -- it will help you later when you do your analysis. Here are the checks that data journalists should  conduct on every dataset, which can also be used by editors to backstop analyses:\nCheck that you have all of the relevant records. It’s easy for an agency to accidentally miss some records, either by copying and pasting data or reusing an old query that pulls only certain records. If there is no reference for the exact correct number, use common sense. Would it be reasonable that the United States would have only 80,000 voters?  \nMake sure all locations, such as cities or counties are included.\nLook for inconsistencies in key fields. For example, are city names spelled the same way? It’s important because it could affect your results. You can do this check by getting a list of all unique possibilities within a given field and sorting them alphabetically.\nMake sure that numeric fields are within valid ranges. For example, does your data include dates of birth that would make individuals too young or too old?\nCheck for missing data or blank fields. Make sure that you did not cause these problems by importing data incorrectly. Look at the file in a text editor to be sure.\nDouble-check totals or counts against summary reports from the agency.\nKnow your data. Know what every field means and how the agency uses it. Something that looks boring to you could be critical to your analysis.\nTalk with the folks who work with the data and ask them about the checks they do. \nOnce you’ve checked your data, you’re ready to do your analysis. Keeping notes about what you do will be crucial at this stage. Those notes will help you write your methodology later and will help you (and your editor) vet the findings. As you go along with your analysis, be sure to regularly back up your data and use a naming convention that makes sense to you and to others who may use the data. Here are a few other tips to keep in mind as you undertake your analysis:\nMake sure you’re using the right tool. You may need to do more than counting and sorting.\nCheck with experts from different sides of the issue about your methods and your findings.\nBeware of lurking variables. The trend you found could be caused by an underlying variable you haven’t considered.\nIf you think you’re in over your head, call on an expert to help. Don’t guess or assume.\nDouble-check surprising results. For example, if citations spiked by 50% in one year, it could be a story or it could (more likely) be an error.\nOften data ‘analyses’ are counting or summing data, but if you need to do a more complex analysis, here are some suggestions to help you figure out the best methodology: \nRead research reports. Academic research on your topic might reveal best practices for working with your data.\nFind an expert to vet your methodology. Many are happy to help, especially once they realise you’re interested in doing a serious analysis. When the Dallas Morning News examined jury strikes, one of the leading experts on bias in jury selection reviewed all of the reporting teams’ findings.\nShow findings to the targets of the story. We’re not suggesting sharing your story, but you should put together a findings document or presentation that you can share with the targets of the story. This helps bulletproof your methodology by surfacing any problems that may exist (or variables you didn’t consider) before publication. \nDuplicate your work. To make sure you didn’t mess something up along the way. Don’t just rerun original scripts, recreate them so you know they were done correctly the first time.\nMaintain a consistent universe of cases. If you have to filter or redefine your universe, be able to explain why you isolated certain records or cases.\nGive yourself enough time to follow through on collecting information for your database before you start writing. If you’ve built your own database, where information may need to be updated or will change after additional reporting, set a cut-off date and don’t make any more changes to the database unless the data is inaccurate or the new information will change the meaning of the story.\nIf you are doing the data entry yourself, make sure at least two people have reviewed every record, or consider hiring a data-entry firm that uses double-entry verification.\nBulletproofing the process\nEditors of data investigations must ask even more questions than usual and do their own research. In much the same way that the reporter may have identified similar works of journalism or scientific studies, editors should familiarise themselves with those methodologies as well.\nAlso, it’s essential for editors to know and understand a database’s ‘record layout’, or more simply put, the kind of information that is contained in the data and how is it broken down and organised. If there is a ‘read me’ file that accompanies the data, which often describes known quirks or problems, it’s the editor’s responsibility as much as the reporter’s to read and understand those details.\nYou should discuss known or suspected problems in the data with your reporters and the whole project team. In fact, even if your reporters don’t bring them to you, ask what the problems are, because data always has problems. Listen to your reporters carefully, and have regular check-ins with them to see what’s worrying them. Look at the data yourself, or if you’re not conversant in the software, ask the reporter to give you a guided tour of the data. Don’t be shy about challenging the data, if there’s anything you don’t understand or that doesn’t pass the sniff test. Encourage creative thinking and brainstorm solutions.\nFinally, have your reporters write their methodology (or a ‘white paper’ if it’s a long and complicated analysis) before they start drafting any story. Most often, methodologies (aka the ‘nerd box’) are written at the end of a project drafting process. But it’s not at all uncommon -- once a reporter has to explain all the details of how they conducted the analysis -- that the story language needs to change. Once a detailed methodology is written, it’s even possible that you find some misunderstandings amongst the team over what was done and how. It’s better to surface these issues before the writing begins.\nA methodology story by the Associated Press.\nHere are a couple of examples of handling methodology in copy. Aside from including a few paragraphs in your story, which is how simple methodologies can be handled, you can write a separate short story on what you did. You can also produce a more detailed white paper, which allows you to go into great detail on a complex analysis and can have the effect of creating greater confidence and transparency around your work.\nTo sum up, here are 10 questions every editor should ask:\nDoes the data answer our questions? Does it surface other questions? \nWhere did you find the data? \nHow did you vet and clean the data? \nHow did you calculate those numbers?\nAre you keeping a data diary? \nDid you replicate your data work? Could someone else? \nHave you consulted experts or done a scientific literature review?\nDo we need a white paper?\nCould you write a nerd graf/story if asked to?\nWhat is the significance of the data? (Don’t confuse effort with importance.)\nWriting the data story\nAs we mentioned at the start, the best data stories are not data heavy. They don’t ask readers to ‘do the math’, and they don’t subject the narrative to a lot of numbers. They tell the story or stories that the data has surfaced, through interesting characters or circumstances. As you guide your reporters through the writing phase, consider some of the below examples.\nTake, for instance, this Associated Press story that was part of a global investigation into medical implants, led in 2018 by the International Consortium of Investigative Journalists.\nNot until the sixth paragraph do the writers introduce the idea that the topic of the story -- spinal cord stimulators -- is being examined because the devices rank among the top of those causing patient harm. In fact, the sixth and seventh paragraphs help form the nut grafs of the story: often where you find data first appearing in stories that take a narrative approach:\n“But the stimulators — devices that use electrical currents to block pain signals before they reach the brain — are more dangerous than many patients know, an Associated Press investigation found. They account for the third-highest number of medical device injury reports to the U.S. Food and Drug Administration, with more than 80,000 incidents flagged since 2008.\nPatients report that they have been shocked or burned or have suffered spinal-cord nerve damage ranging from muscle weakness to paraplegia, FDA data shows. Among the 4,000 types of devices tracked by the FDA, only metal hip replacements and insulin pumps have logged more injury reports.”\nThis is how The Philadelphia Inquirer started its story on the findings of a year-long investigation into how children in public schools were suffering from environmental poisoning:\n“Day after day last September, toxic lead paint chips fluttered from the ceiling of a first-grade classroom and landed on the desk of 6-year-old Dean Pagan.\nDean didn’t want his desk to look messy. But he feared that if he got up to toss the paint slivers in the trash, he’d get in trouble.\nSo he put them in his mouth. And swallowed them.”\nThere’s no indication in the opening paragraphs that these findings are the result of a data analysis until the eighth and ninth paragraphs:\n“As part of its “Toxic City” series, the Inquirer and Daily News investigated the physical conditions at district-run schools. Reporters examined five years of internal maintenance logs and building records, and interviewed 120 teachers, nurses, parents, students, and experts.*\nWhen the newspapers analyzed the district records, they identified more than 9,000 environmental problems since September 2015. They reveal filthy schools and unsafe conditions — mold, deteriorated asbestos, and acres of flaking and peeling paint likely containing lead — that put children at risk.”\nThis is not to say that using the findings of a data analysis in your lead is always a bad idea. Each story, including its methodology and findings, needs to dictate the best approach to follow. Consider these examples:\nThe Post and Courier in South Carolina won a Pulitzer Prize for its 2014 domestic violence investigation, in which the data analysis was the lead because the numbers were so startling:\n“More than 300 women were shot, stabbed, strangled, beaten, bludgeoned or burned to death over the past decade by men in South Carolina, dying at a rate of one every 12 days while the state does little to stem the carnage from domestic abuse.”\nCompared to this narrative-based example from ESPN, on what ends up in some US stadium foods:\n“Most Cracker Jack boxes come with a surprise inside. At Coors Field in Denver, the molasses-flavored popcorn and peanut snacks came with a live mouse.”\nIllustrations, graphics, and videos are often your best friends in presenting data stories, as they can do the heavy lifting of the analysis, allowing your reporter’s storytelling (in any format) to flourish in the findings, not drown in the data.\nSome examples below illustrate how data can achieve storytelling goals: \nA collaborative investigation into the death toll in Puerto Rico caused by Hurricane Maria, which was named 2019 investigation of the year by the Data Journalism Awards. The powerful interactive embedded in the story showed how the numbers grew beyond initial reports. Greatly. It also included a searchable database with profiles of the dead.\nQuartz and Puerto Rico’s Center for Investigative Journalism.\nA look at how thin models must be to walk the catwalk by NOS, Netherlands, which uses a combination of video and graphics to illustrate the findings of an analysis into the sizes of 1000+ models.\n\nOcean Shock, a Reuters investigation into the effect of the climate crisis on marine life, and a stunning and engaging visual presentation of data.\nESPN’s 2018 analysis of food-safety inspection reports for professional sports venues; this powerful data presentation allowed writers to deliver up the mouse lead above. The graphics provide lots of numbers without overwhelming the reader.\nOne last thing...\nEarlier, we referenced the possibility of doing an investigation based on data you assemble and analyse yourself. \nIn many ways, that is the most original form of data investigation because you’re not analysing some other entity’s information but rather doing the ground-up reporting that will give you truly unique findings. \nThat’s the up side. The downside is that this form of investigative data analysis is extremely labor intensive and fraught with potential methodology questions and errors. It requires more time and greater levels of bulletproofing from reporters and their editors, so plan accordingly if you decide it’s the only way to answer that burning question.","guid":"https://datajournalism.com/read/longreads/data-journalism-a-guide-for-editors","isoDate":"2019-10-30T08:56:00.000Z","blogTitle":"DataJournalism.com"}},"__N_SSG":true},"page":"/[...slug]","query":{"slug":["data-journalism-a-guide-for-editors"]},"buildId":"yP12RsHv5AtYmalih9fUM","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>