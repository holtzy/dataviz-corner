<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/7b0d10d140b8a0a5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/7b0d10d140b8a0a5.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-59c5c889f52620d6.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-f11614d8aa7ee555.js" defer=""></script><script src="/_next/static/chunks/pages/_app-694ee0f2821639fc.js" defer=""></script><script src="/_next/static/chunks/996-eeb5175dbd5dba8f.js" defer=""></script><script src="/_next/static/chunks/36-94b5e24e03efc6db.js" defer=""></script><script src="/_next/static/chunks/pages/%5B...slug%5D-af748dcc13a25fcb.js" defer=""></script><script src="/_next/static/yP12RsHv5AtYmalih9fUM/_buildManifest.js" defer=""></script><script src="/_next/static/yP12RsHv5AtYmalih9fUM/_ssgManifest.js" defer=""></script></head><body><div id="__next"></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"creator":"Vojtech Sedlak","title":"De-identification for data journalists","link":"https://datajournalism.com/read/longreads/de-identification-for-data-journalists","pubDate":"Wed, 16 Oct 2019 11:39:00 +0200","author":"Vojtech Sedlak","content":"\n                                                                        \u003cp\u003eIn the pursuit of a story, journalists are often required to protect the identity of their source. Many of the most impactful works of journalism have relied upon such an arrangement, yet the balancing act between publishing information that is vital to a story and protecting the person behind that information can present untold challenges, especially when the personal safety of the source is at risk. \u003c/p\u003e\n\u003cp\u003eThese challenges are particularly heightened in this age of omnipresent data collection. Advances in computing technology have enabled large volumes of data processing, which in turn promotes efforts to monetise data or use it for surveillance. In many cases, the privacy of individuals is seen as an obstacle, rather than an essential requirement. Recent history is peppered with examples of privacy violations, ranging from \u003ca href=\"https://www.theguardian.com/news/2018/mar/17/cambridge-analytica-facebook-influence-us-election\"\u003eCambridge Analytica’s use of personal data\u003c/a\u003e for ad targeting to \u003ca href=\"https://foundation.mozilla.org/en/privacynotincluded/\"\u003einvasive data tracking\u003c/a\u003e by smart devices. The very expectation of privacy protection seems to be withering away in the wake of ongoing \u003ca href=\"https://datajournalism.com/read/longreads/privacy-and-data-leaks\"\u003edata leaks and data breaches\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWith more data available than ever before, journalists are also increasingly relying on it in their reporting. But, just as with confidential sources, they need to be able to evaluate what information to publish without revealing unnecessary personal details. While some personal information may be required, it’s likely that most stories can be published without needing to identify all individuals in a dataset. In these cases, journalists can use various methods to protect these individuals’ privacy, through processes known as \u003ca href=\"https://en.wikipedia.org/wiki/De-identification\"\u003ede-identification or anonymisation\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTo help journalists champion responsible and privacy-centric data practices, this Long Read will cover how to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eidentify personal information\u003c/li\u003e\n\u003cli\u003eevaluate the risks associated with publishing personal information\u003c/li\u003e\n\u003cli\u003eutilise different de-identification methods in data journalism.\u003c/li\u003e\n\u003c/ul\u003e\n\n                                                                     \n                            \n                                                                      \n                            \u003ch1\u003eDefining personal information\u003c/h1\u003e\n\u003cp\u003eWhile the definition of what constitutes personal information has become more formalised through legal reform in the late 2000s, it has long been the role of journalists to uncover if a release of data, whether intended or accidental, jeopardises the privacy of individuals. After AOL published millions of online search queries in 2006, journalists were able to \u003ca href=\"https://www.nytimes.com/2006/08/09/technology/09aol.html\"\u003epiece together\u003c/a\u003e individual identities solely based on individuals’ search histories, including sensitive information about some individuals’ health statuses and dating preferences. Similarly, in the wake of Edward Snowden’s revelations of \u003ca href=\"https://en.wikipedia.org/wiki/Global_surveillance_disclosures_(2013%E2%80%93present)\"\u003eNSA spying\u003c/a\u003e, various researchers have shown how communication metadata -- information generated by our devices -- can be used to \u003ca href=\"https://www.schneier.com/blog/archives/2018/07/identifying_peo_8.html\"\u003eidentify users\u003c/a\u003e, or serve as an \u003ca href=\"https://www.ipc.on.ca/wp-content/uploads/Resources/metadata.pdf\"\u003einstrument of surveillance\u003c/a\u003e. \u003c/p\u003e\n\u003cp\u003eBut, when using a dataset as a source in a story, journalists are put in the new position of having to evaluate the sensitivity of the information at hand themselves. And this assessment starts with understanding what is and isn’t personal information.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Personal_data\"\u003ePersonally identifiable information (PII)\u003c/a\u003e, legally described as ‘personal data’ in Europe or ‘personal information’ in some other jurisdictions, is generally understood as anything that can directly identify an individual, although it is important to note that PII exists along a spectrum of both identifiability and sensitivity. For instance, names or email addresses have a high value in terms of identifiability, but a relatively low sensitivity, as their publication generally doesn’t endanger an individual. Location data or a personal health record may have lower identifiability, but a higher degree of sensitivity. For illustration purposes, we can plot various types of PIII along the sensitivity and identifiability spectrums.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003ePII exists along a spectrum of sensitivity and identifiability.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eThe degree to which information is personally identifiable or sensitive depends on both context and the compounding effect of data mixing. A person’s name may carry a low risk in a dataset of Facebook fans, but if the name is on a list of political dissidents, then the risk of publishing that information increases dramatically. The value of information also changes when combined with other data. On its own, a dataset that contains purchase history may be difficult to link to any given individual; however, when combined with location information or credit card numbers, it can reach higher degrees of both identifiability and sensitivity.\u003c/p\u003e\n\u003cp\u003eIn a 2016 case, the Australian Department of Health published de-identified pharmaceutical data for research purposes, only to have academics decrypt one of the de-identified fields. This created the potential for personal information to be exposed, prompting an \u003ca href=\"https://www.oaic.gov.au/privacy-law/commissioner-initiated-investigation-reports/publication-of-mbs-pbs-data#whether-the-dataset-contained-personal-information\"\u003einvestigation\u003c/a\u003e by the Australian Privacy Commissioner. In another example, Buzzfeed journalists investigating \u003ca href=\"https://www.buzzfeednews.com/article/heidiblake/the-tennis-racket\"\u003efraud among pro tennis players\u003c/a\u003e in 2016 published the anonymised data that they used in their reporting. However, a group of undergraduate students was \u003ca href=\"https://medium.com/@rkaplan/finding-the-tennis-suspects-c2d9f198c33d#.ot3r4eii7\"\u003eable to re-identify\u003c/a\u003e the affected tennis players by using publicly available data. As these examples illustrate, a journalist’s ability to determine the personal nature of a dataset requires a careful evaluation of \u003cem\u003eboth\u003c/em\u003e the information it contains, and also the information that may already be publicly available.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eWhile these tennis players’ names may appear anonymous, BuzzFeed’s open-source methodology also included other data which allowed for the possibility of re-identification.\u003c/p\u003e\n\n                                                                      \n                            \u003ch1\u003eWhat is de-identification?\u003c/h1\u003e\n\u003cp\u003eIn order to conceal the identity of a source, a journalist may infer anonymity or use a pseudonym, such as \u003ca href=\"https://en.wikipedia.org/wiki/Deep_Throat_(Watergate)\"\u003eDeep Throat\u003c/a\u003e in the case of the Watergate scandal. When working with information, the process of removing personal details is called de-identification or, in some jurisdictions, anonymisation. Long before the internet, data de-identification techniques were employed by journalists, for example by redacting names from leaked documents. Today, journalists are armed with new de-identification methods and tools for protecting privacy in digital environments, which make it easier to analyse and manipulate ever larger amounts of data. \u003c/p\u003e\n\u003cp\u003eThe goal of de-identifying data is to avoid possible re-identification, in other words, to anonymise data so that it cannot be used to identify an individual. While some legal definitions of data anonymisation exist, the regulation and enforcement of de-identification is usually handled on an ad-hoc, industry-specific basis. For instance, health records in the United States must comply with the Health Insurance Portability and Accountability Act (HIPAA), which requires the \u003ca href=\"https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html#standard\"\u003eanonymisation of direct identifiers\u003c/a\u003e, such as names, addresses, and social security numbers, before data can be published for public consumption. In the European Union, the \u003ca href=\"https://en.wikipedia.org/wiki/General_Data_Protection_Regulation\"\u003eGeneral Data Protection Regulation (GDPR)\u003c/a\u003e enforces anonymisation of both direct identifiers, such as names, addresses, and emails, as well as indirect identifiers, such as job titles and postal codes. \u003c/p\u003e\n\u003cp\u003eIn developing their story, journalists have to decide what information is vital to a story and what can be omitted. Often, the more valuable a piece of information, the more sensitive it is. For example, health researchers need to be able to access diagnostic or other medical data, even though that data can have a high degree of sensitivity if it is linked to a given individual. To strike the right balance between data usefulness and sensitivity, when deciding what to publish, journalists can choose from a range of de-identification techniques.\u003c/p\u003e\n\n                                                                                                  \n                                \u003cp\u003eAn example of a redacted CIA document. Source: \u003ca href=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Redacted_CIA_document.jpg/512px-Redacted_CIA_document.jpg\"\u003eWikimedia\u003c/a\u003e.\u003c/p\u003e\n\n                                                                                        \u003ch2\u003eData redaction\u003c/h2\u003e\n\u003cp\u003eThe simplest way to de-identify a dataset is to remove or redact any personal or sensitive data. While an obvious drawback is the possible loss of the data’s informative value, redaction is most commonly used to deal with direct identifiers, such as names, addresses, or social security numbers, which usually don’t represent the crux of a story.\u003c/p\u003e\n\n                                                                                                  \n                            \u003cp\u003eThat said, technological advances and the growing availability of data will continue to increase the identifiability potential of indirect identifiers, so journalists shouldn’t rely on data redaction as their only means of de-identification.\u003c/p\u003e\n\n                                                                      \n                            \u003ch2\u003ePseudonymisation\u003c/h2\u003e\n\u003cp\u003eIn some cases, removing information outright limits the usefulness of the data.  Pseudonymisation offers a possible solution, by replacing identifiable data with pseudonyms that are generated either randomly, or by an algorithm. The most common techniques for pseudonymisation are \u003ca href=\"https://www.hackerearth.com/practice/data-structures/hash-tables/basics-of-hash-tables/tutorial/\"\u003ehashing\u003c/a\u003e and \u003ca href=\"https://www.it.northwestern.edu/policies/dataencryption.html\"\u003eencryption\u003c/a\u003e. Hashing relies on mathematical functions to convert data into unreadable hashes. Encryption, on the other hand, relies on a two-way algorithmic transformation of the data. The primary difference between the two methods is that encrypted data can be decrypted with the right key, whereas hashed information is non-reversible. Many databases systems, such as \u003ca href=\"https://www.mysql.com/\"\u003eMySQL\u003c/a\u003e and \u003ca href=\"https://www.postgresql.org/\"\u003ePostgreSQL\u003c/a\u003e, enable both the hashing and encryption of data.\u003c/p\u003e\n\u003cp\u003eData pseudonymisation played an important role in the \u003ca href=\"https://en.wikipedia.org/wiki/Offshore_Leaks\"\u003eOffshore Leaks\u003c/a\u003e investigation by the International Center for Investigative Journalism (ICIJ). Given the vast volume of data that needed to be processed, journalists relied on \u003ca href=\"https://offshoreleaks.icij.org/pages/faq#transformations\"\u003eunique codes\u003c/a\u003e associated with each individual and entity that appeared in the leaked documents. These pseudonymised codes were used to show links between leaked documents, even in cases when the names of individuals and entities didn’t match.\u003c/p\u003e\n\u003cp\u003eInformation is considered pseudonymised if it can no longer be linked to an individual without the use of additional data. At the same time, the ability to combine pseudonymised data with other datasets renders pseudonymisation a possibly weak method of de-identification. Even by using the same pseudonym repeatedly throughout a dataset, its effectiveness can decrease, as the potential for finding relationships between variables grows with every occurrence of the pseudonym. Finally, in some cases, the very algorithms used to create pseudonyms can be \u003ca href=\"https://research.neustar.biz/2014/09/15/riding-with-the-stars-passenger-privacy-in-the-nyc-taxicab-dataset/\"\u003ecracked by third parties\u003c/a\u003e, or have inherent \u003ca href=\"https://ieeexplore.ieee.org/document/4371616\"\u003esecurity vulnerabilities\u003c/a\u003e. Therefore, journalists should  be careful when using pseudonymisation to hide personal data.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eIn 2013, Jonathan Armoza \u003ca href=\"https://research.neustar.biz/2014/09/15/riding-with-the-stars-passenger-privacy-in-the-nyc-taxicab-dataset/\"\u003eidentified\u003c/a\u003e taxi trips made by celebrities Bradley Cooper and Jessica Alba from a dataset of New York taxi rides, where the taxi’s license and medallion numbers were supposedly hashed. To crack the code, he simply searched images of celebrities getting out of taxis and combined it with other information available in the dataset.\u003c/p\u003e\n\n                                                                      \n                            \u003ch2\u003eStatistical noise\u003c/h2\u003e\n\u003cp\u003eSince both data redaction and pseudonymisation carry the risk of re-identification, they are often combined with statistical noise methods, such as \u003ca href=\"https://en.wikipedia.org/wiki/K-anonymity\"\u003ek-anonymization\u003c/a\u003e. These ensure that at least a set number of individuals share the same indirect identifiers, thereby obscuring the process of re-identification. As a best practice, there should be no less than 10 entries with unique combinations of identifiers. Common techniques for introducing statistical noise into a dataset are \u003ca href=\"https://queue.acm.org/detail.cfm?id=2838930\"\u003egeneralisation\u003c/a\u003e, such as replacing the name of a country with a continent, and \u003ca href=\"http://summit.sfu.ca/system/files/iritems1/14177/etd8398_PWang.pdf\"\u003ebucketing\u003c/a\u003e, which is the conversion of numbers into ranges. In addition, data redaction and pseudonymisation are often used with statistical noise techniques to ensure that no unique combinations of identifiers exist in a dataset. In the following example, data in certain columns is generalised or redacted to prevent re-identification of individual entries.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eAdding statistical noise to prevent re-identification.\u003c/p\u003e\n\n                                                                      \n                            \u003ch2\u003eData aggregation\u003c/h2\u003e\n\u003cp\u003eWhen the integrity of raw data doesn’t need to be preserved, journalists can rely on data aggregation as a method for de-identification. Instead of publishing the complete dataset, data can be published in the form of summaries that omit any direct or indirect identifiers. The principal concern with data aggregation is ensuring that the smallest segments of the aggregated data are large enough, so as to not reveal specific individuals. This is particularly relevant when multiple dimensions of aggregated data can be combined, as in the case study below.\u003c/p\u003e\n\n                                                                                                 \u003cp\u003e\u003cstrong\u003eCase study: Mozilla’s Facebook Survey\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eFollowing the Cambridge Analytica scandal, the Mozilla Foundation conducted a survey of internet users about their attitudes toward Facebook. In addition to their attitudes, respondents were asked for information about their age, country of residence, and digital proficiency. The survey tool also recorded IP addresses of users, as well as other metadata, such as the device used and time of submission. \u003c/p\u003e\n\u003cp\u003eThese responses were made available via an \u003ca href=\"https://vojtechsedlak.shinyapps.io/facebook/\"\u003einteractive tool\u003c/a\u003e, which allowed audiences to closely examine the data, including through the ability to cross-tabulate results by demographic criteria, like age or country. But Mozilla also wanted to release all of the survey data to the public for further analysis, so a careful approach to de-identification was required.\u003c/p\u003e\n\u003cp\u003eTo begin the de-identification process, Mozilla removed all communication metadata that wasn’t required to complete the analysis. For instance, IP addresses of respondents, as well as the time of submissions, were scrubbed from the dataset. The survey didn’t record direct identifiers, such as names or email addresses, so no redaction or pseudonymisation was required. Even though the survey included over 46,000 responses, the data included certain combinations of indirect identifiers, such as country and age information, that allowed users to zoom in on small samples of the respondents. Since this increased the risk of re-identification, all countries with less than 700 respondents were bundled into an ‘other’ category, which added sufficient statistical noise to the data. \u003c/p\u003e\n\u003cp\u003eDespite these efforts, Mozilla’s privacy and legal teams remained cautious about publishing the data, since its global character implied possible legal liability in various jurisdictions. But, in the end, the value of publishing the data outweighed any remaining privacy concerns.\u003c/p\u003e\n\n                                                                     \n                            \u003ch1\u003eDe-identification workflows for journalists\u003c/h1\u003e\n\u003cp\u003eFor journalists on a deadline, de-identification may appear to play second fiddle to more substantive decisions, such as assessing data quality or deciding how to visualise a dataset. But ensuring the privacy of individuals should nevertheless have a firm place in the journalistic process, especially since improper handling of personal data can undermine the very credibility of the piece. Legal liability under privacy laws may also be of concern if the publication is responsible for data collection or processing. Therefore, data journalists should take the following steps to incorporate de-identification into their workflows:\u003c/p\u003e\n\u003ch2\u003e1. Does my dataset include personal information?\u003c/h2\u003e\n\u003cp\u003eIt may be the case that the dataset you are working with includes weather data, or publicly available sport statistics, which absolves you from the need to worry about de-identification. In other cases, the presence of names or social security numbers will quickly make any privacy risks apparent. Often, however, determining whether data is personally identifiable may require a closer examination. This is particularly the case when working with leaked data, as explained in \u003ca href=\"https://datajournalism.com/read/longreads/privacy-and-data-leaks\"\u003ethis Long Read\u003c/a\u003e by Susan McGregor and Alice Brennan. Aside from noting the presence of any direct identifiers, journalists should pay close attention to indirect identifiers, such as IP addresses, job information, and geographical records. As a rule of thumb, any information relating to a person should be considered a privacy risk and processed accordingly.\u003c/p\u003e\n\u003ch2\u003e2. How sensitive and identifiable is the data?\u003c/h2\u003e\n\u003cp\u003eAs explained above, personal information carries different risks based on the context in which it exists, including whether it can be combined with other data. This means that journalists need to evaluate two things: 1) how identifiable a piece of data is and 2) how sensitive it is to the privacy of an individual. Ask yourself: Will an individual’s association with the story endanger their safety or reputation? Can the data at hand be combined with other available datasets that may expose an individual’s identity? If so, do the benefits of publishing this data outweigh the associated privacy risks? A case by case approach is required to balance the public interest in publishing with the privacy risks of revealing personal details.\u003c/p\u003e\n\u003ch2\u003e3. How will the data be published?\u003c/h2\u003e\n\u003cp\u003eA journalist writing for a print publication in the pre-internet era didn’t have to worry about how data would be disclosed, as printed charts and statistics do not allow for the further querying of their underlying data. However, at the cutting edge of data journalism today, sophisticated tools and interactive visualisations enable audiences to undertake a detailed examination of the data used in a particular story. For example, many journalists opt for an open source approach, with code and data shared on Github. To open source with privacy in mind, all data needs to be carefully scrubbed of personal information. When it comes to visualisations, some journalists protect privacy by leveraging pre-aggregated data, which obfuscates the original dataset. But it’s important to check whether these aggregated samples exceed a minimum threshold of identifiability.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003e\u003ca href=\"https://fusion.tv/story/287382/panama-papers-data-visualization/\"\u003eFusion\u003c/a\u003e uses a visualisation of interrelated entities to illustrate network investigated as part of the Panama Papers, while still maintaining privacy boundaries on the data.\u003c/p\u003e\n\n                                                                      \n                            \u003ch2\u003e4. Which de-identification technique is right for your data?\u003c/h2\u003e\n\u003cp\u003eJournalists will often need to deploy a combination of de-identification techniques to best suit the data at hand. For direct identifiers, data redactions and pseudonymisation -- if properly implemented -- usually suffice in protecting the privacy of individuals. For indirect identifiers, consider adding some statistical noise by grouping data into buckets or generalising information that may not be vital to the story. Data aggregation is the best option for highly sensitive data, although journalists still have to ensure that there is a broad enough range of data and sufficiently uniform distribution in the aggregated variables to ensure that no personal information is inadvertently exposed.\u003c/p\u003e\n\n                                                                      \n                            \u003ch1\u003eLeading by example\u003c/h1\u003e\n\u003cp\u003eOnce data is available online, there is no possibility for revisions or corrections. Even if you consider that your dataset has been scrubbed of any personal details, there remains a risk that someone, somewhere may combine your data with another source to re-identify individuals, or crack your pseudonymisation algorithm and expose the personal information that it contains. As always, the risks of re-identification will continue to increase with the development of new technologies, such as machine learning and pattern recognition, which enable unanticipated ways of combining and transforming data. \u003c/p\u003e\n\u003cp\u003eRemember that seemingly impersonal data points may be used for identification purposes when combined with the right data. When Netflix announced its notorious Netflix prize for the best recommendation algorithm, the available data was scrubbed of any personal identifiers. But again, researchers were able to \u003ca href=\"https://www.cs.utexas.edu/~shmat/shmat_oak08netflix.pdf\"\u003ecross-reference personal movie preferences\u003c/a\u003e with data from IMDb.com and other online sources to identify individuals in the ‘anonymised’ Netflix dataset. \u003c/p\u003e\n\u003cp\u003eDespite the limitations of today’s de-identification methods, journalists should always be diligent in their efforts to protect the privacy of individuals. Whether it is by concealing the identity of their source, or by de-identifying personal information behind their stories.\u003c/p\u003e\n\u003cp\u003eLeading by example, the ICIJ handles vast volumes of personal data with privacy front of mind. When reporting on the \u003ca href=\"https://www.occrp.org/en/panamapapers/\"\u003ePanama Papers\u003c/a\u003e, journalists both protected the anonymity of the source of the leak, by using the \u003ca href=\"https://panamapapers.sueddeutsche.de/articles/572c897a5632a39742ed34ef/\"\u003epseudonym\u003c/a\u003e John Doe, and carefully evaluated how to publish the private information within the leaked documents. There is no reason why journalists of all backgrounds can’t take similar steps to strike a balance between privacy and the public interest in their reporting.\u003c/p\u003e\n\u003cp\u003eAnd there are many examples of possible fallouts from disclosure of personal data when privacy conscious steps aren’t taken, from \u003ca href=\"https://www.nytimes.com/2016/11/05/opinion/what-were-missing-while-we-obsess-over-john-podestas-email.html\"\u003epersonal tragedies\u003c/a\u003e following the Ashley Madison leak, or the vast \u003ca href=\"https://www.nytimes.com/2016/11/05/opinion/what-were-missing-while-we-obsess-over-john-podestas-email.html\"\u003eexposure of sensitive data\u003c/a\u003e associated with Wikileaks. Data journalists should strive to avoid the same pitfalls and instead promote responsible data practices in their reporting at all times.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eFor more on privacy and data journalism:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://datajournalism.com/read/longreads/privacy-and-data-leaks\"\u003ePrivacy and data leaks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://datajournalism.com/read/longreads/data-journalism-and-the-ethics-of-publishing-twitter-data\"\u003eData journalism and the ethics of publishing Twitter data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://datajournalism.com/read/longreads/ethical-questions-in-data-journalism-and-the-power-of-online-discussion\"\u003eEthical questions in data journalism and the power of online discussion\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://datajournalism.com/read/longreads/editorial-transparency-in-computational-journalism\"\u003eEditorial transparency in computational journalism\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n                                              \n                ","contentSnippet":"In the pursuit of a story, journalists are often required to protect the identity of their source. Many of the most impactful works of journalism have relied upon such an arrangement, yet the balancing act between publishing information that is vital to a story and protecting the person behind that information can present untold challenges, especially when the personal safety of the source is at risk. \nThese challenges are particularly heightened in this age of omnipresent data collection. Advances in computing technology have enabled large volumes of data processing, which in turn promotes efforts to monetise data or use it for surveillance. In many cases, the privacy of individuals is seen as an obstacle, rather than an essential requirement. Recent history is peppered with examples of privacy violations, ranging from Cambridge Analytica’s use of personal data for ad targeting to invasive data tracking by smart devices. The very expectation of privacy protection seems to be withering away in the wake of ongoing data leaks and data breaches.\nWith more data available than ever before, journalists are also increasingly relying on it in their reporting. But, just as with confidential sources, they need to be able to evaluate what information to publish without revealing unnecessary personal details. While some personal information may be required, it’s likely that most stories can be published without needing to identify all individuals in a dataset. In these cases, journalists can use various methods to protect these individuals’ privacy, through processes known as de-identification or anonymisation.\nTo help journalists champion responsible and privacy-centric data practices, this Long Read will cover how to:\nidentify personal information\nevaluate the risks associated with publishing personal information\nutilise different de-identification methods in data journalism.\nDefining personal information\nWhile the definition of what constitutes personal information has become more formalised through legal reform in the late 2000s, it has long been the role of journalists to uncover if a release of data, whether intended or accidental, jeopardises the privacy of individuals. After AOL published millions of online search queries in 2006, journalists were able to piece together individual identities solely based on individuals’ search histories, including sensitive information about some individuals’ health statuses and dating preferences. Similarly, in the wake of Edward Snowden’s revelations of NSA spying, various researchers have shown how communication metadata -- information generated by our devices -- can be used to identify users, or serve as an instrument of surveillance. \nBut, when using a dataset as a source in a story, journalists are put in the new position of having to evaluate the sensitivity of the information at hand themselves. And this assessment starts with understanding what is and isn’t personal information.\nPersonally identifiable information (PII), legally described as ‘personal data’ in Europe or ‘personal information’ in some other jurisdictions, is generally understood as anything that can directly identify an individual, although it is important to note that PII exists along a spectrum of both identifiability and sensitivity. For instance, names or email addresses have a high value in terms of identifiability, but a relatively low sensitivity, as their publication generally doesn’t endanger an individual. Location data or a personal health record may have lower identifiability, but a higher degree of sensitivity. For illustration purposes, we can plot various types of PIII along the sensitivity and identifiability spectrums.\nPII exists along a spectrum of sensitivity and identifiability.\nThe degree to which information is personally identifiable or sensitive depends on both context and the compounding effect of data mixing. A person’s name may carry a low risk in a dataset of Facebook fans, but if the name is on a list of political dissidents, then the risk of publishing that information increases dramatically. The value of information also changes when combined with other data. On its own, a dataset that contains purchase history may be difficult to link to any given individual; however, when combined with location information or credit card numbers, it can reach higher degrees of both identifiability and sensitivity.\nIn a 2016 case, the Australian Department of Health published de-identified pharmaceutical data for research purposes, only to have academics decrypt one of the de-identified fields. This created the potential for personal information to be exposed, prompting an investigation by the Australian Privacy Commissioner. In another example, Buzzfeed journalists investigating fraud among pro tennis players in 2016 published the anonymised data that they used in their reporting. However, a group of undergraduate students was able to re-identify the affected tennis players by using publicly available data. As these examples illustrate, a journalist’s ability to determine the personal nature of a dataset requires a careful evaluation of both the information it contains, and also the information that may already be publicly available.\nWhile these tennis players’ names may appear anonymous, BuzzFeed’s open-source methodology also included other data which allowed for the possibility of re-identification.\nWhat is de-identification?\nIn order to conceal the identity of a source, a journalist may infer anonymity or use a pseudonym, such as Deep Throat in the case of the Watergate scandal. When working with information, the process of removing personal details is called de-identification or, in some jurisdictions, anonymisation. Long before the internet, data de-identification techniques were employed by journalists, for example by redacting names from leaked documents. Today, journalists are armed with new de-identification methods and tools for protecting privacy in digital environments, which make it easier to analyse and manipulate ever larger amounts of data. \nThe goal of de-identifying data is to avoid possible re-identification, in other words, to anonymise data so that it cannot be used to identify an individual. While some legal definitions of data anonymisation exist, the regulation and enforcement of de-identification is usually handled on an ad-hoc, industry-specific basis. For instance, health records in the United States must comply with the Health Insurance Portability and Accountability Act (HIPAA), which requires the anonymisation of direct identifiers, such as names, addresses, and social security numbers, before data can be published for public consumption. In the European Union, the General Data Protection Regulation (GDPR) enforces anonymisation of both direct identifiers, such as names, addresses, and emails, as well as indirect identifiers, such as job titles and postal codes. \nIn developing their story, journalists have to decide what information is vital to a story and what can be omitted. Often, the more valuable a piece of information, the more sensitive it is. For example, health researchers need to be able to access diagnostic or other medical data, even though that data can have a high degree of sensitivity if it is linked to a given individual. To strike the right balance between data usefulness and sensitivity, when deciding what to publish, journalists can choose from a range of de-identification techniques.\nAn example of a redacted CIA document. Source: Wikimedia.\nData redaction\nThe simplest way to de-identify a dataset is to remove or redact any personal or sensitive data. While an obvious drawback is the possible loss of the data’s informative value, redaction is most commonly used to deal with direct identifiers, such as names, addresses, or social security numbers, which usually don’t represent the crux of a story.\nThat said, technological advances and the growing availability of data will continue to increase the identifiability potential of indirect identifiers, so journalists shouldn’t rely on data redaction as their only means of de-identification.\nPseudonymisation\nIn some cases, removing information outright limits the usefulness of the data.  Pseudonymisation offers a possible solution, by replacing identifiable data with pseudonyms that are generated either randomly, or by an algorithm. The most common techniques for pseudonymisation are hashing and encryption. Hashing relies on mathematical functions to convert data into unreadable hashes. Encryption, on the other hand, relies on a two-way algorithmic transformation of the data. The primary difference between the two methods is that encrypted data can be decrypted with the right key, whereas hashed information is non-reversible. Many databases systems, such as MySQL and PostgreSQL, enable both the hashing and encryption of data.\nData pseudonymisation played an important role in the Offshore Leaks investigation by the International Center for Investigative Journalism (ICIJ). Given the vast volume of data that needed to be processed, journalists relied on unique codes associated with each individual and entity that appeared in the leaked documents. These pseudonymised codes were used to show links between leaked documents, even in cases when the names of individuals and entities didn’t match.\nInformation is considered pseudonymised if it can no longer be linked to an individual without the use of additional data. At the same time, the ability to combine pseudonymised data with other datasets renders pseudonymisation a possibly weak method of de-identification. Even by using the same pseudonym repeatedly throughout a dataset, its effectiveness can decrease, as the potential for finding relationships between variables grows with every occurrence of the pseudonym. Finally, in some cases, the very algorithms used to create pseudonyms can be cracked by third parties, or have inherent security vulnerabilities. Therefore, journalists should  be careful when using pseudonymisation to hide personal data.\nIn 2013, Jonathan Armoza identified taxi trips made by celebrities Bradley Cooper and Jessica Alba from a dataset of New York taxi rides, where the taxi’s license and medallion numbers were supposedly hashed. To crack the code, he simply searched images of celebrities getting out of taxis and combined it with other information available in the dataset.\nStatistical noise\nSince both data redaction and pseudonymisation carry the risk of re-identification, they are often combined with statistical noise methods, such as k-anonymization. These ensure that at least a set number of individuals share the same indirect identifiers, thereby obscuring the process of re-identification. As a best practice, there should be no less than 10 entries with unique combinations of identifiers. Common techniques for introducing statistical noise into a dataset are generalisation, such as replacing the name of a country with a continent, and bucketing, which is the conversion of numbers into ranges. In addition, data redaction and pseudonymisation are often used with statistical noise techniques to ensure that no unique combinations of identifiers exist in a dataset. In the following example, data in certain columns is generalised or redacted to prevent re-identification of individual entries.\nAdding statistical noise to prevent re-identification.\nData aggregation\nWhen the integrity of raw data doesn’t need to be preserved, journalists can rely on data aggregation as a method for de-identification. Instead of publishing the complete dataset, data can be published in the form of summaries that omit any direct or indirect identifiers. The principal concern with data aggregation is ensuring that the smallest segments of the aggregated data are large enough, so as to not reveal specific individuals. This is particularly relevant when multiple dimensions of aggregated data can be combined, as in the case study below.\nCase study: Mozilla’s Facebook Survey\nFollowing the Cambridge Analytica scandal, the Mozilla Foundation conducted a survey of internet users about their attitudes toward Facebook. In addition to their attitudes, respondents were asked for information about their age, country of residence, and digital proficiency. The survey tool also recorded IP addresses of users, as well as other metadata, such as the device used and time of submission. \nThese responses were made available via an interactive tool, which allowed audiences to closely examine the data, including through the ability to cross-tabulate results by demographic criteria, like age or country. But Mozilla also wanted to release all of the survey data to the public for further analysis, so a careful approach to de-identification was required.\nTo begin the de-identification process, Mozilla removed all communication metadata that wasn’t required to complete the analysis. For instance, IP addresses of respondents, as well as the time of submissions, were scrubbed from the dataset. The survey didn’t record direct identifiers, such as names or email addresses, so no redaction or pseudonymisation was required. Even though the survey included over 46,000 responses, the data included certain combinations of indirect identifiers, such as country and age information, that allowed users to zoom in on small samples of the respondents. Since this increased the risk of re-identification, all countries with less than 700 respondents were bundled into an ‘other’ category, which added sufficient statistical noise to the data. \nDespite these efforts, Mozilla’s privacy and legal teams remained cautious about publishing the data, since its global character implied possible legal liability in various jurisdictions. But, in the end, the value of publishing the data outweighed any remaining privacy concerns.\nDe-identification workflows for journalists\nFor journalists on a deadline, de-identification may appear to play second fiddle to more substantive decisions, such as assessing data quality or deciding how to visualise a dataset. But ensuring the privacy of individuals should nevertheless have a firm place in the journalistic process, especially since improper handling of personal data can undermine the very credibility of the piece. Legal liability under privacy laws may also be of concern if the publication is responsible for data collection or processing. Therefore, data journalists should take the following steps to incorporate de-identification into their workflows:\n1. Does my dataset include personal information?\nIt may be the case that the dataset you are working with includes weather data, or publicly available sport statistics, which absolves you from the need to worry about de-identification. In other cases, the presence of names or social security numbers will quickly make any privacy risks apparent. Often, however, determining whether data is personally identifiable may require a closer examination. This is particularly the case when working with leaked data, as explained in this Long Read by Susan McGregor and Alice Brennan. Aside from noting the presence of any direct identifiers, journalists should pay close attention to indirect identifiers, such as IP addresses, job information, and geographical records. As a rule of thumb, any information relating to a person should be considered a privacy risk and processed accordingly.\n2. How sensitive and identifiable is the data?\nAs explained above, personal information carries different risks based on the context in which it exists, including whether it can be combined with other data. This means that journalists need to evaluate two things: 1) how identifiable a piece of data is and 2) how sensitive it is to the privacy of an individual. Ask yourself: Will an individual’s association with the story endanger their safety or reputation? Can the data at hand be combined with other available datasets that may expose an individual’s identity? If so, do the benefits of publishing this data outweigh the associated privacy risks? A case by case approach is required to balance the public interest in publishing with the privacy risks of revealing personal details.\n3. How will the data be published?\nA journalist writing for a print publication in the pre-internet era didn’t have to worry about how data would be disclosed, as printed charts and statistics do not allow for the further querying of their underlying data. However, at the cutting edge of data journalism today, sophisticated tools and interactive visualisations enable audiences to undertake a detailed examination of the data used in a particular story. For example, many journalists opt for an open source approach, with code and data shared on Github. To open source with privacy in mind, all data needs to be carefully scrubbed of personal information. When it comes to visualisations, some journalists protect privacy by leveraging pre-aggregated data, which obfuscates the original dataset. But it’s important to check whether these aggregated samples exceed a minimum threshold of identifiability.\nFusion uses a visualisation of interrelated entities to illustrate network investigated as part of the Panama Papers, while still maintaining privacy boundaries on the data.\n4. Which de-identification technique is right for your data?\nJournalists will often need to deploy a combination of de-identification techniques to best suit the data at hand. For direct identifiers, data redactions and pseudonymisation -- if properly implemented -- usually suffice in protecting the privacy of individuals. For indirect identifiers, consider adding some statistical noise by grouping data into buckets or generalising information that may not be vital to the story. Data aggregation is the best option for highly sensitive data, although journalists still have to ensure that there is a broad enough range of data and sufficiently uniform distribution in the aggregated variables to ensure that no personal information is inadvertently exposed.\nLeading by example\nOnce data is available online, there is no possibility for revisions or corrections. Even if you consider that your dataset has been scrubbed of any personal details, there remains a risk that someone, somewhere may combine your data with another source to re-identify individuals, or crack your pseudonymisation algorithm and expose the personal information that it contains. As always, the risks of re-identification will continue to increase with the development of new technologies, such as machine learning and pattern recognition, which enable unanticipated ways of combining and transforming data. \nRemember that seemingly impersonal data points may be used for identification purposes when combined with the right data. When Netflix announced its notorious Netflix prize for the best recommendation algorithm, the available data was scrubbed of any personal identifiers. But again, researchers were able to cross-reference personal movie preferences with data from IMDb.com and other online sources to identify individuals in the ‘anonymised’ Netflix dataset. \nDespite the limitations of today’s de-identification methods, journalists should always be diligent in their efforts to protect the privacy of individuals. Whether it is by concealing the identity of their source, or by de-identifying personal information behind their stories.\nLeading by example, the ICIJ handles vast volumes of personal data with privacy front of mind. When reporting on the Panama Papers, journalists both protected the anonymity of the source of the leak, by using the pseudonym John Doe, and carefully evaluated how to publish the private information within the leaked documents. There is no reason why journalists of all backgrounds can’t take similar steps to strike a balance between privacy and the public interest in their reporting.\nAnd there are many examples of possible fallouts from disclosure of personal data when privacy conscious steps aren’t taken, from personal tragedies following the Ashley Madison leak, or the vast exposure of sensitive data associated with Wikileaks. Data journalists should strive to avoid the same pitfalls and instead promote responsible data practices in their reporting at all times.\nFor more on privacy and data journalism:\nPrivacy and data leaks\nData journalism and the ethics of publishing Twitter data\nEthical questions in data journalism and the power of online discussion\nEditorial transparency in computational journalism","guid":"https://datajournalism.com/read/longreads/de-identification-for-data-journalists","isoDate":"2019-10-16T09:39:00.000Z","blogTitle":"DataJournalism.com"}},"__N_SSG":true},"page":"/[...slug]","query":{"slug":["de-identification-for-data-journalists"]},"buildId":"yP12RsHv5AtYmalih9fUM","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>