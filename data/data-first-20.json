[
  {
    "title": "Look, Ma, No More Mercator Tiles",
    "link": "https://vis4.net/blog/2015/01/no-more-mercator-tiles/",
    "pubDate": "2015-01-06T20:06:32.000Z",
    "isoDate": "2015-01-06T20:06:32.000Z",
    "content": "<p>Using open source tools it is now super easy to make your own map tiles, and with a little extra work you can render them in whatever map projection you want. No more excuses to use Mercator! For example, <a href=\"http://www.nytimes.com/interactive/2015/01/06/upshot/where-working-women-are-most-common.html?rref=upshot\" target=\"_blank\" rel=\"noopener\">here is a map</a> we published today at The Upshot. It shows where prime-age women are working more or less then average, and includes data from county-level in the overview map down to every census tract once you zoom in. And all is nicely projected in <a href=\"http://en.wikipedia.org/wiki/Albers_projection\" target=\"_blank\" rel=\"noopener\">Albers Equal-Area Conic</a>, a projection widely adopted as standard for U.S. maps.</p><div class=\"poster poster-945\"><p><img src=\"/blog/images/old/35b642ff6b3c1acd1e2813e252d22693.png\" alt=\"\"></p></div><h2 id=\"so-how-does-this-work\"><a class=\"anchor\" href=\"#so-how-does-this-work\"><span class=\"header-anchor\">#</span></a> So how does this work?</h2><p>After many years of blindly accepting the dominance of Web Mercator tile maps, I was quite surprised to learn how easy it is to use whatever projection you want. So how does this work? The answer is that it works because <a href=\"http://mapnik.org/\" target=\"_blank\" rel=\"noopener\">Mapnik</a>, the core of many open source tile mapping frameworks, supports custom projections out of the box. It is just the tools built around Mapnik that are not supporting other projections. <a href=\"https://www.mapbox.com/tilemill/\" target=\"_blank\" rel=\"noopener\">Tilemill</a>, for instance, is a super nice tool for styling map tiles in a <a href=\"https://www.mapbox.com/tilemill/docs/manual/carto/\" target=\"_blank\" rel=\"noopener\">CSS like language</a>. But if you export your map as tiles using Tilemill, you are stuck with Web Mercator, even though internally it is Mapnik that renders all the tiles. Fortunately this doesn’t mean that you have to deal with Mapnik and it’s quirks directly to get custom projections.</p><h2 id=\"step-1-export-mapnik-xml-and-change-projection\"><a class=\"anchor\" href=\"#step-1-export-mapnik-xml-and-change-projection\"><span class=\"header-anchor\">#</span></a> Step 1: Export Mapnik XML and change projection</h2><p>So the first step is to style your map just as you would with a Mercator map, enjoying the full feature-set of Tilemill. Once you’rd done with that you <a href=\"https://www.mapbox.com/tilemill/docs/manual/exporting/#mapnik-xml-export\" target=\"_blank\" rel=\"noopener\">export your project as Mapnik XML</a>. Think of this XML file as the entire description of your map. It contains all the references to the source layers and all the styles for the map features – in one single file. If you actually read the code of the file you will quickly realize how extremely lucky we are to have Tilemill and CartoCSS. And you might also notice that in the root element of the document, named <code>&lt;Map&gt;</code>, you find the the definition of the map projection as Proj.4 string in the <code>srs</code> attribute (for spatial reference system). And you can simply change it to whatever you want. In this case I replaced the Mercator projection with the Albers projection (copied from the Proj.4 definition <a href=\"http://spatialreference.org/ref/esri/usa-contiguous-albers-equal-area-conic/\" target=\"_blank\" rel=\"noopener\">linked here</a>):</p><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">Map</span> <span class=\"attr\">srs</span>=<span class=\"string\">\"+proj=aea +lat_1=29.5 +lat_2=45.5</span></span></span><br><span class=\"line\"><span class=\"tag\"><span class=\"string\">   +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +ellps=sphere</span></span></span><br><span class=\"line\"><span class=\"tag\"><span class=\"string\">   +towgs84=0,0,0,0,0,0,0 +units=m +no_defs +nadgrids=@null\"</span></span></span><br><span class=\"line\"><span class=\"tag\">   <span class=\"attr\">background-color</span>=<span class=\"string\">\"#ffffff\"</span>&gt;</span></span><br></pre></td></tr></table></figure><p>I copied the Proj.4 string from <a href=\"http://spatialreference.org/ref/esri/usa-contiguous-albers-equal-area-conic/\" target=\"_blank\" rel=\"noopener\">spatialreference.org</a>, where you find many other standard projections as well. (Hint: If you plan on aligning the tile map to a D3.js projected vector overlay you should append <code>+nadgrids=@null</code> to the Proj.4 string) To preview the projection I used <a href=\"http://tilestache.org/\" target=\"_blank\" rel=\"noopener\">TileStache</a>. All it needs is a <a href=\"http://tilestache.org/doc/#configuring-tilestache\" target=\"_blank\" rel=\"noopener\">JSON configuration</a> file that points to the exported Mapnik XML as input layer.</p><figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"attr\">\"cache\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"Disk\"</span>, <span class=\"attr\">\"path\"</span>: <span class=\"string\">\"tiles\"</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">\"layers\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"map\"</span>: &#123;</span><br><span class=\"line\">            <span class=\"attr\">\"provider\"</span>: &#123;<span class=\"attr\">\"name\"</span>: <span class=\"string\">\"mapnik\"</span>, <span class=\"attr\">\"mapfile\"</span>: <span class=\"string\">\"mapnik.xml\"</span>&#125;,</span><br><span class=\"line\">            <span class=\"attr\">\"projection\"</span>: <span class=\"string\">\"spherical mercator\"</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><p>Then you run <a href=\"http://tilestache.org/doc/#serving-tiles\" target=\"_blank\" rel=\"noopener\">tilestache-server.py</a> with your config and open <a href=\"http://localhost:8080/map/preview.html\" target=\"_blank\" rel=\"noopener\">http://localhost:8080/map/preview.html</a> to preview the freshly rendered projected tiles, just as you would in Tilemill.</p><h2 id=\"step-2-figuring-out-which-tiles-to-render\"><a class=\"anchor\" href=\"#step-2-figuring-out-which-tiles-to-render\"><span class=\"header-anchor\">#</span></a> Step 2: Figuring out which tiles to render</h2><p>Now we come to the first tricky part of the process. We need to figure out which tiles we actually want to be rendered. You probably already know the zoom levels and the bounding box in WGS 84 lat/lon coordinates, but the latter won’t help us much since TileStache is designed for rectangular projections like Mercator. Fortunately TileStache also takes a text file with a list of Z/X/Y tile coordinates as input. To get this list of tiles I wrote <a href=\"https://gist.github.com/gka/52ef3bf442bed80367f6\" target=\"_blank\" rel=\"noopener\">a Python script</a> (feel free to re-use if you want). The script uses <a href=\"https://github.com/mapbox/mercantile\" target=\"_blank\" rel=\"noopener\">mercantile</a>, a Python library for tile calculations, which returns the tile coordinates for points in WGS84 lat/lon, assuming that the tiles are projected in Web Mercator. To trick the library into giving me the correct tiles, I converted the points to the custom projection first and then projected them “back” from Web Mercator (even though the coordinates aren’t in Mercator). When mercantile gets my “fake” lat/lon coordinates it projects them to Mercator (reversing my inverse projection) and ends up with the Albers Equal Area coordinates.</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_tile</span><span class=\"params\">(lon, lat, z)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># project to custom projection</span></span><br><span class=\"line\">    pt = albers(lon, lat)</span><br><span class=\"line\">    <span class=\"comment\"># project \"back\" from Mercator</span></span><br><span class=\"line\">    pt2 = mercator(pt[<span class=\"number\">0</span>], pt[<span class=\"number\">1</span>], inverse=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mercantile.tile(pt2[<span class=\"number\">0</span>],pt2[<span class=\"number\">1</span>], z)</span><br></pre></td></tr></table></figure><p>Using this function I then <a href=\"https://gist.github.com/gka/52ef3bf442bed80367f6#file-make-tile-urls-py-L28-L30\" target=\"_blank\" rel=\"noopener\">compute the top-left and bottom-right tile</a> for each zoom level and <a href=\"https://gist.github.com/gka/52ef3bf442bed80367f6#file-make-tile-urls-py-L31-L33\" target=\"_blank\" rel=\"noopener\">add every tile</a> in between the two to my tile list. <img src=\"/blog/images/old/72fe22250e32ef5dfab3880b62fdb1f3.png\" alt=\"\"> The bounding box coordinates deserve a further note. Bounding boxes cannot be projected between non-rectangular coordinate systems. Here is an example showing a lat/lon bounding box that works fine in Mercator, projected to the Albers projection. Not only do we get too much empty space but we’re also missing significant parts of U.S. territory. <img src=\"/blog/images/old/073e5f51eef85b4206c2d4880352b499.png\" alt=\"\"> So instead we have to do the reverse approach and grab the Albers bounding box and then project it back to WGS 84 lat/lon (I used QGIS for this step). This bounding box I then used <a href=\"https://gist.github.com/gka/52ef3bf442bed80367f6#file-make-tile-urls-py-L21\" target=\"_blank\" rel=\"noopener\">in the Python script</a> to generate the tile urls. <img src=\"/blog/images/old/a14386ba2ae66a99aae91e9299dc67a9.png\" alt=\"\"> Finally we run <code>tilestache-seed.py</code> to pre-generate all the tiles in our list (you probably want to host them as static files somewhere), which may or may not take quite a while to finish.</p><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python make-tile-urls.py</span><br><span class=\"line\">tilestache-seed.py -c config.json --tile-list=tile-urls.txt --layer=map</span><br></pre></td></tr></table></figure><h2 id=\"step-3-translating-coordinates-in-web-map\"><a class=\"anchor\" href=\"#step-3-translating-coordinates-in-web-map\"><span class=\"header-anchor\">#</span></a> Step 3: Translating coordinates in web map</h2><p>Once we generated all our tiles we almost made it to the end. The tiles can be used just like Mercator tiles, so you are free to pick your favorite tilemap framework such as <a href=\"http://leafletjs.com/\" target=\"_blank\" rel=\"noopener\">Leaflet.js</a>, <a href=\"http://polymaps.org\" target=\"_blank\" rel=\"noopener\">Polymaps</a>, <a href=\"http://openlayers.org/\" target=\"_blank\" rel=\"noopener\">OpenLayers</a> or whatever you prefer. For our women employment map I went with <a href=\"http://modestmaps.com/\" target=\"_blank\" rel=\"noopener\">ModestMaps</a> which I like for its simplicity and “hackability”. However, all of these frameworks assume that your tiles are in Mercator projection, so the built-in conversion from WGS 84 lat/lon to Mercator tile coordinates won’t work for us – unless we do the same trick I showed above. First, we are going to need the two projections Web Mercator and Albers, which you find in the <a href=\"http://proj4js.org/\" target=\"_blank\" rel=\"noopener\">Proj.4 JavaScript fork</a>. Needless to say that you should use the exact same projection definition here.</p><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Web Mercator</span></span><br><span class=\"line\"><span class=\"keyword\">var</span> mercator = proj4(<span class=\"string\">'+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 '</span></span><br><span class=\"line\">               + <span class=\"string\">'+x_0=0.0 +y_0=0 +k=1.0 +units=m +nadgrids=@null +wktext '</span></span><br><span class=\"line\">               + <span class=\"string\">'+over +no_defs'</span>),</span><br><span class=\"line\">    <span class=\"comment\">// Albers Equal Area</span></span><br><span class=\"line\">    albers = proj4(<span class=\"string\">'+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 '</span></span><br><span class=\"line\">              + <span class=\"string\">'+x_0=0 +y_0=0 +ellps=sphere +nadgrids=@null '</span></span><br><span class=\"line\">              + <span class=\"string\">'+towgs84=0,0,0,0,0,0,0 +units=m +no_defs'</span>);</span><br></pre></td></tr></table></figure><p>Using these projection classes we can convert coordinates from the ‘real’ WGS 84 lat/lon coordinates to the ‘wrong’ lat/lon coordinates on our map, by first projecting to Albers, and then inverse projecting “back” from Web Mercator.</p><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">WGS84ToMap</span>(<span class=\"params\">pt</span>) </span>&#123;</span><br><span class=\"line\">    pt = mercator.inverse(albers.forward([pt.lon, pt.lat]));</span><br><span class=\"line\">    <span class=\"keyword\">return</span> &#123; <span class=\"attr\">lon</span>: pt[<span class=\"number\">0</span>], <span class=\"attr\">lat</span>: pt[<span class=\"number\">1</span>] &#125;;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><p>The same works in the reverse direction, too. I used this to display the current map coordinates in the URL hash, simply by passing the map center (the ‘wrong’ coordinates I get from <code>map.getCenter()</code>) to <code>mapToWGS84()</code>.</p><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">mapToWGS84</span>(<span class=\"params\">pt</span>) </span>&#123;</span><br><span class=\"line\">    pt = albers.inverse(mercator.forward([pt.lon, pt.lat]));</span><br><span class=\"line\">    <span class=\"keyword\">return</span> &#123; <span class=\"attr\">lon</span>: pt[<span class=\"number\">0</span>], <span class=\"attr\">lat</span>: pt[<span class=\"number\">1</span>] &#125;;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><p>And that’s basically all it needed to get the tile map library working with my custom projected tiles. Of course there is a lot more I could write about (and I will in future posts), but for now, let’s wrap this up: Making tile maps using custom projections isn’t that hard, and the beautiful maps are definitely worth it. You can use Tilemill just as usual, export Mapnik XML, change the projection in the XML and then render the tiles using TileStache. You can use these tiles in any tile map framework, but you have to convert coordinates from and to your ‘wrong’ lat/lons. If you have further questions or know a different approach for tile maps in custom projections, let us know in the comment section. Also, if you happen to work at <a href=\"https://www.mapbox.com/\" target=\"_blank\" rel=\"noopener\">Mapbox</a> or another company that produces tile map tools with a built-in limitation to rectangular projections, I would be curious to hear why you’re not willing to go the extra mile for the sake of beautiful maps. And in case you find this blog post helpful, please <a href=\"https://twitter.com/intent/tweet?text=How%20to%20make%20tile%20maps%20without%20using%20the%20Mercator%20projection%20(via%20@driven_by_data)&amp;url=http://vis4.net/blog/posts/no-more-mercator-tiles/\" target=\"_blank\" rel=\"noopener\">tweet it</a>! 😃</p><h1 id=\"comments\"><a class=\"anchor\" href=\"#comments\"><span class=\"header-anchor\">#</span></a> Comments</h1><p>Hannes (Jan 07, 2015)</p><blockquote><p>You should be able to find lots of software in/from the geo world, rather than the JS world, that was designed with support for any projection in mind. OpenLayers can handle any projection, there also is a Proj4 plugin for Leaflet. There are standards like TMS or WMS variants. This might make it all easier and less “through the backdoor”. 😉</p><p>Seeing web mercator being used for everything, just because some software dictated or suggested it, is painful for people with a cartographic background.</p></blockquote><p>Gregor (Jan 07, 2015)</p><blockquote><p>Indeed. I saw that OpenLayers can deal with projections and that there is a Leaflet Plugin for Proj4. I guess I partly wrote this post to convince maintainers of Mercator-bound geo software to consider adding support for custom projected tiles. The coordinate transformation should really be part of the frameworks,  and not of the map code.</p></blockquote><p>Calvin (Jan 07, 2015)</p><blockquote><p>Jason Davies’ work on <a href=\"http://www.jasondavies.com/maps/raster/\" rel=\"noopener\" target=\"_blank\">projected tiles</a> is pretty good and avoids the pitfall your solution has which is interoperability, the reason for using mercator tiles is because thats that the other tiles are in and for most maps it’s good enough.</p></blockquote><p>Gregor (Jan 07, 2015)</p><blockquote><p>Yeah, I know his work on projected tiles. It’s a nice trick, but it wouldn’t work in a real use case as the map labels get rotated and look blurry. You want crisp and horizontally aligned text on a map, right?</p></blockquote><p>s (Jan 07, 2015)</p><blockquote><p>Why not simply use a d3 projection for your project? I see no need for tiles.</p></blockquote><p>Gregor (Jan 07, 2015)</p><blockquote><p>Ha, I’m curious to hear how you would render this detail view of NYC just using D3.</p><p><a href=\"http://new.tinygrab.com/f3aa221ede71c30897f1a0998015ed988375dffdfc.png\" target=\"_blank\" rel=\"noopener\">http://new.tinygrab.com/f3aa221ede71c30897f1a0998015ed988375dffdfc.png</a></p><p>At the very least you would need vector tiles instead of image tiles, since the streets alone are several gigabytes.</p></blockquote><p>Steven Romalewski (Jan 08, 2015)</p><blockquote><p>This is a helpful post.  But following on Calvin’s comment, hasn’t the sole reason for using the Web Mercator projection been so you can overlay tiles on now-ubiquitous basemaps such as Google’s?</p><p>I agree that we shouldn’t “blindly accept” the web Mercator projection, but there’s a reason for it, whether it’s a good reason or not.  I thought it would have made your post stronger if you had stated that obvious but perhaps overlooked point.</p><p>In other words, it wasn’t like we all just woke up yesterday and realized that we should be making maps in other projections.  There are thousands of map projections/coordinate systems that have been used for decades (centuries?), each with a particular purpose and/or location in mind.</p><p>But if you want to easily overlay your choropleth or other map layers on a pre-rendered basemap, for now there’s Google, Bing, MapQuest, and OSM and its variants.  And those are all web Mercator.</p><p>Noticeably the Times’s map in your blog post (which is very nice, btw) does not have much of a basemap other than neighborhood labels and streets.  True, you could argue that it doesn’t need any more than that.  But for now, if you don’t use web Mercator, you can’t use an “off-the-shelf” basemap - you’d need to create your own in the same custom projection, not necessarily a trivial task.</p><p>Anyway, you’re right that “making tile maps using custom projections” isn’t complicated. But I thought the reason <em>why</em> we’ve all been using web Mercator should’ve been pointed out.  And it’d be nice if we could overlay non-web Mercator tiles on the now-standard basemaps (or, better yet, reproject those pre-rendered basemaps on the fly).  But maybe not being able to do that is (one of) the price(s) we’ve been paying for these otherwise “free” services.</p></blockquote><p>Tom (Jan 08, 2015)</p><blockquote><p>Okay, I’ll bite.</p><p>Reasons why Mapbox doesn’t support non-Mercator projections (yet):</p><p>Projection definitions are stuck in the 80s: you have to look up some string defining a projection and copy and paste it from place to place, or rely on a shorthand to that string and hope that it’s the right shorthand and you have the right CSV of projections stashed somewhere on your computer.</p><p>This is a fixable problem, but currently no-one has the technical and political skills to do it as well as a good reason to spend a lot of time on it.</p><p>Next: “compatibility over the internet”: the sort of problem for which other technology has the html ‘lang’ attribute and magic numbers in binary files and so on. There is nothing for this: having Leaflet know what projection a layer is in, without copying and pasting JavaScript or some hopefully-accurate proj4 string is just not a reality.</p><p>Which brings us to the user expectation that layered maps should work. Back in the day, I worked with OpenLayers, which offered glorious projection support: really, it had gone all-in on making everything projection-related possible, though not easy. And this was the cause of a tremendous percentage of all support: it is a reasonable expectation that you can combine map layers of different projections and it’ll work, but it can’t. Tiles are baked images and they are baked in projections, never to be warped without significant artifacting (see MapProxy).</p><p>So, metadata doesn’t exist, user expectations are not set, finally tiling. Virtually all tiled sources use the OSM “XYZ” tiling scheme, which is only defined for Spherical Mercator and, for each other projection, is a big question mark. Schemes that try to resolve this, like WMTS, have fallen into the overstandardized complexity gotcha, with each projection also having sort of meta-choices around exactly where you want to slice and how. They’re too big to be popular, when XYZ tiles are so simple a novice coder can write a tiling client.</p><p>Which brings us to vector tiles. The popular conception of vector tiles is that they’re “just vector data” and thus all further choices are free. Unfortunately, this isn’t true: vector tiles are <em>tiles</em>, and to be efficient they need to be simplified. An efficient simplification is perceptual, so they’re simplified to the tiling scheme, which is - you guessed it - Spherical Mercator. A bummer, but the upside is that dynamic reprojection of vector tiles has radically less distortion than reprojection of raster tiles, so it is a big win.</p><p>I hope this clears up some of the rationale for why this isn’t easy to do technically. I’ll close with two quick non-technical notes:</p><p>Firstly: demand for non-Mercator projections is a high-level user request from people who are really into cartography. Projections are invisible for the vast majority of developers, designers, and consumers. And for people who are into cartography but also need a different dimension - like tons of data or easy compatibility with non-Leaflet APIs - the complexity disadvantages of custom projections outweigh the wins, fast.</p><p>The problem here is that, while this was doable for you, it consisted of four technical steps that all have decisions baked in - decisions that you can make and you can accept at each level. You made the map, you know it’s in a projection, you can configure Leaflet and know that it isn’t going to work with a different tile layer, or a map you make in a different projection. The problem that a larger system (eg Mapbox) would have to solve is to make a general solution for each of these steps, as well as all of the bumpers to make this less of a usability nightmare: how do you use this map in iOS? In GL? If you have related information to geocode from, is the vector tile data projected as well? Or not projected, in a separate tileset?</p></blockquote><p>Paul Ramsey (Jan 08, 2015)</p><blockquote><p>Agree w/ everything except the metadata argument at the top. OGC WKT for SRS is “good enough” to represent all kinds of projections in a nice standard way, and even includes niceties like spheroid/datum handling (which proj is more flakey about). And handling it is a solved problem. Death to ESPG codes.</p></blockquote><p>Matthew (Jan 08, 2015)</p><blockquote><p>The cartographic argument for non-Mercator maps is compelling - I too like looking at maps in other projections - but I think it’s a fair point that, if all you’re doing is plotting some data on a map, the interop advantages to Mercator are worth the compromise.</p><p>&gt; Projections are invisible for the vast majority of developers…</p><p>For anything beyond simply dumping your data on a map, that’s precisely the problem. Sweeping the complexity of projections under the rug and saying “look how easy this is now” is not a solution. Projections fundamentally alter the results of geospatial analyses; they are a necessary and fundamental core concept, an absolute prerequisite to doing anything with geographic data.</p><p>Any analysis with spatial relationships, area, distance, direction and geometric overlays are compromised if you are working in the wrong projection.  I can’t wait to see what stupid mistakes arise “doing GIS” in javascript without consideration for projections.</p></blockquote><p>Tom (Jan 08, 2015)</p><blockquote><p>This is a discussion of <em>output</em> in projections for display. This is entirely separate to how data is stored, or in which projection it is processed - I don’t see how jabs at ‘GIS in JavaScript’ are relevant. Nobody stores their raw data in ‘mercator’ and nobody’s suggesting that.</p></blockquote><p>Matthew (Jan 08, 2015)</p><blockquote><p>Yeah sorry, the jab at “GIS in Javascript” was unnecessary. It’s more a jab at the general idea that you can do anything significant with geographic data without consideration for projections. You can’t sweep it under the rug: projections have real implications for storing, analyzing <em>and</em> displaying any geographic data. The “it doesn’t matter, just use Mercator” approach is not ultimately helpful to the geospatial developer community.</p></blockquote><p>Gregor (Jan 08, 2015)</p><blockquote><p>Btw, Mike just pointed me to this demonstration of streets rendered from vector tiles in D3: <a href=\"http://bl.ocks.org/mbostock/5593150\" target=\"_blank\" rel=\"noopener\">http://bl.ocks.org/mbostock/5593150</a></p></blockquote><p>Tom (Jan 08, 2015)</p><blockquote><p>Then we’re in agreement: it is extremely important to know about geographic data in depth if you’re going to analyze it. The difference between projections <em>in data</em> and <em>in output</em> is important: nobody’s should do their analysis in web mercator. The output of their analysis on a web mercator map is a totally different concern.</p><p>And I don’t mean to belittle projections in general: they are useful and interesting, and for some analysis, mercator really doesn’t cut it (mostly for the arctic). I think in a lot of cases, mercator <em>does</em> cut it: a person doing a scaled-point map in their small American city shouldn’t spend an hour choosing their projection, just like you shouldn’t spend a bunch of time choosing fonts in your school essays: that’s a distraction.</p><p>But it is important to understand <em>why</em> mercator is the default in so many places and the technical issues surrounding it. That’s the only way we can really grasp what’s on the todo list to make custom output projections less of a technical gotcha.</p></blockquote><p>Gregor (Jan 08, 2015)</p><blockquote><p>Tom, thanks so much for helping us to understand this.</p><p>I agree, once you’re at the point where you want to combine different map layers you get into big trouble. And while I can see why Mapbox is not investing more resources into this feature, there are still a few (simple) things that could significantly help making custom projected tiles in Tilemill:</p><ul><li>Allowing to set the projection per project and simply passing it on to Mapnik. If I can preview maps in custom projections easily in TileStache, this should work in Tilemill, too. This could be a feature marked as ‘experimental’ with big warning signs to keep away less ‘high-level’ users.</li><li>It would be nice if Tilemill could export separate Mapnik XML styles for Retina tiles.  I found a way to do this using a global @scale variable to rescale line withs and font sizes, and shifting the zoom level.</li><li>Finally, Tilemill could also allow exporting tiles in custom projection, essentially just by letting Mapnik do it’s job. Then end-users like me would not need to use TileStache.</li></ul><p>Maybe I try hacking this into Tilemill one day…</p></blockquote><p>Bill (Jan 08, 2015)</p><blockquote><p>Gregor, I’m probably oversimplifying for your specific use case, but I’ve always found it easy enough to use custom projections in Tilemill without resorting to the Mapnik XML or a custom tile server. By lying to Tilemill when loading layers (“Why yes, computer friend, this shapefile IS in EPSG:900913!”), you can <a href=\"http://wboykinm.github.io/income-2012/#5/16.067/-1.362\" rel=\"noopener\" target=\"_blank\">render in Albers US</a>, then bake the tiles to mapbox hosting or chop them up with mbutil for static serving. The only downside is the obviously-incorrect internal geolocation (note the hash).</p></blockquote><p>Friedrich Hartmann (Jan 09, 2015)</p><blockquote><p>First I want to point out that Spherical Mercator still excels for global coverage of raster data to be viewed at small scales (street level to region) due to it’s conformal properties. It’s just ill-suited for continental to global map view extents.</p><p>And since, in the past, clients were only meant to stitch image tiles back together the tiling was inherently linked to the map projection since all the labels were already baked in and distorting them by means of reprojection simply would make things look not as nice as they should be.</p><p>This problem and limitation is avoidable in client side rendering of vector data as stated here before, the hard dependency between the geo data’s coordinate system and the displayed map projection is not in existence anymore.  So why keep the old tiling schemes? Projection definitions are definitely not the problem here, neither are they stuck in the 80’s nor are they inherently hard to understand, especially the so called geometric projections. Also there is the need to be made a distinction between the data’s geographic coordinate system and Datum (WGS84, NAD83) and projected coordinate systems like UTM called coordinate reference systems (CRS), these all are different beast. The part that is responsible for the complexity of the CRS string is not the projection it’s the arbitrary definition of the different coordinate systems and the transformation between them.</p><p>I see it as eventually inevitable to adopt a better tiling scheme for global vector data than an equirectangular or Mercator grid. They both have at least some major drawbacks, compared to better suited hierarchical geodesic grids. One is being not equal area, which means an uneven number of calls and different amount of area coverage per tile based on latitude of the tile at the same scale. Where the equirectangular grid merely has two singularities,  the spherical Mercator square has no coverage at all, at the poles beyond 85°, which disqualifies it as a global all purpose tiling scheme all together. An improvement would be to replace the usual rectangular data tiling with something more appropriate for the type of data we’re dealing with here, which are in the end points on an ellipsoid or sphere; in most cases a sphere will do.</p><p>There exist some not too complex alternatives like the Icosahedron Snyder Equal Area (ISEA) grid or HEALPix, which is well established and has quite solid support and libraries around it in the astronomical community (check out TPZF/GlobWeb on GitHub). Another benefit of having an equal area grid is being able to make statistical comparison between tiles on the same hierarchy level, like nodes per km², which is not dorictly possible in the current grids used for vector map tiles.</p><p>Doing the server side generalization within the projected CRS of the geodesic grid tiles would further harmonize the level of generalization across the globe, generalizing in equirectangular Cartesian space is a bad idea to begin with.</p><p>To sum up, I strongly recommend adopting a geodesic grid based tiling scheme for vector based maps.</p></blockquote><p>Aaron Dennis (Jan 09, 2015)</p><blockquote><p>I hate looking at the whole US in web mercator, too, but I find it equally unattractive to see zoomed in versions of the states on the east and west coasts that are tilted because the central meridian of their Albers projection is a thousand miles away.</p><p>Beyond interoperability with other web maps, Mercator is actually great at preserving local angles. If you want a map that let’s you zoom to any place and scale in the country, Mercator might be the best choice.</p><p>It seems the solution might be vector tiles projected “on the fly.” We’ll probably get there, but Mercator does okay for now.</p></blockquote><p>Gregor (Jan 09, 2015)</p><blockquote><p>I could hold against your argument that (1) the idea of North direction showing up is a rather artificial concept anyway and that (2) this wouldn’t be the first map where North is not showing up and also that (3) North being <em>exactly</em> up isn’t a really important feature of a map unless you looking for shortest way to the North pole — but still I can see your point. Especially when you look at Alaska and Hawaii, which are shown equal-area but ‘rotated’ to an unfortunate degree.</p><p>Still better than Mercator in the full U.S. view. Guess next time I switch from Albers to Mercator tiles at some zoom level.</p></blockquote><p>Friedrich Hartmann (Jan 10, 2015)</p><blockquote><p>Albers also preservers angles within and around it’s standard parallels very well. And Lambert conformal conic projection even preserves them everywhere. It’s basically the same as Mercator but using a cylinder as projective Surface.</p><p>For interactive maps there is even the possibility to have dynamic projection parameters based on the active view scale. While this is hard to realize for raster tile maps (no smooth transitions feasible due to fixed number of zoom scales), it’s quite realistic to do this for vector based ones. Continental view would be Lambert, as you zoom in it’s being gradually transitioned into Mercator, by interpolating between the two projections, this way it’s even directly compatible with raster tiles on higher zoom levels.</p><p><a href=\"http://cartography.oregonstate.edu/ScaleAdaptiveWebMapProjections.html\" target=\"_blank\" rel=\"noopener\">http://cartography.oregonstate.edu/ScaleAdaptiveWebMapProjections.html</a></p></blockquote><p>Jamie Robertson (Mar 20, 2015)</p><blockquote><p>Great Post! I’ve been working on my own tiles and they are working great in the tilestache previewer. I’m struggling with getting the last few functions you noted in step 3 to work with leaflet. Do you happen to have some example code for that, or with modestmaps? Thanks!</p></blockquote><p>Chris (Jan 16, 2015)</p><blockquote><p>Saw this a bit late but thought i’d add a few notes that might be helpful to those who want to try using different projections in web maps.</p><p>As others have already hinted for a zoomable map in conic projection Lambert is probably a better choice than Albers, for a whole country map showing statistical quantities the equal-area property might be considered more important of course.</p><p>The most difficulties arise when your projection includes one of the poles or the 180 degree meridian.  Then you nearly always run into issues resulting in artefacts of some kind that require some more work than just setting the projection in your software.</p><p>And finally when you are advocating Mercator keep in mind that you cannot show the whole world in Mercator, web maps are cut off at 85 degrees and are often unusable beyond about 75 degrees since map styling is way off with regards to scale then.  This does not mean it cannot work great for other parts of the planet of course.  But while for lower latitudes use of other projections is just often better and nicer for polar regions it is essential.</p></blockquote>",
    "contentSnippet": "Using open source tools it is now super easy to make your own map tiles, and with a little extra work you can render them in whatever map projection you want. No more excuses to use Mercator! For example, here is a map we published today at The Upshot. It shows where prime-age women are working more or less then average, and includes data from county-level in the overview map down to every census tract once you zoom in. And all is nicely projected in Albers Equal-Area Conic, a projection widely adopted as standard for U.S. maps.\n\n\n\n# So how does this work?\nAfter many years of blindly accepting the dominance of Web Mercator tile maps, I was quite surprised to learn how easy it is to use whatever projection you want. So how does this work? The answer is that it works because Mapnik, the core of many open source tile mapping frameworks, supports custom projections out of the box. It is just the tools built around Mapnik that are not supporting other projections. Tilemill, for instance, is a super nice tool for styling map tiles in a CSS like language. But if you export your map as tiles using Tilemill, you are stuck with Web Mercator, even though internally it is Mapnik that renders all the tiles. Fortunately this doesn’t mean that you have to deal with Mapnik and it’s quirks directly to get custom projections.\n# Step 1: Export Mapnik XML and change projection\nSo the first step is to style your map just as you would with a Mercator map, enjoying the full feature-set of Tilemill. Once you’rd done with that you export your project as Mapnik XML. Think of this XML file as the entire description of your map. It contains all the references to the source layers and all the styles for the map features – in one single file. If you actually read the code of the file you will quickly realize how extremely lucky we are to have Tilemill and CartoCSS. And you might also notice that in the root element of the document, named <Map>, you find the the definition of the map projection as Proj.4 string in the srs attribute (for spatial reference system). And you can simply change it to whatever you want. In this case I replaced the Mercator projection with the Albers projection (copied from the Proj.4 definition linked here):\n\n\n1\n2\n3\n4\n\n<Map srs=\"+proj=aea +lat_1=29.5 +lat_2=45.5\n   +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +ellps=sphere\n   +towgs84=0,0,0,0,0,0,0 +units=m +no_defs +nadgrids=@null\"\n   background-color=\"#ffffff\">\n\n\nI copied the Proj.4 string from spatialreference.org, where you find many other standard projections as well. (Hint: If you plan on aligning the tile map to a D3.js projected vector overlay you should append +nadgrids=@null to the Proj.4 string) To preview the projection I used TileStache. All it needs is a JSON configuration file that points to the exported Mapnik XML as input layer.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n{\n    \"cache\": {\n        \"name\": \"Disk\", \"path\": \"tiles\"\n    },\n    \"layers\": {\n        \"map\": {\n            \"provider\": {\"name\": \"mapnik\", \"mapfile\": \"mapnik.xml\"},\n            \"projection\": \"spherical mercator\"\n        }\n    }\n}\n\n\nThen you run tilestache-server.py with your config and open http://localhost:8080/map/preview.html to preview the freshly rendered projected tiles, just as you would in Tilemill.\n# Step 2: Figuring out which tiles to render\nNow we come to the first tricky part of the process. We need to figure out which tiles we actually want to be rendered. You probably already know the zoom levels and the bounding box in WGS 84 lat/lon coordinates, but the latter won’t help us much since TileStache is designed for rectangular projections like Mercator. Fortunately TileStache also takes a text file with a list of Z/X/Y tile coordinates as input. To get this list of tiles I wrote a Python script (feel free to re-use if you want). The script uses mercantile, a Python library for tile calculations, which returns the tile coordinates for points in WGS84 lat/lon, assuming that the tiles are projected in Web Mercator. To trick the library into giving me the correct tiles, I converted the points to the custom projection first and then projected them “back” from Web Mercator (even though the coordinates aren’t in Mercator). When mercantile gets my “fake” lat/lon coordinates it projects them to Mercator (reversing my inverse projection) and ends up with the Albers Equal Area coordinates.\n\n\n1\n2\n3\n4\n5\n6\n\ndef get_tile(lon, lat, z):\n    # project to custom projection\n    pt = albers(lon, lat)\n    # project \"back\" from Mercator\n    pt2 = mercator(pt[0], pt[1], inverse=True)\n    return mercantile.tile(pt2[0],pt2[1], z)\n\n\nUsing this function I then compute the top-left and bottom-right tile for each zoom level and add every tile in between the two to my tile list.  The bounding box coordinates deserve a further note. Bounding boxes cannot be projected between non-rectangular coordinate systems. Here is an example showing a lat/lon bounding box that works fine in Mercator, projected to the Albers projection. Not only do we get too much empty space but we’re also missing significant parts of U.S. territory.  So instead we have to do the reverse approach and grab the Albers bounding box and then project it back to WGS 84 lat/lon (I used QGIS for this step). This bounding box I then used in the Python script to generate the tile urls.  Finally we run tilestache-seed.py to pre-generate all the tiles in our list (you probably want to host them as static files somewhere), which may or may not take quite a while to finish.\n\n\n1\n2\n\npython make-tile-urls.py\ntilestache-seed.py -c config.json --tile-list=tile-urls.txt --layer=map\n\n\n# Step 3: Translating coordinates in web map\nOnce we generated all our tiles we almost made it to the end. The tiles can be used just like Mercator tiles, so you are free to pick your favorite tilemap framework such as Leaflet.js, Polymaps, OpenLayers or whatever you prefer. For our women employment map I went with ModestMaps which I like for its simplicity and “hackability”. However, all of these frameworks assume that your tiles are in Mercator projection, so the built-in conversion from WGS 84 lat/lon to Mercator tile coordinates won’t work for us – unless we do the same trick I showed above. First, we are going to need the two projections Web Mercator and Albers, which you find in the Proj.4 JavaScript fork. Needless to say that you should use the exact same projection definition here.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n// Web Mercator\nvar mercator = proj4('+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 '\n               + '+x_0=0.0 +y_0=0 +k=1.0 +units=m +nadgrids=@null +wktext '\n               + '+over +no_defs'),\n    // Albers Equal Area\n    albers = proj4('+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 '\n              + '+x_0=0 +y_0=0 +ellps=sphere +nadgrids=@null '\n              + '+towgs84=0,0,0,0,0,0,0 +units=m +no_defs');\n\n\nUsing these projection classes we can convert coordinates from the ‘real’ WGS 84 lat/lon coordinates to the ‘wrong’ lat/lon coordinates on our map, by first projecting to Albers, and then inverse projecting “back” from Web Mercator.\n\n\n1\n2\n3\n4\n\nfunction WGS84ToMap(pt) {\n    pt = mercator.inverse(albers.forward([pt.lon, pt.lat]));\n    return { lon: pt[0], lat: pt[1] };\n}\n\n\nThe same works in the reverse direction, too. I used this to display the current map coordinates in the URL hash, simply by passing the map center (the ‘wrong’ coordinates I get from map.getCenter()) to mapToWGS84().\n\n\n1\n2\n3\n4\n\nfunction mapToWGS84(pt) {\n    pt = albers.inverse(mercator.forward([pt.lon, pt.lat]));\n    return { lon: pt[0], lat: pt[1] };\n}\n\n\nAnd that’s basically all it needed to get the tile map library working with my custom projected tiles. Of course there is a lot more I could write about (and I will in future posts), but for now, let’s wrap this up: Making tile maps using custom projections isn’t that hard, and the beautiful maps are definitely worth it. You can use Tilemill just as usual, export Mapnik XML, change the projection in the XML and then render the tiles using TileStache. You can use these tiles in any tile map framework, but you have to convert coordinates from and to your ‘wrong’ lat/lons. If you have further questions or know a different approach for tile maps in custom projections, let us know in the comment section. Also, if you happen to work at Mapbox or another company that produces tile map tools with a built-in limitation to rectangular projections, I would be curious to hear why you’re not willing to go the extra mile for the sake of beautiful maps. And in case you find this blog post helpful, please tweet it! 😃\n# Comments\nHannes (Jan 07, 2015)\n\nYou should be able to find lots of software in/from the geo world, rather than the JS world, that was designed with support for any projection in mind. OpenLayers can handle any projection, there also is a Proj4 plugin for Leaflet. There are standards like TMS or WMS variants. This might make it all easier and less “through the backdoor”. 😉\nSeeing web mercator being used for everything, just because some software dictated or suggested it, is painful for people with a cartographic background.\n\nGregor (Jan 07, 2015)\n\nIndeed. I saw that OpenLayers can deal with projections and that there is a Leaflet Plugin for Proj4. I guess I partly wrote this post to convince maintainers of Mercator-bound geo software to consider adding support for custom projected tiles. The coordinate transformation should really be part of the frameworks,  and not of the map code.\n\nCalvin (Jan 07, 2015)\n\nJason Davies’ work on projected tiles is pretty good and avoids the pitfall your solution has which is interoperability, the reason for using mercator tiles is because thats that the other tiles are in and for most maps it’s good enough.\n\nGregor (Jan 07, 2015)\n\nYeah, I know his work on projected tiles. It’s a nice trick, but it wouldn’t work in a real use case as the map labels get rotated and look blurry. You want crisp and horizontally aligned text on a map, right?\n\ns (Jan 07, 2015)\n\nWhy not simply use a d3 projection for your project? I see no need for tiles.\n\nGregor (Jan 07, 2015)\n\nHa, I’m curious to hear how you would render this detail view of NYC just using D3.\nhttp://new.tinygrab.com/f3aa221ede71c30897f1a0998015ed988375dffdfc.png\nAt the very least you would need vector tiles instead of image tiles, since the streets alone are several gigabytes.\n\nSteven Romalewski (Jan 08, 2015)\n\nThis is a helpful post.  But following on Calvin’s comment, hasn’t the sole reason for using the Web Mercator projection been so you can overlay tiles on now-ubiquitous basemaps such as Google’s?\nI agree that we shouldn’t “blindly accept” the web Mercator projection, but there’s a reason for it, whether it’s a good reason or not.  I thought it would have made your post stronger if you had stated that obvious but perhaps overlooked point.\nIn other words, it wasn’t like we all just woke up yesterday and realized that we should be making maps in other projections.  There are thousands of map projections/coordinate systems that have been used for decades (centuries?), each with a particular purpose and/or location in mind.\nBut if you want to easily overlay your choropleth or other map layers on a pre-rendered basemap, for now there’s Google, Bing, MapQuest, and OSM and its variants.  And those are all web Mercator.\nNoticeably the Times’s map in your blog post (which is very nice, btw) does not have much of a basemap other than neighborhood labels and streets.  True, you could argue that it doesn’t need any more than that.  But for now, if you don’t use web Mercator, you can’t use an “off-the-shelf” basemap - you’d need to create your own in the same custom projection, not necessarily a trivial task.\nAnyway, you’re right that “making tile maps using custom projections” isn’t complicated. But I thought the reason why we’ve all been using web Mercator should’ve been pointed out.  And it’d be nice if we could overlay non-web Mercator tiles on the now-standard basemaps (or, better yet, reproject those pre-rendered basemaps on the fly).  But maybe not being able to do that is (one of) the price(s) we’ve been paying for these otherwise “free” services.\n\nTom (Jan 08, 2015)\n\nOkay, I’ll bite.\nReasons why Mapbox doesn’t support non-Mercator projections (yet):\nProjection definitions are stuck in the 80s: you have to look up some string defining a projection and copy and paste it from place to place, or rely on a shorthand to that string and hope that it’s the right shorthand and you have the right CSV of projections stashed somewhere on your computer.\nThis is a fixable problem, but currently no-one has the technical and political skills to do it as well as a good reason to spend a lot of time on it.\nNext: “compatibility over the internet”: the sort of problem for which other technology has the html ‘lang’ attribute and magic numbers in binary files and so on. There is nothing for this: having Leaflet know what projection a layer is in, without copying and pasting JavaScript or some hopefully-accurate proj4 string is just not a reality.\nWhich brings us to the user expectation that layered maps should work. Back in the day, I worked with OpenLayers, which offered glorious projection support: really, it had gone all-in on making everything projection-related possible, though not easy. And this was the cause of a tremendous percentage of all support: it is a reasonable expectation that you can combine map layers of different projections and it’ll work, but it can’t. Tiles are baked images and they are baked in projections, never to be warped without significant artifacting (see MapProxy).\nSo, metadata doesn’t exist, user expectations are not set, finally tiling. Virtually all tiled sources use the OSM “XYZ” tiling scheme, which is only defined for Spherical Mercator and, for each other projection, is a big question mark. Schemes that try to resolve this, like WMTS, have fallen into the overstandardized complexity gotcha, with each projection also having sort of meta-choices around exactly where you want to slice and how. They’re too big to be popular, when XYZ tiles are so simple a novice coder can write a tiling client.\nWhich brings us to vector tiles. The popular conception of vector tiles is that they’re “just vector data” and thus all further choices are free. Unfortunately, this isn’t true: vector tiles are tiles, and to be efficient they need to be simplified. An efficient simplification is perceptual, so they’re simplified to the tiling scheme, which is - you guessed it - Spherical Mercator. A bummer, but the upside is that dynamic reprojection of vector tiles has radically less distortion than reprojection of raster tiles, so it is a big win.\nI hope this clears up some of the rationale for why this isn’t easy to do technically. I’ll close with two quick non-technical notes:\nFirstly: demand for non-Mercator projections is a high-level user request from people who are really into cartography. Projections are invisible for the vast majority of developers, designers, and consumers. And for people who are into cartography but also need a different dimension - like tons of data or easy compatibility with non-Leaflet APIs - the complexity disadvantages of custom projections outweigh the wins, fast.\nThe problem here is that, while this was doable for you, it consisted of four technical steps that all have decisions baked in - decisions that you can make and you can accept at each level. You made the map, you know it’s in a projection, you can configure Leaflet and know that it isn’t going to work with a different tile layer, or a map you make in a different projection. The problem that a larger system (eg Mapbox) would have to solve is to make a general solution for each of these steps, as well as all of the bumpers to make this less of a usability nightmare: how do you use this map in iOS? In GL? If you have related information to geocode from, is the vector tile data projected as well? Or not projected, in a separate tileset?\n\nPaul Ramsey (Jan 08, 2015)\n\nAgree w/ everything except the metadata argument at the top. OGC WKT for SRS is “good enough” to represent all kinds of projections in a nice standard way, and even includes niceties like spheroid/datum handling (which proj is more flakey about). And handling it is a solved problem. Death to ESPG codes.\n\nMatthew (Jan 08, 2015)\n\nThe cartographic argument for non-Mercator maps is compelling - I too like looking at maps in other projections - but I think it’s a fair point that, if all you’re doing is plotting some data on a map, the interop advantages to Mercator are worth the compromise.\n> Projections are invisible for the vast majority of developers…\nFor anything beyond simply dumping your data on a map, that’s precisely the problem. Sweeping the complexity of projections under the rug and saying “look how easy this is now” is not a solution. Projections fundamentally alter the results of geospatial analyses; they are a necessary and fundamental core concept, an absolute prerequisite to doing anything with geographic data.\nAny analysis with spatial relationships, area, distance, direction and geometric overlays are compromised if you are working in the wrong projection.  I can’t wait to see what stupid mistakes arise “doing GIS” in javascript without consideration for projections.\n\nTom (Jan 08, 2015)\n\nThis is a discussion of output in projections for display. This is entirely separate to how data is stored, or in which projection it is processed - I don’t see how jabs at ‘GIS in JavaScript’ are relevant. Nobody stores their raw data in ‘mercator’ and nobody’s suggesting that.\n\nMatthew (Jan 08, 2015)\n\nYeah sorry, the jab at “GIS in Javascript” was unnecessary. It’s more a jab at the general idea that you can do anything significant with geographic data without consideration for projections. You can’t sweep it under the rug: projections have real implications for storing, analyzing and displaying any geographic data. The “it doesn’t matter, just use Mercator” approach is not ultimately helpful to the geospatial developer community.\n\nGregor (Jan 08, 2015)\n\nBtw, Mike just pointed me to this demonstration of streets rendered from vector tiles in D3: http://bl.ocks.org/mbostock/5593150\n\nTom (Jan 08, 2015)\n\nThen we’re in agreement: it is extremely important to know about geographic data in depth if you’re going to analyze it. The difference between projections in data and in output is important: nobody’s should do their analysis in web mercator. The output of their analysis on a web mercator map is a totally different concern.\nAnd I don’t mean to belittle projections in general: they are useful and interesting, and for some analysis, mercator really doesn’t cut it (mostly for the arctic). I think in a lot of cases, mercator does cut it: a person doing a scaled-point map in their small American city shouldn’t spend an hour choosing their projection, just like you shouldn’t spend a bunch of time choosing fonts in your school essays: that’s a distraction.\nBut it is important to understand why mercator is the default in so many places and the technical issues surrounding it. That’s the only way we can really grasp what’s on the todo list to make custom output projections less of a technical gotcha.\n\nGregor (Jan 08, 2015)\n\nTom, thanks so much for helping us to understand this.\nI agree, once you’re at the point where you want to combine different map layers you get into big trouble. And while I can see why Mapbox is not investing more resources into this feature, there are still a few (simple) things that could significantly help making custom projected tiles in Tilemill:\n\nAllowing to set the projection per project and simply passing it on to Mapnik. If I can preview maps in custom projections easily in TileStache, this should work in Tilemill, too. This could be a feature marked as ‘experimental’ with big warning signs to keep away less ‘high-level’ users.\nIt would be nice if Tilemill could export separate Mapnik XML styles for Retina tiles.  I found a way to do this using a global @scale variable to rescale line withs and font sizes, and shifting the zoom level.\nFinally, Tilemill could also allow exporting tiles in custom projection, essentially just by letting Mapnik do it’s job. Then end-users like me would not need to use TileStache.\n\nMaybe I try hacking this into Tilemill one day…\n\nBill (Jan 08, 2015)\n\nGregor, I’m probably oversimplifying for your specific use case, but I’ve always found it easy enough to use custom projections in Tilemill without resorting to the Mapnik XML or a custom tile server. By lying to Tilemill when loading layers (“Why yes, computer friend, this shapefile IS in EPSG:900913!”), you can render in Albers US, then bake the tiles to mapbox hosting or chop them up with mbutil for static serving. The only downside is the obviously-incorrect internal geolocation (note the hash).\n\nFriedrich Hartmann (Jan 09, 2015)\n\nFirst I want to point out that Spherical Mercator still excels for global coverage of raster data to be viewed at small scales (street level to region) due to it’s conformal properties. It’s just ill-suited for continental to global map view extents.\nAnd since, in the past, clients were only meant to stitch image tiles back together the tiling was inherently linked to the map projection since all the labels were already baked in and distorting them by means of reprojection simply would make things look not as nice as they should be.\nThis problem and limitation is avoidable in client side rendering of vector data as stated here before, the hard dependency between the geo data’s coordinate system and the displayed map projection is not in existence anymore.  So why keep the old tiling schemes? Projection definitions are definitely not the problem here, neither are they stuck in the 80’s nor are they inherently hard to understand, especially the so called geometric projections. Also there is the need to be made a distinction between the data’s geographic coordinate system and Datum (WGS84, NAD83) and projected coordinate systems like UTM called coordinate reference systems (CRS), these all are different beast. The part that is responsible for the complexity of the CRS string is not the projection it’s the arbitrary definition of the different coordinate systems and the transformation between them.\nI see it as eventually inevitable to adopt a better tiling scheme for global vector data than an equirectangular or Mercator grid. They both have at least some major drawbacks, compared to better suited hierarchical geodesic grids. One is being not equal area, which means an uneven number of calls and different amount of area coverage per tile based on latitude of the tile at the same scale. Where the equirectangular grid merely has two singularities,  the spherical Mercator square has no coverage at all, at the poles beyond 85°, which disqualifies it as a global all purpose tiling scheme all together. An improvement would be to replace the usual rectangular data tiling with something more appropriate for the type of data we’re dealing with here, which are in the end points on an ellipsoid or sphere; in most cases a sphere will do.\nThere exist some not too complex alternatives like the Icosahedron Snyder Equal Area (ISEA) grid or HEALPix, which is well established and has quite solid support and libraries around it in the astronomical community (check out TPZF/GlobWeb on GitHub). Another benefit of having an equal area grid is being able to make statistical comparison between tiles on the same hierarchy level, like nodes per km², which is not dorictly possible in the current grids used for vector map tiles.\nDoing the server side generalization within the projected CRS of the geodesic grid tiles would further harmonize the level of generalization across the globe, generalizing in equirectangular Cartesian space is a bad idea to begin with.\nTo sum up, I strongly recommend adopting a geodesic grid based tiling scheme for vector based maps.\n\nAaron Dennis (Jan 09, 2015)\n\nI hate looking at the whole US in web mercator, too, but I find it equally unattractive to see zoomed in versions of the states on the east and west coasts that are tilted because the central meridian of their Albers projection is a thousand miles away.\nBeyond interoperability with other web maps, Mercator is actually great at preserving local angles. If you want a map that let’s you zoom to any place and scale in the country, Mercator might be the best choice.\nIt seems the solution might be vector tiles projected “on the fly.” We’ll probably get there, but Mercator does okay for now.\n\nGregor (Jan 09, 2015)\n\nI could hold against your argument that (1) the idea of North direction showing up is a rather artificial concept anyway and that (2) this wouldn’t be the first map where North is not showing up and also that (3) North being exactly up isn’t a really important feature of a map unless you looking for shortest way to the North pole — but still I can see your point. Especially when you look at Alaska and Hawaii, which are shown equal-area but ‘rotated’ to an unfortunate degree.\nStill better than Mercator in the full U.S. view. Guess next time I switch from Albers to Mercator tiles at some zoom level.\n\nFriedrich Hartmann (Jan 10, 2015)\n\nAlbers also preservers angles within and around it’s standard parallels very well. And Lambert conformal conic projection even preserves them everywhere. It’s basically the same as Mercator but using a cylinder as projective Surface.\nFor interactive maps there is even the possibility to have dynamic projection parameters based on the active view scale. While this is hard to realize for raster tile maps (no smooth transitions feasible due to fixed number of zoom scales), it’s quite realistic to do this for vector based ones. Continental view would be Lambert, as you zoom in it’s being gradually transitioned into Mercator, by interpolating between the two projections, this way it’s even directly compatible with raster tiles on higher zoom levels.\nhttp://cartography.oregonstate.edu/ScaleAdaptiveWebMapProjections.html\n\nJamie Robertson (Mar 20, 2015)\n\nGreat Post! I’ve been working on my own tiles and they are working great in the tilestache previewer. I’m struggling with getting the last few functions you noted in step 3 to work with leaflet. Do you happen to have some example code for that, or with modestmaps? Thanks!\n\nChris (Jan 16, 2015)\n\nSaw this a bit late but thought i’d add a few notes that might be helpful to those who want to try using different projections in web maps.\nAs others have already hinted for a zoomable map in conic projection Lambert is probably a better choice than Albers, for a whole country map showing statistical quantities the equal-area property might be considered more important of course.\nThe most difficulties arise when your projection includes one of the poles or the 180 degree meridian.  Then you nearly always run into issues resulting in artefacts of some kind that require some more work than just setting the projection in your software.\nAnd finally when you are advocating Mercator keep in mind that you cannot show the whole world in Mercator, web maps are cut off at 85 degrees and are often unusable beyond about 75 degrees since map styling is way off with regards to scale then.  This does not mean it cannot work great for other parts of the planet of course.  But while for lower latitudes use of other projections is just often better and nicer for polar regions it is essential."
  },
  {
    "title": "When It's Ok to Use Word Clouds",
    "link": "https://vis4.net/blog/2015/01/when-its-ok-to-use-word-clouds/",
    "pubDate": "2015-01-30T17:30:28.000Z",
    "isoDate": "2015-01-30T17:30:28.000Z",
    "content": "<p><strong>tl;dr</strong> <em>It’s ok to use word clouds if your goal is to encourage reading of a large set of otherwise unrelated words that are connected to one or two interesting values (and word count in a text doesn’t qualify as interesting).</em></p><blockquote><p><a href=\"https://twitter.com/driven_by_data/status/560894208870195201\" target=\"_blank\" rel=\"noopener\">@driven_by_data</a>: In #datavis, no rule’s without exception. On rare occasions you can even use word clouds 😃</p></blockquote><p>This I <a href=\"https://twitter.com/driven_by_data/status/560894208870195201\" target=\"_blank\" rel=\"noopener\">tweeted yesterday</a> and now I feel that if I encourage the (dangerous) use of word clouds, I have to explain this exception in a little more detail. Why is it sometimes ok to use a widely rejected visualization method, and most times not?</p><p>A lot had been written about <a href=\"http://www.niemanlab.org/2011/10/word-clouds-considered-harmful/\" target=\"_blank\" rel=\"noopener\">why not to use</a> <a href=\"http://stephanieevergreen.com/word-cloud-dog-vomit/\" target=\"_blank\" rel=\"noopener\">word clouds</a>. It is super hard to decode or compare the values encoded in the font size or text color (if the latter isn’t random at all). Position, perhaps the most effective visual encoding, is essentially ‘wasted’ for the sake of a space filling algorithm (well, due to the spiraling nature of the layout algorithm, positions aren’t entirely random, but still, they don’t “mean” anything in a word cloud).</p><p>Plus, thanks to <a href=\"http://wordle.net/\" target=\"_blank\" rel=\"noopener\">wordle.net</a>, a free-to-use online tool for creating word clouds, we probably all saw enough (bad) word clouds for a lifetime.</p><p>So why did I <a href=\"http://www.nytimes.com/interactive/2015/01/29/sunday-review/road-map-home-values-street-names.html\" target=\"_blank\" rel=\"noopener\">use a word cloud</a>?</p><p>The short answer is: because it was a good fit for the dataset. The data we got was pretty interesting. For each combination of street name and street suffix, <a href=\"http://zillow.com\" target=\"_blank\" rel=\"noopener\">Zillow</a> gave us the median home value according to their real estate database. Recently they used this data to <a href=\"http://www.nytimes.com/2015/01/25/opinion/sunday/the-secrets-of-street-names-and-home-values.html\" target=\"_blank\" rel=\"noopener\">write a story</a> on what a street name reveals about the home value. If you live on a <a href=\"http://www.nytimes.com/2015/01/25/opinion/sunday/the-secrets-of-street-names-and-home-values.html#street=Main&amp;suffix=St&amp;state=NY\" target=\"_blank\" rel=\"noopener\">Main Street</a>, your home is most likely less valuable than homes on Main Roads. Let’s put aside the problem of comparing individual homes with median homes across all streets with the same name (indeed, some readers complained that the data must be wrong, because in <em>their</em> city, Main St. clearly is the better place to live on etc.). It’s still an interesting dataset that you don’t see every day. So we wanted to do something more with it.</p><p>One of the first ideas that popped into my mind was Monopoly. Since we also got a per state and city breakdown of street name values, we could have made a ‘create your own Monopoly’ tool easily. Of course, plenty of localized Monopoly boards have been made already, but how many of them select streets based on actual home price value instead of some editorial judgement?</p><p><img src=\"/blog/images/old/ad3a81c55659f70db37c281b444c114d.png\" alt=\"Image by Fir0002/Flagstaffotos, CC BY-NC\"><span class=\"image-caption\">Image by Fir0002/Flagstaffotos, CC BY-NC</span></p><p>However, after checking back with our legal team at The Times, we had to dismiss this idea, which probably would not have been covered by the fair-use exception for use of copyrighted material.</p><p>So I was looking for something else to do with that data. I ran into some problems with simply showing a top x list of streets. Showing the most common streets was easy – I forgot to mention that we also got the total number of homes per street name – but showing the most valuable street names wasn’t. Unsurprisingly, the most valuable street names are in fact just some single streets on places like <a href=\"https://www.google.com/maps/place/Indian+Creek+Island+Rd,+Florida+33154/@25.8782246,-80.1323674,3a,75y,90t/data=!3m5!1e2!3m3!1s-v_Bk1EBVp9c%2FVDrzbFMaZNI%2FAAAAAAAAi7k%2F_SKyxFFl9Tc!2e4!3e15!4m2!3m1!1s0x88d9b291246d6ee9:0x80241346eaf539a3\" target=\"_blank\" rel=\"noopener\">Miami Beach golf course islands</a>. So that’s not really representative. Of course you can just filter out street names with less than X homes, or that appear in at fewer than Y states. But these cuts are arbitrary. The values I “pick” for X and Y will directly change the make-up of the list of most valuable street names (because the top of the list will always include street names that barely make my filter criteria).</p><p>This kind of brought me to the idea of showing as many street names as possible. If top 20 list of most common street names is boring, maybe the top 1000 list isn’t. That’s where word clouds came into the game first. So mainly for looking at the most common streets I created this word cloud, in which street names are sized by the number of homes, and colored by the median home value (from the least valuable streets in gray to the most valuable – among those 1000 streets – in dark red):</p><p><img src=\"/blog/images/old/079894a0fbbd309bdf2e96aecdb65496.png\" alt=\"\"></p><p>And I immediately liked it. Limiting to two angles made it easier to read, and by shifting both angles a bit I reduced the readability disadvantage that the vertical text would have had otherwise. I felt that this form actually encourages to read through hundreds of street names, something that’s hard to achieve in any other form. Like, reading a thousand street names in an alphabetically ordered list is boring, ordering them by number of homes puts the more valuable streets at the bottom, making the list sortable makes it harder to visually memorize the location of streets, etc. The word cloud solves all these problems. But we weren’t quite there yet.</p><p>After all, by increasing the number of streets shown to a thousand I didn’t really solve my arbitrary filter problem. And by focussing on the most common streets I lose the interesting pattern of ‘rich’ street names.</p><p>That’s when I got back to Monopoly.</p><p>If we can’t use the Monopoly idea, perhaps I can find out what exactly I liked of it. Of course, the idea is without doubt the most fun view on street names, and with the game <a href=\"https://upload.wikimedia.org/wikipedia/commons/3/3c/BoardGamePatentMagie.png\" target=\"_blank\" rel=\"noopener\">being around for more than a century</a>, the view is also immediately familiar to a wide audience.</p><p>But what’s really great about Monopoly is the diversity of street names. The Monopoly board doesn’t just show the richest streets. It shows the most common (or at least recognizable) street names in eight different groups of street value (each with its own color). As if it wanted to include people from all levels of society, lower incomes, the middle class and the richest one percent.</p><p>At this point I had enough to make the word clouds work. By splitting the data up into sub-groups I was able to see the most common street names for different home values. Just like a Monopoly board, but with hundreds of streets in each cluster. The initially break-down into five equally sized groups (by of number of homes) was later I changed to a three group split, mainly because I felt the differences between the ‘middle groups’ wasn’t interesting enough to justify the extra panels. Then I wrote a few sentences to guide our readers through the graphic, and added the filters to help identifying interesting groups of street names in the mess of the cloud. And I also had some fun making the little icons 😃.</p><p>Click <a href=\"http://www.nytimes.com/interactive/2015/01/29/sunday-review/road-map-home-values-street-names.html\" target=\"_blank\" rel=\"noopener\">here to see the full graphic</a>.</p><div class=\"poster poster-945\"><p><img src=\"/blog/images/old/7f43cbd3b39a45e47f28cc5c9266d8f4.png\" alt=\"\"></p></div><p>Summing it up, I think that word clouds can work. But please don’t use them to make randomly colored word frequency charts. Just use <a href=\"http://www.nytimes.com/interactive/2012/09/06/us/politics/convention-word-counts.html?_r=1&amp;\" target=\"_blank\" rel=\"noopener\">word bubbles instead</a> (circle sizes are easier to compare, and you can use position to encode another value). Jim Vallandingham <a href=\"http://vallandingham.me/building_a_bubble_cloud.html\" target=\"_blank\" rel=\"noopener\">wrote a nice tutorial</a> that helps you getting started.</p><p><img src=\"/blog/images/old/c43a16eac827c1a01c4a8c4405880164.png\" alt=\"Source: The New York Times, 2012 (graphic by Mike Bostock, Shan Carter and Matthew Ericson)\"><span class=\"image-caption\">Source: The New York Times, 2012 (graphic by Mike Bostock, Shan Carter and Matthew Ericson)</span></p><p>Do you think I made a huge mistake by using word clouds? Have a different idea for this dataset that might have worked? Let me know!</p><p>Cheers!</p>",
    "contentSnippet": "tl;dr It’s ok to use word clouds if your goal is to encourage reading of a large set of otherwise unrelated words that are connected to one or two interesting values (and word count in a text doesn’t qualify as interesting).\n\n@driven_by_data: In #datavis, no rule’s without exception. On rare occasions you can even use word clouds 😃\n\nThis I tweeted yesterday and now I feel that if I encourage the (dangerous) use of word clouds, I have to explain this exception in a little more detail. Why is it sometimes ok to use a widely rejected visualization method, and most times not?\nA lot had been written about why not to use word clouds. It is super hard to decode or compare the values encoded in the font size or text color (if the latter isn’t random at all). Position, perhaps the most effective visual encoding, is essentially ‘wasted’ for the sake of a space filling algorithm (well, due to the spiraling nature of the layout algorithm, positions aren’t entirely random, but still, they don’t “mean” anything in a word cloud).\nPlus, thanks to wordle.net, a free-to-use online tool for creating word clouds, we probably all saw enough (bad) word clouds for a lifetime.\nSo why did I use a word cloud?\nThe short answer is: because it was a good fit for the dataset. The data we got was pretty interesting. For each combination of street name and street suffix, Zillow gave us the median home value according to their real estate database. Recently they used this data to write a story on what a street name reveals about the home value. If you live on a Main Street, your home is most likely less valuable than homes on Main Roads. Let’s put aside the problem of comparing individual homes with median homes across all streets with the same name (indeed, some readers complained that the data must be wrong, because in their city, Main St. clearly is the better place to live on etc.). It’s still an interesting dataset that you don’t see every day. So we wanted to do something more with it.\nOne of the first ideas that popped into my mind was Monopoly. Since we also got a per state and city breakdown of street name values, we could have made a ‘create your own Monopoly’ tool easily. Of course, plenty of localized Monopoly boards have been made already, but how many of them select streets based on actual home price value instead of some editorial judgement?\nImage by Fir0002/Flagstaffotos, CC BY-NC\nHowever, after checking back with our legal team at The Times, we had to dismiss this idea, which probably would not have been covered by the fair-use exception for use of copyrighted material.\nSo I was looking for something else to do with that data. I ran into some problems with simply showing a top x list of streets. Showing the most common streets was easy – I forgot to mention that we also got the total number of homes per street name – but showing the most valuable street names wasn’t. Unsurprisingly, the most valuable street names are in fact just some single streets on places like Miami Beach golf course islands. So that’s not really representative. Of course you can just filter out street names with less than X homes, or that appear in at fewer than Y states. But these cuts are arbitrary. The values I “pick” for X and Y will directly change the make-up of the list of most valuable street names (because the top of the list will always include street names that barely make my filter criteria).\nThis kind of brought me to the idea of showing as many street names as possible. If top 20 list of most common street names is boring, maybe the top 1000 list isn’t. That’s where word clouds came into the game first. So mainly for looking at the most common streets I created this word cloud, in which street names are sized by the number of homes, and colored by the median home value (from the least valuable streets in gray to the most valuable – among those 1000 streets – in dark red):\n\nAnd I immediately liked it. Limiting to two angles made it easier to read, and by shifting both angles a bit I reduced the readability disadvantage that the vertical text would have had otherwise. I felt that this form actually encourages to read through hundreds of street names, something that’s hard to achieve in any other form. Like, reading a thousand street names in an alphabetically ordered list is boring, ordering them by number of homes puts the more valuable streets at the bottom, making the list sortable makes it harder to visually memorize the location of streets, etc. The word cloud solves all these problems. But we weren’t quite there yet.\nAfter all, by increasing the number of streets shown to a thousand I didn’t really solve my arbitrary filter problem. And by focussing on the most common streets I lose the interesting pattern of ‘rich’ street names.\nThat’s when I got back to Monopoly.\nIf we can’t use the Monopoly idea, perhaps I can find out what exactly I liked of it. Of course, the idea is without doubt the most fun view on street names, and with the game being around for more than a century, the view is also immediately familiar to a wide audience.\nBut what’s really great about Monopoly is the diversity of street names. The Monopoly board doesn’t just show the richest streets. It shows the most common (or at least recognizable) street names in eight different groups of street value (each with its own color). As if it wanted to include people from all levels of society, lower incomes, the middle class and the richest one percent.\nAt this point I had enough to make the word clouds work. By splitting the data up into sub-groups I was able to see the most common street names for different home values. Just like a Monopoly board, but with hundreds of streets in each cluster. The initially break-down into five equally sized groups (by of number of homes) was later I changed to a three group split, mainly because I felt the differences between the ‘middle groups’ wasn’t interesting enough to justify the extra panels. Then I wrote a few sentences to guide our readers through the graphic, and added the filters to help identifying interesting groups of street names in the mess of the cloud. And I also had some fun making the little icons 😃.\nClick here to see the full graphic.\n\n\n\nSumming it up, I think that word clouds can work. But please don’t use them to make randomly colored word frequency charts. Just use word bubbles instead (circle sizes are easier to compare, and you can use position to encode another value). Jim Vallandingham wrote a nice tutorial that helps you getting started.\nSource: The New York Times, 2012 (graphic by Mike Bostock, Shan Carter and Matthew Ericson)\nDo you think I made a huge mistake by using word clouds? Have a different idea for this dataset that might have worked? Let me know!\nCheers!"
  },
  {
    "title": "Seven Features You'll Want In Your Next Charting Tool",
    "link": "https://vis4.net/blog/2015/03/seven-features-youll-wantin-your-next-charting-tool/",
    "pubDate": "2015-03-07T16:03:44.000Z",
    "isoDate": "2015-03-07T16:03:44.000Z",
    "content": "<p><em>This is a transcript of my lightning talk at <a href=\"http://ire.org/conferences/nicar2015/\" target=\"_blank\" rel=\"noopener\">NICAR 2015</a> yesterday. Please give the animated GIFs some time to load 😃</em></p><div class=\"poster poster-945\"><p><img src=\"/blog/images/old/92b2940b60eb99770c72a7d9b89fcf18.png\" alt=\"\"></p></div><p>Charting tools are great. They let us create charts and visualizations without writing code, and in a fraction of the time it would take to do in tools like Adobe Illustrator.  Two years ago I helped building an open source charting tool called <a href=\"https://datawrapper.de/\" target=\"_blank\" rel=\"noopener\">Datawrapper</a> and when I got hired by The New York Times last year, the first assignment I was given was to build yet another charting tool which would be used by reporters and editors of our soon-to-be-launched site <a href=\"http://nytimes.com/upshot\" target=\"_blank\" rel=\"noopener\">TheUpshot</a>.</p><div class=\"poster poster-720\"><p><img src=\"/blog/images/old/6ae2a8a265b1e4544c604e8b74b5369f.png\" alt=\"\"></p></div><p>So, I created “Mr. Chartmaker”<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>. Naturally, I decided to start over from scratch, which is always more fun than messing with code you wrote like two years ago. Also this allowed me to add some cool features I always wanted to have in Datawrapper.</p><h2 id=\"inline-text-editing\"><a class=\"anchor\" href=\"#inline-text-editing\"><span class=\"header-anchor\">#</span></a> Inline text editing</h2><p>Like inline text editing. In most charting tools, if you want to edit the labels shown in a chart, you have to go back one step and edit the data table, and you visualize them again and so forth, which kind of sucks. In Chartmaker you can simply edit all texts exactly where you see them in the chart, which makes the editing much easier. <img src=\"/blog/images/old/2-mcm-inline-text-2.gif\" alt=\"inline text editing in chartmaker\"><span class=\"image-caption\">inline text editing in chartmaker</span></p><h2 id=\"direct-selection-of-data-points\"><a class=\"anchor\" href=\"#direct-selection-of-data-points\"><span class=\"header-anchor\">#</span></a> Direct selection of data points</h2><p>In scatterplots like this often it would be nice to highlight some additional data points, but the problem is you don’t really know which ones you want until you see them all plotted. So in our charting tool you can simply click on points in scatterplots or lines in line charts, and they get labeled. <img src=\"/blog/images/old/3-mcm-direct-selection.gif\" alt=\"direct selection\"><span class=\"image-caption\">direct selection</span></p><h2 id=\"free-text-annotations\"><a class=\"anchor\" href=\"#free-text-annotations\"><span class=\"header-anchor\">#</span></a> Free text annotations</h2><p>Sometimes, to explain a chart to your readers, writing title and an intro sentence isn’t enough. So with our free annotation tool editors can just add text anywhere. This helps guiding readers through the chart and tell them what exactly to pay attention to. <img src=\"/blog/images/old/4-mcm-custom-text.gif\" alt=\"custom text annotations\"><span class=\"image-caption\">custom text annotations</span></p><h2 id=\"adapting-charts-for-different-viewports\"><a class=\"anchor\" href=\"#adapting-charts-for-different-viewports\"><span class=\"header-anchor\">#</span></a> Adapting charts for different viewports</h2><p>Now, with all this custom annotations, it gets harder to automatically resize the chart to fit in different viewports. So we added a feature that allows adapting charts for different sizes. It works like this: while editing, you can switch between different sizes of your chart and your edits are only applied to the selected size. For instance, you can use the short year format on mobile, or abbreviate state names that fit nicely in the full version but aren’t fitting on a phone. <img src=\"/blog/images/old/5-mcm-mobile-2.gif\" alt=\"mobiile adaptions\"><span class=\"image-caption\">mobiile adaptions</span></p><h2 id=\"newsroom-integration\"><a class=\"anchor\" href=\"#newsroom-integration\"><span class=\"header-anchor\">#</span></a> Newsroom integration</h2><p>One of the great advantages of building your own internal charting tool is that you can integrate it into your newsroom infrastructure and workflows. For instance, Chartmaker lets us publish charts directly to our CMS, reducing the time wasted clicking through the same dialogs again and again. <img src=\"/blog/images/old/14c8273fe7adb11eceaf5485599d327a.png\" alt=\"\"> Of course, we also have a Chartmaker slack bot, which posts notifications whenever someone publishes a new chart.</p><h2 id=\"multi-user-awareness-and-chart-versioning\"><a class=\"anchor\" href=\"#multi-user-awareness-and-chart-versioning\"><span class=\"header-anchor\">#</span></a> Multi-user awareness and chart versioning</h2><p>Which reminds me of the fact that a newsroom is a multi-user environment, and your charting tool should respect that. It happens that multiple editors are working on the same charts at the same time so chartmaker shows a gentle warning whenever you’re not the only one working on a chart. Also we keep all versions of charts so we can restore changes that got overwritten by accident. <img src=\"/blog/images/old/fe21bc904595c2b37ae27859ab2a5138.png\" alt=\"\"></p><h2 id=\"expert-custom-javascript-mode\"><a class=\"anchor\" href=\"#expert-custom-javascript-mode\"><span class=\"header-anchor\">#</span></a> Expert custom javascript mode</h2><p>Now, while it makes a lot of sense to re-use generic chart templates, sometimes there are chart forms you would love to do, but your charting tool just doesn’t support them out of the box. Like this “shifting arrows chart” we ran last year: <img src=\"/blog/images/old/81656a1ad71b3d40a8496d4a01bb275b.png\" alt=\"\"> And — even though I said earlier that charting tools are great because they’re made to be used without programming skills, they sometimes are also being used by journalists who <em>can</em> write some code (if they must). So we<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup> came up with the idea of an expert mode where you can just add some final touches to a chart by writing a few lines of code. <img src=\"/blog/images/old/8-mcm-custom-js-4.gif\" alt=\"custom javascript\"><span class=\"image-caption\">custom javascript</span> By the way, this is also a great way to learn code for some of our reporters. I hope you enjoyed me showing off these features, and maybe I inspired some of you for their work on similar tools.</p><h2 id=\"appendix-when-will-chartmaker-be-released-for-everyone\"><a class=\"anchor\" href=\"#appendix-when-will-chartmaker-be-released-for-everyone\"><span class=\"header-anchor\">#</span></a> Appendix: When will Chartmaker be released for everyone?</h2><p>I knew that this question would come, but unfortunately I don’t have an answer at this point. But I will say this: “open sourcing” a charting tool is actually a lot of work, which I know because <a href=\"https://datawrapper.de/\" target=\"_blank\" rel=\"noopener\">I already did so once</a>. Chartmaker is built to work with our CMS, and our Timesian charting styles are pretty-much hard-wired into the tool. But there are already a bunch of open-source charting tools available that are good starting points for hacking away your own tool. Some newsrooms are using <a href=\"http://quartz.github.io/Chartbuilder/\" target=\"_blank\" rel=\"noopener\">Quartz Chartbuilder</a>, which isn’t terribly far away from Chartmaker. I’d also like to recommend <a href=\"http://app.raw.densitydesign.org/\" target=\"_blank\" rel=\"noopener\">RAW</a>, which inspired the design of Chartmaker a great deal. And then there’s also my old love <a href=\"https://datawrapper.de/\" target=\"_blank\" rel=\"noopener\">Datawrapper</a>, without which Chartmaker wouldn’t be possible at all (huge credits go to <a href=\"https://mirkolorenz.com/\" target=\"_blank\" rel=\"noopener\">Mirko Lorenz</a> and <a href=\"https://www.nkb.fr/\" target=\"_blank\" rel=\"noopener\">Nicolas Kayser-Bril</a>). And remember, Datawrapper <em>is</em> open source too, so you can go ahead and install it on your server right now. It has a Wordpress-like plugin architecture that makes it easy to add new features and to integrate it with your newsroom. Finally, I’d like to point to the open source release of <a href=\"https://ai2html.org/\" target=\"_blank\" rel=\"noopener\">ai2html</a>, the Illustrator script that is the very heart of our print-to-web workflow at NYT graphics. This is actually pretty huge!</p><h1 id=\"comments\"><a class=\"anchor\" href=\"#comments\"><span class=\"header-anchor\">#</span></a> Comments</h1><p>luv (Apr 13, 2015)</p><blockquote><p>Hello mates, good article and good arguments commented here, I am truly enjoying by these.</p></blockquote><p>Joshua watson (Apr 25, 2015)</p><blockquote><p>Very much helpful.</p></blockquote><hr class=\"footnotes-sep\"><section class=\"footnotes\"><ol class=\"footnotes-list\"><li id=\"fn1\" class=\"footnote-item\"><p>the name is a reference to Shan Carter's <a href=\"https://shancarter.github.io/mr-data-converter/\" target=\"_blank\" rel=\"noopener\">Mr. Data Converter</a>, and Shan explained the origin of that name <a href=\"https:/.twitter.com/shancarter/status/574069220071227392\" target=\"_blank\" rel=\"noopener\">here</a> <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p></li><li id=\"fn2\" class=\"footnote-item\"><p>And by &quot;we&quot; I mostly mean Amanda Cox <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p></li></ol></section>",
    "contentSnippet": "This is a transcript of my lightning talk at NICAR 2015 yesterday. Please give the animated GIFs some time to load 😃\n\n\n\nCharting tools are great. They let us create charts and visualizations without writing code, and in a fraction of the time it would take to do in tools like Adobe Illustrator.  Two years ago I helped building an open source charting tool called Datawrapper and when I got hired by The New York Times last year, the first assignment I was given was to build yet another charting tool which would be used by reporters and editors of our soon-to-be-launched site TheUpshot.\n\n\n\nSo, I created “Mr. Chartmaker”[1]. Naturally, I decided to start over from scratch, which is always more fun than messing with code you wrote like two years ago. Also this allowed me to add some cool features I always wanted to have in Datawrapper.\n# Inline text editing\nLike inline text editing. In most charting tools, if you want to edit the labels shown in a chart, you have to go back one step and edit the data table, and you visualize them again and so forth, which kind of sucks. In Chartmaker you can simply edit all texts exactly where you see them in the chart, which makes the editing much easier. inline text editing in chartmaker\n# Direct selection of data points\nIn scatterplots like this often it would be nice to highlight some additional data points, but the problem is you don’t really know which ones you want until you see them all plotted. So in our charting tool you can simply click on points in scatterplots or lines in line charts, and they get labeled. direct selection\n# Free text annotations\nSometimes, to explain a chart to your readers, writing title and an intro sentence isn’t enough. So with our free annotation tool editors can just add text anywhere. This helps guiding readers through the chart and tell them what exactly to pay attention to. custom text annotations\n# Adapting charts for different viewports\nNow, with all this custom annotations, it gets harder to automatically resize the chart to fit in different viewports. So we added a feature that allows adapting charts for different sizes. It works like this: while editing, you can switch between different sizes of your chart and your edits are only applied to the selected size. For instance, you can use the short year format on mobile, or abbreviate state names that fit nicely in the full version but aren’t fitting on a phone. mobiile adaptions\n# Newsroom integration\nOne of the great advantages of building your own internal charting tool is that you can integrate it into your newsroom infrastructure and workflows. For instance, Chartmaker lets us publish charts directly to our CMS, reducing the time wasted clicking through the same dialogs again and again.  Of course, we also have a Chartmaker slack bot, which posts notifications whenever someone publishes a new chart.\n# Multi-user awareness and chart versioning\nWhich reminds me of the fact that a newsroom is a multi-user environment, and your charting tool should respect that. It happens that multiple editors are working on the same charts at the same time so chartmaker shows a gentle warning whenever you’re not the only one working on a chart. Also we keep all versions of charts so we can restore changes that got overwritten by accident. \n# Expert custom javascript mode\nNow, while it makes a lot of sense to re-use generic chart templates, sometimes there are chart forms you would love to do, but your charting tool just doesn’t support them out of the box. Like this “shifting arrows chart” we ran last year:  And — even though I said earlier that charting tools are great because they’re made to be used without programming skills, they sometimes are also being used by journalists who can write some code (if they must). So we[2] came up with the idea of an expert mode where you can just add some final touches to a chart by writing a few lines of code. custom javascript By the way, this is also a great way to learn code for some of our reporters. I hope you enjoyed me showing off these features, and maybe I inspired some of you for their work on similar tools.\n# Appendix: When will Chartmaker be released for everyone?\nI knew that this question would come, but unfortunately I don’t have an answer at this point. But I will say this: “open sourcing” a charting tool is actually a lot of work, which I know because I already did so once. Chartmaker is built to work with our CMS, and our Timesian charting styles are pretty-much hard-wired into the tool. But there are already a bunch of open-source charting tools available that are good starting points for hacking away your own tool. Some newsrooms are using Quartz Chartbuilder, which isn’t terribly far away from Chartmaker. I’d also like to recommend RAW, which inspired the design of Chartmaker a great deal. And then there’s also my old love Datawrapper, without which Chartmaker wouldn’t be possible at all (huge credits go to Mirko Lorenz and Nicolas Kayser-Bril). And remember, Datawrapper is open source too, so you can go ahead and install it on your server right now. It has a Wordpress-like plugin architecture that makes it easy to add new features and to integrate it with your newsroom. Finally, I’d like to point to the open source release of ai2html, the Illustrator script that is the very heart of our print-to-web workflow at NYT graphics. This is actually pretty huge!\n# Comments\nluv (Apr 13, 2015)\n\nHello mates, good article and good arguments commented here, I am truly enjoying by these.\n\nJoshua watson (Apr 25, 2015)\n\nVery much helpful.\n\n\n\nthe name is a reference to Shan Carter's Mr. Data Converter, and Shan explained the origin of that name here ↩︎\n\nAnd by \"we\" I mostly mean Amanda Cox ↩︎"
  },
  {
    "title": "Making HTML tables in D3 doesn't need to be a pain",
    "link": "https://vis4.net/blog/2015/04/making-html-tables-in-d3-doesnt-need-to-be-a-pain/",
    "pubDate": "2015-04-24T16:48:27.000Z",
    "isoDate": "2015-04-24T16:48:27.000Z",
    "content": "<p><strong>tl;dr:</strong> Here’s a <a href=\"https://jsfiddle.net/vis4/Lby2eq1j/\" target=\"_blank\" rel=\"noopener\">demo with source code</a>. D3 is nice, but it also makes some simple things look really complicated. One of them is making a simple HTML table. Let’s say you got a simple dataset, stored as array of objects just as you would get from d3.csv:</p><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> movies = [</span><br><span class=\"line\">    &#123; <span class=\"attr\">title</span>: <span class=\"string\">\"The Godfather\"</span>, <span class=\"attr\">year</span>: <span class=\"number\">1972</span>, <span class=\"attr\">length</span>: <span class=\"number\">175</span>,</span><br><span class=\"line\">      budget: <span class=\"number\">6000000</span>, <span class=\"attr\">rating</span>: <span class=\"number\">9.1</span> &#125;,</span><br><span class=\"line\">    &#123; <span class=\"attr\">title</span>: <span class=\"string\">\"The Shawshank Redemption\"</span>, <span class=\"attr\">year</span>: <span class=\"number\">1994</span>,</span><br><span class=\"line\">      length: <span class=\"number\">142</span>, <span class=\"attr\">budget</span>: <span class=\"number\">25000000</span>, <span class=\"attr\">rating</span>: <span class=\"number\">9.1</span> &#125;,</span><br><span class=\"line\">    &#123; <span class=\"attr\">title</span>: <span class=\"string\">\"The Lord of the Rings 3\"</span>, <span class=\"attr\">year</span>: <span class=\"number\">2003</span>,</span><br><span class=\"line\">      length: <span class=\"number\">251</span>, <span class=\"attr\">budget</span>: <span class=\"number\">94000000</span>, <span class=\"attr\">rating</span>: <span class=\"number\">9</span> &#125;,</span><br><span class=\"line\">    <span class=\"comment\">/* ... */</span></span><br><span class=\"line\">];</span><br></pre></td></tr></table></figure><p>To render this in a table you would typically start writing some code like this:</p><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> table = d3.select(<span class=\"string\">'body'</span>).append(<span class=\"string\">'table'</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> tr = table.selectAll(<span class=\"string\">'tr'</span>)</span><br><span class=\"line\">    .data(movies).enter()</span><br><span class=\"line\">    .append(<span class=\"string\">'tr'</span>);</span><br></pre></td></tr></table></figure><p>Now you got a selection of table row elements, each of which is bound to one movie. But how do you make the table columns? What I did a lot was this:</p><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tr.append(<span class=\"string\">'td'</span>).html(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">m</span>) </span>&#123; <span class=\"keyword\">return</span> m.title; &#125;);</span><br><span class=\"line\">tr.append(<span class=\"string\">'td'</span>).html(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">m</span>) </span>&#123; <span class=\"keyword\">return</span> m.year; &#125;);</span><br><span class=\"line\">tr.append(<span class=\"string\">'td'</span>).html(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">m</span>) </span>&#123; <span class=\"keyword\">return</span> m.budget; &#125;);</span><br></pre></td></tr></table></figure><p>That looks easy at first, but of course you want more stuff, like a class name depending on the column etc. So the above code turns into this:</p><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tr.append(<span class=\"string\">'td'</span>)</span><br><span class=\"line\">    .attr(<span class=\"string\">'class'</span>, <span class=\"string\">'title'</span>)</span><br><span class=\"line\">    .html(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">m</span>) </span>&#123; <span class=\"keyword\">return</span> m.title; &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">tr.append(<span class=\"string\">'td'</span>)</span><br><span class=\"line\">    .attr(<span class=\"string\">'class'</span>, <span class=\"string\">'center'</span>)</span><br><span class=\"line\">    .html(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">m</span>) </span>&#123; <span class=\"keyword\">return</span> m.year; &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">tr.append(<span class=\"string\">'td'</span>)</span><br><span class=\"line\">    .attr(<span class=\"string\">'class'</span>, <span class=\"string\">'num'</span>)</span><br><span class=\"line\">    .html(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">m</span>) </span>&#123; <span class=\"keyword\">return</span> m.budget; &#125;);</span><br></pre></td></tr></table></figure><p>Also you might need a table header, so essentially you copy this entire block to create the <strong>th</strong> elements. Better make sure you keep them in the same order if you decide to change your code later. To make it short, this is an entire mess. It’s not the right way to do a table.</p><h2 id=\"html-tables-in-d3-the-right-way\"><a class=\"anchor\" href=\"#html-tables-in-d3-the-right-way\"><span class=\"header-anchor\">#</span></a> HTML tables in D3, the right way</h2><p>To make tables fun again, we simply define a set of columns as an array of objects. Note that some of the attributes of the column objects are functions, these will later be evaluated against the row objects to get the values for each cell.</p><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> columns = [</span><br><span class=\"line\">    &#123; <span class=\"attr\">head</span>: <span class=\"string\">'Movie title'</span>, <span class=\"attr\">cl</span>: <span class=\"string\">'title'</span>,</span><br><span class=\"line\">      html: <span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">row</span>) </span>&#123; <span class=\"keyword\">return</span> r.title; &#125; &#125;,</span><br><span class=\"line\">    &#123; <span class=\"attr\">head</span>: <span class=\"string\">'Year'</span>, <span class=\"attr\">cl</span>: <span class=\"string\">'center'</span>,</span><br><span class=\"line\">      html: <span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">row</span>) </span>&#123; <span class=\"keyword\">return</span> r.year; &#125; &#125;,</span><br><span class=\"line\">    &#123; <span class=\"attr\">head</span>: <span class=\"string\">'Length'</span>, <span class=\"attr\">cl</span>: <span class=\"string\">'center'</span>,</span><br><span class=\"line\">      html: <span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">row</span>) </span>&#123; <span class=\"keyword\">return</span> r.length; &#125; &#125;,</span><br><span class=\"line\">    &#123; <span class=\"attr\">head</span>: <span class=\"string\">'Budget'</span>, <span class=\"attr\">cl</span>: <span class=\"string\">'num'</span>,</span><br><span class=\"line\">      html: <span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">row</span>) </span>&#123; <span class=\"keyword\">return</span> r.budget; &#125; &#125;,</span><br><span class=\"line\">    &#123; <span class=\"attr\">head</span>: <span class=\"string\">'Rating'</span>, <span class=\"attr\">cl</span>: <span class=\"string\">'num'</span>,</span><br><span class=\"line\">      html: <span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">row</span>) </span>&#123; <span class=\"keyword\">return</span> r.rating; &#125; &#125;</span><br><span class=\"line\">];</span><br></pre></td></tr></table></figure><p>Actually, since I really don’t like all these verbose getter functions here, let’s instead use the nice <strong>ƒ</strong> helper function from the <a href=\"https://github.com/gka/d3-jetpack\" target=\"_blank\" rel=\"noopener\">d3-jetpack</a> and compress the code a bit:</p><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> columns = [</span><br><span class=\"line\">    &#123; <span class=\"attr\">head</span>: <span class=\"string\">'Movie title'</span>, <span class=\"attr\">cl</span>: <span class=\"string\">'title'</span>, <span class=\"attr\">html</span>: ƒ(<span class=\"string\">'title'</span>) &#125;,</span><br><span class=\"line\">    &#123; <span class=\"attr\">head</span>: <span class=\"string\">'Year'</span>, <span class=\"attr\">cl</span>: <span class=\"string\">'center'</span>, <span class=\"attr\">html</span>: ƒ(<span class=\"string\">'year'</span>) &#125;,</span><br><span class=\"line\">    &#123; <span class=\"attr\">head</span>: <span class=\"string\">'Length'</span>, <span class=\"attr\">cl</span>: <span class=\"string\">'center'</span>, <span class=\"attr\">html</span>: ƒ(<span class=\"string\">'length'</span>) &#125;,</span><br><span class=\"line\">    &#123; <span class=\"attr\">head</span>: <span class=\"string\">'Budget'</span>, <span class=\"attr\">cl</span>: <span class=\"string\">'num'</span>, <span class=\"attr\">html</span>: ƒ(<span class=\"string\">'budget'</span>) &#125;,</span><br><span class=\"line\">    &#123; <span class=\"attr\">head</span>: <span class=\"string\">'Rating'</span>, <span class=\"attr\">cl</span>: <span class=\"string\">'num'</span>, <span class=\"attr\">html</span>: ƒ(<span class=\"string\">'rating'</span>) &#125;</span><br><span class=\"line\">];</span><br></pre></td></tr></table></figure><p>We can now use these column objects in a data join to create the table header. Much more fun than duplicating all the code for each column.</p><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">table.append(<span class=\"string\">'thead'</span>).append(<span class=\"string\">'tr'</span>)</span><br><span class=\"line\">   .selectAll(<span class=\"string\">'th'</span>)</span><br><span class=\"line\">   .data(columns).enter()</span><br><span class=\"line\">   .append(<span class=\"string\">'th'</span>)</span><br><span class=\"line\">   .attr(<span class=\"string\">'class'</span>, ƒ(<span class=\"string\">'cl'</span>))</span><br><span class=\"line\">   .text(ƒ(<span class=\"string\">'head'</span>));</span><br></pre></td></tr></table></figure><p>Finally, we can do the same with the table body. But if we would just pass the column objects here, we would lose the information of the row. So that’s why we evaluate all the function properties of the column objects against the row objects. This way we convert the list of column objects into a list of cell objects to use in the second data join:</p><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">table.append(<span class=\"string\">'tbody'</span>)</span><br><span class=\"line\">   .selectAll(<span class=\"string\">'tr'</span>)</span><br><span class=\"line\">   .data(movies).enter()</span><br><span class=\"line\">   .append(<span class=\"string\">'tr'</span>)</span><br><span class=\"line\">   .selectAll(<span class=\"string\">'td'</span>)</span><br><span class=\"line\">   .data(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">row, i</span>) </span>&#123;</span><br><span class=\"line\">       <span class=\"comment\">// evaluate column objects against the current row</span></span><br><span class=\"line\">       <span class=\"keyword\">return</span> columns.map(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">c</span>) </span>&#123;</span><br><span class=\"line\">           <span class=\"keyword\">var</span> cell = &#123;&#125;;</span><br><span class=\"line\">           d3.keys(c).forEach(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">k</span>) </span>&#123;</span><br><span class=\"line\">               cell[k] = <span class=\"keyword\">typeof</span> c[k] == <span class=\"string\">'function'</span> ? c[k](row,i) : c[k];</span><br><span class=\"line\">           &#125;);</span><br><span class=\"line\">           <span class=\"keyword\">return</span> cell;</span><br><span class=\"line\">       &#125;);</span><br><span class=\"line\">   &#125;).enter()</span><br><span class=\"line\">   .append(<span class=\"string\">'td'</span>)</span><br><span class=\"line\">   .html(ƒ(<span class=\"string\">'html'</span>))</span><br><span class=\"line\">   .attr(<span class=\"string\">'class'</span>, ƒ(<span class=\"string\">'cl'</span>));</span><br></pre></td></tr></table></figure><p>And that’s it. Again, here’s a link to the <a href=\"http://bl.ocks.org/gka/17ee676dc59aa752b4e6\" target=\"_blank\" rel=\"noopener\">demo with full source code</a>.</p><h1 id=\"comments\"><a class=\"anchor\" href=\"#comments\"><span class=\"header-anchor\">#</span></a> Comments</h1><p>Dheepan (Jul 21, 2015)</p><blockquote><p>Hi Gregor,</p><p>How do I access the values inside the rows to use in functions? In my case I want to use the values in my class td.num to determine the background fill based on variable sentcolor.</p><p>My return function for .style selected on (“td.num”)  can only access the index number but not the actual value.</p><p>See example here, <a href=\"http://jsfiddle.net/dheepanr/p6uovdL7/2/\" rel=\"noopener\" target=\"_blank\">link</a></p></blockquote><p>Nagarajan Chinnasamy (Apr 29, 2015)</p><blockquote><p>In case if you need an SVG based Grid (Table with adjustable columns, sorting etc.), you can look at:  <a href=\"https://github.com/PMSI-AlignAlytics/scrollgrid\" target=\"_blank\" rel=\"noopener\">https://github.com/PMSI-AlignAlytics/scrollgrid</a></p><p>This is based on D3.</p></blockquote><p>keith (May 10, 2015)</p><blockquote><p>including datatables.js is very simple to add to your example and makes for nice user enabled sorting/filtering.</p><p>i was working on a project where the data drawn by d3 was to be controlled by datatables but never did implement that part of it. i think that would be pretty slick.</p></blockquote><p>Will Morris (Apr 24, 2015)</p><blockquote><p>This is a nice solution. However, for an HTML table, why not use something like handlebars.js? Just curious about your experience with one vs the other.</p></blockquote><p>Gregor Aisch (Apr 24, 2015)</p><blockquote><p>In the context where I am using this code, D3 is part of the default project setup, but handlebars isn’t. We do a lot of graphics so it kind of makes sense to stay in one framework…</p></blockquote><p>Pat (Jul 20, 2015)</p><blockquote><p>The array-of-columns is a very cool approach.</p><p>It seems like the column definitions would be a good place to put a sorting function, which would enable a simple version of the kinds of sorting the other commenters are suggesting.</p></blockquote>",
    "contentSnippet": "tl;dr: Here’s a demo with source code. D3 is nice, but it also makes some simple things look really complicated. One of them is making a simple HTML table. Let’s say you got a simple dataset, stored as array of objects just as you would get from d3.csv:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\nvar movies = [\n    { title: \"The Godfather\", year: 1972, length: 175,\n      budget: 6000000, rating: 9.1 },\n    { title: \"The Shawshank Redemption\", year: 1994,\n      length: 142, budget: 25000000, rating: 9.1 },\n    { title: \"The Lord of the Rings 3\", year: 2003,\n      length: 251, budget: 94000000, rating: 9 },\n    /* ... */\n];\n\n\nTo render this in a table you would typically start writing some code like this:\n\n\n1\n2\n3\n4\n5\n\nvar table = d3.select('body').append('table');\n\nvar tr = table.selectAll('tr')\n    .data(movies).enter()\n    .append('tr');\n\n\nNow you got a selection of table row elements, each of which is bound to one movie. But how do you make the table columns? What I did a lot was this:\n\n\n1\n2\n3\n\ntr.append('td').html(function(m) { return m.title; });\ntr.append('td').html(function(m) { return m.year; });\ntr.append('td').html(function(m) { return m.budget; });\n\n\nThat looks easy at first, but of course you want more stuff, like a class name depending on the column etc. So the above code turns into this:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\ntr.append('td')\n    .attr('class', 'title')\n    .html(function(m) { return m.title; });\n\ntr.append('td')\n    .attr('class', 'center')\n    .html(function(m) { return m.year; });\n\ntr.append('td')\n    .attr('class', 'num')\n    .html(function(m) { return m.budget; });\n\n\nAlso you might need a table header, so essentially you copy this entire block to create the th elements. Better make sure you keep them in the same order if you decide to change your code later. To make it short, this is an entire mess. It’s not the right way to do a table.\n# HTML tables in D3, the right way\nTo make tables fun again, we simply define a set of columns as an array of objects. Note that some of the attributes of the column objects are functions, these will later be evaluated against the row objects to get the values for each cell.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\nvar columns = [\n    { head: 'Movie title', cl: 'title',\n      html: function(row) { return r.title; } },\n    { head: 'Year', cl: 'center',\n      html: function(row) { return r.year; } },\n    { head: 'Length', cl: 'center',\n      html: function(row) { return r.length; } },\n    { head: 'Budget', cl: 'num',\n      html: function(row) { return r.budget; } },\n    { head: 'Rating', cl: 'num',\n      html: function(row) { return r.rating; } }\n];\n\n\nActually, since I really don’t like all these verbose getter functions here, let’s instead use the nice ƒ helper function from the d3-jetpack and compress the code a bit:\n\n\n1\n2\n3\n4\n5\n6\n7\n\nvar columns = [\n    { head: 'Movie title', cl: 'title', html: ƒ('title') },\n    { head: 'Year', cl: 'center', html: ƒ('year') },\n    { head: 'Length', cl: 'center', html: ƒ('length') },\n    { head: 'Budget', cl: 'num', html: ƒ('budget') },\n    { head: 'Rating', cl: 'num', html: ƒ('rating') }\n];\n\n\nWe can now use these column objects in a data join to create the table header. Much more fun than duplicating all the code for each column.\n\n\n1\n2\n3\n4\n5\n6\n\ntable.append('thead').append('tr')\n   .selectAll('th')\n   .data(columns).enter()\n   .append('th')\n   .attr('class', ƒ('cl'))\n   .text(ƒ('head'));\n\n\nFinally, we can do the same with the table body. But if we would just pass the column objects here, we would lose the information of the row. So that’s why we evaluate all the function properties of the column objects against the row objects. This way we convert the list of column objects into a list of cell objects to use in the second data join:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\ntable.append('tbody')\n   .selectAll('tr')\n   .data(movies).enter()\n   .append('tr')\n   .selectAll('td')\n   .data(function(row, i) {\n       // evaluate column objects against the current row\n       return columns.map(function(c) {\n           var cell = {};\n           d3.keys(c).forEach(function(k) {\n               cell[k] = typeof c[k] == 'function' ? c[k](row,i) : c[k];\n           });\n           return cell;\n       });\n   }).enter()\n   .append('td')\n   .html(ƒ('html'))\n   .attr('class', ƒ('cl'));\n\n\nAnd that’s it. Again, here’s a link to the demo with full source code.\n# Comments\nDheepan (Jul 21, 2015)\n\nHi Gregor,\nHow do I access the values inside the rows to use in functions? In my case I want to use the values in my class td.num to determine the background fill based on variable sentcolor.\nMy return function for .style selected on (“td.num”)  can only access the index number but not the actual value.\nSee example here, link\n\nNagarajan Chinnasamy (Apr 29, 2015)\n\nIn case if you need an SVG based Grid (Table with adjustable columns, sorting etc.), you can look at:  https://github.com/PMSI-AlignAlytics/scrollgrid\nThis is based on D3.\n\nkeith (May 10, 2015)\n\nincluding datatables.js is very simple to add to your example and makes for nice user enabled sorting/filtering.\ni was working on a project where the data drawn by d3 was to be controlled by datatables but never did implement that part of it. i think that would be pretty slick.\n\nWill Morris (Apr 24, 2015)\n\nThis is a nice solution. However, for an HTML table, why not use something like handlebars.js? Just curious about your experience with one vs the other.\n\nGregor Aisch (Apr 24, 2015)\n\nIn the context where I am using this code, D3 is part of the default project setup, but handlebars isn’t. We do a lot of graphics so it kind of makes sense to stay in one framework…\n\nPat (Jul 20, 2015)\n\nThe array-of-columns is a very cool approach.\nIt seems like the column definitions would be a good place to put a sorting function, which would enable a simple version of the kinds of sorting the other commenters are suggesting."
  },
  {
    "title": "Why we didn't use a cartogram in the Brexit referrendum map",
    "link": "https://vis4.net/blog/2016/06/to-cartogram-or-not-to-cartogram-the-brexit/",
    "pubDate": "2016-06-24T16:33:49.000Z",
    "isoDate": "2016-06-24T16:33:49.000Z",
    "content": "<p>Great Britain has voted to leave the E.U., and election result cartograms are all over the internet. However, for our map we decided to stick with a simple map instead.</p><div class=\"poster poster-720\"><p><img src=\"/blog/images/old/brexit-map-1-1371x1200.png\" alt=\"Source: New York Times, How Britain Voted in the E.U. Referendum\"><span class=\"image-caption\">Source: New York Times, How Britain Voted in the E.U. Referendum</span></p></div><p>Here are a couple of the cartograms I saw:</p><div class=\"poster poster-720\" style=\"background: white\"><div style=\"width:45%;\"><p><img src=\"/blog/images/old/28669015bc0bcb5f80ce982bc295c720.png\" alt=\"Source: The Guardian\"><span class=\"image-caption\">Source: The Guardian</span></p></div><div style=\"width:55%;\"><p><img src=\"/blog/images/old/cd143fbd8c796f024536aa8dd8df25e5.png\" alt=\"Source: Buzzfeed\"><span class=\"image-caption\">Source: Buzzfeed</span></p></div></div><p>While these cartogram are undeniably pieces of beauty, there are still some significant problems.</p><h2 id=\"cartograms-are-too-confusing-for-readers-unfamiliar-with-the-geography\"><a class=\"anchor\" href=\"#cartograms-are-too-confusing-for-readers-unfamiliar-with-the-geography\"><span class=\"header-anchor\">#</span></a> <strong>Cartograms are too confusing for readers unfamiliar with the geography</strong></h2><p>Distorted geography works for people who know the undistorted geography in and out. But it is very hard to make sense of cartograms of countries you are not familiar with. First of all, we might not recognize the country’s outer shape, which might make some wonder what that thing is that we’re looking at. The same is true with regions within the country. It’s hard to point at Wales in both of the cartograms.</p><h2 id=\"reading-and-comparing-areas-is-hard\"><a class=\"anchor\" href=\"#reading-and-comparing-areas-is-hard\"><span class=\"header-anchor\">#</span></a> <strong>Reading and comparing areas is hard</strong></h2><p>Interpreting circles sizes is hard enough, but for irregular shapes like in the Guardian’s hexgrid cartogram this becomes even harder. I just doubt that by looking at the cartogram, anyone would correctly “guess” that the total vote was 52% for leave. That’s why, every map was accompanied by at least one bar chart showing the result.</p><h2 id=\"geographical-area-isnt-just-noise\"><a class=\"anchor\" href=\"#geographical-area-isnt-just-noise\"><span class=\"header-anchor\">#</span></a> <strong>Geographical area isn’t just &quot;noise&quot;</strong></h2><p>Geographical area is not just noise in a map. Especially with election maps, where regions are often roughly designed to contain similar population, the area tells us something important about the regions: population density. Which is an indicator whether some shape you’re looking at might be a city or a rural area. In cartograms, this information gets lost. <img src=\"/blog/images/old/7524f8875488886e417b5d21fff46fd9.png\" alt=\"\"></p><h2 id=\"cartograms-are-harder-to-label\"><a class=\"anchor\" href=\"#cartograms-are-harder-to-label\"><span class=\"header-anchor\">#</span></a> <strong>Cartograms are harder to label</strong></h2><p>As with all data visualization, labeling makes the difference! But with cartograms the boundaries between regions can disappear, which makes labeling a lot harder. The hexgrid cartogram does a better job of maintaining region outlines, but the distortion doesn’t help.</p><p><img src=\"/blog/images/old/5f403bbb0c6538f7b9c5782478617e93.png\" alt=\"Where does Wales start and end?\"><span class=\"image-caption\">Where does Wales start and end?</span></p><h2 id=\"there-are-other-ways-to-address-the-same-problem\"><a class=\"anchor\" href=\"#there-are-other-ways-to-address-the-same-problem\"><span class=\"header-anchor\">#</span></a> <strong>There are other ways to address the same problem</strong></h2><p>Sometimes there are other solutions to address an uneven population distribution. In this case we added a simple table that breaks down the result by region, along with population figures for each. This is not as efficient to read, but it still tells you that London’s population is bigger than Scotland and Wales combined. Another idea was to use a simple map call-out to magnify the London area, both to give it more visual weight and to make it easier to see details inside the city.</p><div class=\"poster poster-720 grid-2\" style=\"background: white\"><div><p><img src=\"/blog/images/old/e749ea8d58d13d3f6a244bccf4760e0e.png\" alt=\"Simple results table\"><span class=\"image-caption\">Simple results table</span></p></div><div><p><img src=\"/blog/images/old/7ef8f1cd0456c5f16343fef86aa9c682.png\" alt=\"London call-out\"><span class=\"image-caption\">London call-out</span></p></div></div><h2 id=\"but-wait-cartograms-are-still-great\"><a class=\"anchor\" href=\"#but-wait-cartograms-are-still-great\"><span class=\"header-anchor\">#</span></a> <strong>But wait, cartograms are still great!</strong></h2><p>This post is not meant to be a takedown of cartograms. They are useful and extremely powerful tools in data visualization, and for a lot more good examples I recommend Benjamin Hennings blog <a href=\"http://www.viewsoftheworld.net/\" target=\"_blank\" rel=\"noopener\">viewsoftheworld.net</a>. Here, I just wanted to explain some of the problems cartograms face in news, which eventually led us to drop the idea.</p><h1 id=\"comments\"><a class=\"anchor\" href=\"#comments\"><span class=\"header-anchor\">#</span></a> Comments</h1><p>wormcast (Jun 28, 2016)</p><blockquote><p>Lame. You should have used both. Displaying only non-cartogram maps misleads the audience by concealing the population difference</p></blockquote><p>eesur (Jun 28, 2016)</p><blockquote><p>out of all the Brexit maps, I found this to be clearest and the one I referenced— thanks for sharing the reasoning—much appreciated!</p></blockquote><p>David (Jun 29, 2016)</p><blockquote><p>I wish you would write more of these cogent and succinct posts explaining the design choices made by the NYT’s graphics department. We could all benefit from your profound knowledge.</p></blockquote>",
    "contentSnippet": "Great Britain has voted to leave the E.U., and election result cartograms are all over the internet. However, for our map we decided to stick with a simple map instead.\n\nSource: New York Times, How Britain Voted in the E.U. Referendum\n\nHere are a couple of the cartograms I saw:\n\nSource: The Guardian\n\nSource: Buzzfeed\n\nWhile these cartogram are undeniably pieces of beauty, there are still some significant problems.\n# Cartograms are too confusing for readers unfamiliar with the geography\nDistorted geography works for people who know the undistorted geography in and out. But it is very hard to make sense of cartograms of countries you are not familiar with. First of all, we might not recognize the country’s outer shape, which might make some wonder what that thing is that we’re looking at. The same is true with regions within the country. It’s hard to point at Wales in both of the cartograms.\n# Reading and comparing areas is hard\nInterpreting circles sizes is hard enough, but for irregular shapes like in the Guardian’s hexgrid cartogram this becomes even harder. I just doubt that by looking at the cartogram, anyone would correctly “guess” that the total vote was 52% for leave. That’s why, every map was accompanied by at least one bar chart showing the result.\n# Geographical area isn’t just \"noise\"\nGeographical area is not just noise in a map. Especially with election maps, where regions are often roughly designed to contain similar population, the area tells us something important about the regions: population density. Which is an indicator whether some shape you’re looking at might be a city or a rural area. In cartograms, this information gets lost. \n# Cartograms are harder to label\nAs with all data visualization, labeling makes the difference! But with cartograms the boundaries between regions can disappear, which makes labeling a lot harder. The hexgrid cartogram does a better job of maintaining region outlines, but the distortion doesn’t help.\nWhere does Wales start and end?\n# There are other ways to address the same problem\nSometimes there are other solutions to address an uneven population distribution. In this case we added a simple table that breaks down the result by region, along with population figures for each. This is not as efficient to read, but it still tells you that London’s population is bigger than Scotland and Wales combined. Another idea was to use a simple map call-out to magnify the London area, both to give it more visual weight and to make it easier to see details inside the city.\n\nSimple results table\n\nLondon call-out\n\n# But wait, cartograms are still great!\nThis post is not meant to be a takedown of cartograms. They are useful and extremely powerful tools in data visualization, and for a lot more good examples I recommend Benjamin Hennings blog viewsoftheworld.net. Here, I just wanted to explain some of the problems cartograms face in news, which eventually led us to drop the idea.\n# Comments\nwormcast (Jun 28, 2016)\n\nLame. You should have used both. Displaying only non-cartogram maps misleads the audience by concealing the population difference\n\neesur (Jun 28, 2016)\n\nout of all the Brexit maps, I found this to be clearest and the one I referenced— thanks for sharing the reasoning—much appreciated!\n\nDavid (Jun 29, 2016)\n\nI wish you would write more of these cogent and succinct posts explaining the design choices made by the NYT’s graphics department. We could all benefit from your profound knowledge."
  },
  {
    "title": "Why we used jittery gauges in our live election forecast",
    "link": "https://vis4.net/blog/2016/11/jittery-gauges-election-forecast/",
    "pubDate": "2016-11-14T19:45:03.000Z",
    "isoDate": "2016-11-14T19:45:03.000Z",
    "content": "<p><strong><img src=\"/blog/images/old/jitter4.gif\" alt=\"jitter4\"><span class=\"image-caption\">jitter4</span></strong> There has been <a href=\"https://twitter.com/ftrain/status/797210858942251009\" target=\"_blank\" rel=\"noopener\">some</a> <a href=\"https://twitter.com/tsiro/status/796185282718511104\" target=\"_blank\" rel=\"noopener\">debate</a> about the jittery gauge chart we used in our <a href=\"http://www.nytimes.com/elections/forecast/president\" target=\"_blank\" rel=\"noopener\">live election forecast</a>. Rather than replying to dozens of tweets I decided to wrap this up in an old-fashioned blog post™. Feel free to add your thoughts to the comments.</p><blockquote><p><a href=\"https://twitter.com/tsiro/status/796185282718511104\" target=\"_blank\" rel=\"noopener\">@tsiro</a>: straight up: the NYT needle jitter is irresponsible design at best and unethical design at worst and you should stop looking at it<br>(Nov 9, 2016)</p></blockquote><h2 id=\"why-did-we-let-the-gauge-needles-jitter\"><a class=\"anchor\" href=\"#why-did-we-let-the-gauge-needles-jitter\"><span class=\"header-anchor\">#</span></a> Why did we let the gauge needles jitter?</h2><p>We added movement in our gauges for two reasons. First, we wanted to convey the reality that our forecast was “live” – connected to a live data feed and updating continuously, without any need for a user to refresh his or her browser. Movement helps convey that reality. This technique is common across the web, whether it’s on a site like chartbeat or a live gamecast of a baseball game. A “live” web page where nothing moves feels quite dead to the user. Second, we thought (and still think!) this movement actually helped demonstrate the uncertainty around our forecast, conveying the relative precision of our estimates. In our opinion, having the dial fluctuate a small amount – bound by the 25th and 75 percentile of simulated outcomes – was more successful at conveying the uncertainty around our forecast than simply listing what those percentiles were. As the night went on, the gauges fluctuated less and less as our forecast became more precise. By the end of the night, the gauges barely moved.</p><h2 id=\"why-didnt-you-just-show-the-chance-of-winning\"><a class=\"anchor\" href=\"#why-didnt-you-just-show-the-chance-of-winning\"><span class=\"header-anchor\">#</span></a> Why didn’t you just show the chance of winning?</h2><p>The chance of winning is an important indicator that includes the uncertainty of the forecast. So why was that not enough? Well, the problem is that most people aren’t very good at interpreting a probability scale. While it is relatively easy to understand that a 50:50 chance is pretty high, and that a one in a million chance is pretty low, it is very hard to make sense of probabilities in between. How good is a 75% chance of winning? How much better is 85%? So rather than displaying just the chance of winning we wanted to show likely outcomes both in vote margin and electoral votes.</p><h2 id=\"why-didnt-you-just-display-the-median-outcome\"><a class=\"anchor\" href=\"#why-didnt-you-just-display-the-median-outcome\"><span class=\"header-anchor\">#</span></a> Why didn’t you just display the median outcome?</h2><p>Because we thought that most people would read the median outcome as <em>the one</em> most likely outcome, as opposed to <em>one of many</em> very likely outcomes. Seeing the needle actually walk to the lower and higher end of the projected range makes us aware that these are real possibilities. <strong>Was it really just random jitter?</strong> The jittering was random, but the jitter range was not. The range was fixed between the 25th and 75th percentile of the simulated outcomes. Early in the night, the uncertainty was higher and the ranges wider. Towards the end of the night they got more narrow. The range was displayed as pie chart slice on the gauge, along with the 5-95th range.</p><h2 id=\"why-did-the-needle-movements-look-almost-real\"><a class=\"anchor\" href=\"#why-did-the-needle-movements-look-almost-real\"><span class=\"header-anchor\">#</span></a> Why did the needle movements look almost real?</h2><p>To make the movements of the needle look more real we used <a href=\"https://en.wikipedia.org/wiki/Perlin_noise\" target=\"_blank\" rel=\"noopener\">Perlin noise</a>, an algorithm that originates in computer graphics to simulate natural textures like clouds. We used the implementation from <a href=\"http://p5js.org/reference/#/p5/noise\" target=\"_blank\" rel=\"noopener\">processing.js</a>.</p><h2 id=\"why-was-the-jittering-stressing-out-a-lot-of-people\"><a class=\"anchor\" href=\"#why-was-the-jittering-stressing-out-a-lot-of-people\"><span class=\"header-anchor\">#</span></a> Why was the jittering <a href=\"http://www.theverge.com/2016/11/8/13571216/new-york-times-election-forecast-jitter-needle\" target=\"_blank\" rel=\"noopener\">stressing out</a> a lot of people?</h2><p>Part of the answer is that we simply don’t feel comfortable being left in uncertainty about something as important as this election. We like to feel safe and cozy, being told our favorite candidate has a “narrow but persistent” lead, but we don’t like being confronted with likely scenarios in which the candidate might actually lose. Of course, the bigger reason for the stress was probably the outcome of the election itself.</p><hr><p><em>Update:</em> here’s another take on this subject by <a href=\"http://www.visualisingdata.com/2016/11/gauging-election-reaction/\" target=\"_blank\" rel=\"noopener\">Andy Kirk</a></p><h1 id=\"comments\"><a class=\"anchor\" href=\"#comments\"><span class=\"header-anchor\">#</span></a> Comments</h1><p>David (Nov 14, 2016)</p><blockquote><p>Another question, seriously posed: I totally understand why you added the jitter. Did you anticipate it making people nervous, and now that you know how people felt, would you do it that way again?</p></blockquote><p>Gregor (Nov 14, 2016)</p><blockquote><p>Yes, absolutely. Next time we’ll put the gauges on any forecast and polling average display we publish.</p></blockquote><p>Alper (Nov 14, 2016)</p><blockquote><p>I liked the implementation of animation to communicate model uncertainty, but I felt like it needed some explicit, static annotation of what it was doing.  The absense of this information may have led to viewer stress 😃</p><p>Something like showing a “density field” that shows the probability field under the needle – communicates both the range of variance and the likelihood of the needle landing on a particular position.</p></blockquote><p>hrbrmstr (Nov 14, 2016)</p><blockquote><p>+100 both for the design and the willingness to help convey uncertainty to a general audience. I wish that interpreting and being comfortable with uncertainty was taught as a fundamental skill in school. Oddly enough, folks have no issue with rounding (which inherently introduces uncertainty to a fixed number) but tend to reject the lack of precision when statistics like this are presented.</p></blockquote><p>Nick (Nov 15, 2016)</p><blockquote><p>Are you going to use the jitter animations again?</p></blockquote><p>-stephen (Nov 15, 2016)</p><blockquote><p>It goes well with Fox News’s shakey-cam - they use it on every show with standing mods and TV displays. Makes me nauseous.</p></blockquote><p>Marc Lajoie (Nov 15, 2016)</p><blockquote><p>It made people nervous because it implied a TREND where none existed. They would see their preferred candidate’s chances rising and falling, but the situation actually hadn’t changed at all! Oh no, my candidate is on a downward trajectory! (But not really.) Especially early, when the swings were wild.</p><p>I was on a ferry on my way to work, biting my nails like the rest, then my boat passed through the part of my route that has no cell signal. And the needle was still moving! This was absolutely irresponsible. The whole thing could have been fixed with an editor’s note explaining what was going on, btw.</p></blockquote><p>Gregor (Nov 17, 2016)</p><blockquote><p>I agree on the annotation. That’s something we just forgot to add…</p></blockquote><p>Francisco Guerrero (Nov 17, 2016)</p><blockquote><p>Oh man! your plots were really inspiring. They are like dynamic story arcs: you can see how uncertainty raises, reaches a max, and declines. Actually, this behavior can be described according to the principles of Shannon’s information theory (the mathematics of uncertainty). Changes in uncertainty in natural systems following the same pattern have been documented in molecular biology, stream ecology, and palaeoecology. Now, we can add another example to that growing list: human politics. Thanks.</p></blockquote><p>Robert Monfera (Nov 18, 2016)</p><blockquote><p>No need to be defensive - the jitter, or other means of sampling from a probability distribution visually is great <em>precisely</em> because it causes an emotional response. If probabilities are conveyed properly, then the uncertainty registers with the reader and becomes visceral, which can lead to anxiety. Avoiding this effect would either operate on a false sense of certainty (untruthful) or lagging behind the events (untimely). Embrace and highlight, rather than shield readers from uncertainty.</p></blockquote><p>Axel (Nov 21, 2016)</p><blockquote><p>Did you consider keeping the number below the gauge static? I understand and agree with the arguments for the jitter effect on the needle, but to me the fact that it was applied to the number as well made the whole component and the probability data it conveyed feel unreliable.</p></blockquote>",
    "contentSnippet": "jitter4 There has been some debate about the jittery gauge chart we used in our live election forecast. Rather than replying to dozens of tweets I decided to wrap this up in an old-fashioned blog post™. Feel free to add your thoughts to the comments.\n\n@tsiro: straight up: the NYT needle jitter is irresponsible design at best and unethical design at worst and you should stop looking at it\n(Nov 9, 2016)\n\n# Why did we let the gauge needles jitter?\nWe added movement in our gauges for two reasons. First, we wanted to convey the reality that our forecast was “live” – connected to a live data feed and updating continuously, without any need for a user to refresh his or her browser. Movement helps convey that reality. This technique is common across the web, whether it’s on a site like chartbeat or a live gamecast of a baseball game. A “live” web page where nothing moves feels quite dead to the user. Second, we thought (and still think!) this movement actually helped demonstrate the uncertainty around our forecast, conveying the relative precision of our estimates. In our opinion, having the dial fluctuate a small amount – bound by the 25th and 75 percentile of simulated outcomes – was more successful at conveying the uncertainty around our forecast than simply listing what those percentiles were. As the night went on, the gauges fluctuated less and less as our forecast became more precise. By the end of the night, the gauges barely moved.\n# Why didn’t you just show the chance of winning?\nThe chance of winning is an important indicator that includes the uncertainty of the forecast. So why was that not enough? Well, the problem is that most people aren’t very good at interpreting a probability scale. While it is relatively easy to understand that a 50:50 chance is pretty high, and that a one in a million chance is pretty low, it is very hard to make sense of probabilities in between. How good is a 75% chance of winning? How much better is 85%? So rather than displaying just the chance of winning we wanted to show likely outcomes both in vote margin and electoral votes.\n# Why didn’t you just display the median outcome?\nBecause we thought that most people would read the median outcome as the one most likely outcome, as opposed to one of many very likely outcomes. Seeing the needle actually walk to the lower and higher end of the projected range makes us aware that these are real possibilities. Was it really just random jitter? The jittering was random, but the jitter range was not. The range was fixed between the 25th and 75th percentile of the simulated outcomes. Early in the night, the uncertainty was higher and the ranges wider. Towards the end of the night they got more narrow. The range was displayed as pie chart slice on the gauge, along with the 5-95th range.\n# Why did the needle movements look almost real?\nTo make the movements of the needle look more real we used Perlin noise, an algorithm that originates in computer graphics to simulate natural textures like clouds. We used the implementation from processing.js.\n# Why was the jittering stressing out a lot of people?\nPart of the answer is that we simply don’t feel comfortable being left in uncertainty about something as important as this election. We like to feel safe and cozy, being told our favorite candidate has a “narrow but persistent” lead, but we don’t like being confronted with likely scenarios in which the candidate might actually lose. Of course, the bigger reason for the stress was probably the outcome of the election itself.\n\nUpdate: here’s another take on this subject by Andy Kirk\n# Comments\nDavid (Nov 14, 2016)\n\nAnother question, seriously posed: I totally understand why you added the jitter. Did you anticipate it making people nervous, and now that you know how people felt, would you do it that way again?\n\nGregor (Nov 14, 2016)\n\nYes, absolutely. Next time we’ll put the gauges on any forecast and polling average display we publish.\n\nAlper (Nov 14, 2016)\n\nI liked the implementation of animation to communicate model uncertainty, but I felt like it needed some explicit, static annotation of what it was doing.  The absense of this information may have led to viewer stress 😃\nSomething like showing a “density field” that shows the probability field under the needle – communicates both the range of variance and the likelihood of the needle landing on a particular position.\n\nhrbrmstr (Nov 14, 2016)\n\n+100 both for the design and the willingness to help convey uncertainty to a general audience. I wish that interpreting and being comfortable with uncertainty was taught as a fundamental skill in school. Oddly enough, folks have no issue with rounding (which inherently introduces uncertainty to a fixed number) but tend to reject the lack of precision when statistics like this are presented.\n\nNick (Nov 15, 2016)\n\nAre you going to use the jitter animations again?\n\n-stephen (Nov 15, 2016)\n\nIt goes well with Fox News’s shakey-cam - they use it on every show with standing mods and TV displays. Makes me nauseous.\n\nMarc Lajoie (Nov 15, 2016)\n\nIt made people nervous because it implied a TREND where none existed. They would see their preferred candidate’s chances rising and falling, but the situation actually hadn’t changed at all! Oh no, my candidate is on a downward trajectory! (But not really.) Especially early, when the swings were wild.\nI was on a ferry on my way to work, biting my nails like the rest, then my boat passed through the part of my route that has no cell signal. And the needle was still moving! This was absolutely irresponsible. The whole thing could have been fixed with an editor’s note explaining what was going on, btw.\n\nGregor (Nov 17, 2016)\n\nI agree on the annotation. That’s something we just forgot to add…\n\nFrancisco Guerrero (Nov 17, 2016)\n\nOh man! your plots were really inspiring. They are like dynamic story arcs: you can see how uncertainty raises, reaches a max, and declines. Actually, this behavior can be described according to the principles of Shannon’s information theory (the mathematics of uncertainty). Changes in uncertainty in natural systems following the same pattern have been documented in molecular biology, stream ecology, and palaeoecology. Now, we can add another example to that growing list: human politics. Thanks.\n\nRobert Monfera (Nov 18, 2016)\n\nNo need to be defensive - the jitter, or other means of sampling from a probability distribution visually is great precisely because it causes an emotional response. If probabilities are conveyed properly, then the uncertainty registers with the reader and becomes visceral, which can lead to anxiety. Avoiding this effect would either operate on a false sense of certainty (untruthful) or lagging behind the events (untimely). Embrace and highlight, rather than shield readers from uncertainty.\n\nAxel (Nov 21, 2016)\n\nDid you consider keeping the number below the gauge static? I understand and agree with the arguments for the jitter effect on the needle, but to me the fact that it was applied to the number as well made the whole component and the probability data it conveyed feel unreliable."
  },
  {
    "title": "Re-coloring Illustrator graphics based on JSON data files",
    "link": "https://vis4.net/blog/2017/03/re-coloring-illustrator-graphics-based-on-data-files/",
    "pubDate": "2017-03-02T09:31:18.000Z",
    "isoDate": "2017-03-02T09:31:18.000Z",
    "content": "<p>When working on <a href=\"https://vis4.net/blog/posts/to-cartogram-or-not-to-cartogram-the-brexit/\">choropleth maps</a> or charts in Illustrator, sometimes the (final) data is not yet available by the time you’re designing the graphic. The typical work-around is to re-import the updated part of the graphic and align it with the rest of the artwork. But this is tedious work, especially if you’re dealing with multiple maps. To address this problem I wrote an <a href=\"https://github.com/newsdev/ai-scripts/blob/master/colorize%20artwork.jsx\" target=\"_blank\" rel=\"noopener\">Illustrator script</a> that can re-color the artwork based on a JSON file. This blog post will walk you through how to use the script.</p><h2 id=\"step-1-installing-the-colorizer-script\"><a class=\"anchor\" href=\"#step-1-installing-the-colorizer-script\"><span class=\"header-anchor\">#</span></a> Step 1: Installing the colorizer script</h2><p>To install the script you will need to clone our <a href=\"https://github.com/newsdev/ai-scripts\" target=\"_blank\" rel=\"noopener\">ai-scripts</a> repository inside your Illustrator script folder. If you’re on macOS or OSX this would look something like this:</p><pre><code>❯ cd /Applications/Adobe\\ Illustrator\\ CC\\ 2017❯ cd Presets.localized/en_US/Scripts❯ git clone git@github.com:newsdev/ai-scripts.git</code></pre><p>Depending on what Illustrator version you’re using you might need to adjust the path in the first line. Alternative to cloning the repo, you can also just download the ai-scripts <a href=\"https://github.com/newsdev/ai-scripts/archive/master.zip\" target=\"_blank\" rel=\"noopener\">ZIP archive</a> and unpack it into the Illustrator Scripts folder. <img src=\"/blog/images/old/1f044cc830f3d20d07c58a760eb72ba5.png\" alt=\"\"> After restarting Illustrator you should see the new scripts in the <em>File &gt; Scripts</em> menu: <img src=\"/blog/images/old/c0033e9528c888f1fb92bb8b1e216a47.png\" alt=\"\"></p><h2 id=\"step-2-setting-up-the-illustrator-document\"><a class=\"anchor\" href=\"#step-2-setting-up-the-illustrator-document\"><span class=\"header-anchor\">#</span></a> Step 2: Setting up the Illustrator document</h2><p>In this tutorial we’re going to create a map of U.S. states. Let’s download the <strong><a href=\"https://gist.githubusercontent.com/gka/9864daba96d1111634d319ffe61137ee/raw/9ee5e357a6f97ffbf27b311d47447368b82dffdf/us-states.svg\" target=\"_blank\" rel=\"noopener\">us-states.svg</a></strong> file, which I created by throwing a US census <a href=\"https://www.census.gov/geo/maps-data/data/cbf/cbf_state.html\" target=\"_blank\" rel=\"noopener\">shapefile</a> into the <a href=\"http://mapshaper.org/\" target=\"_blank\" rel=\"noopener\">mapshaper</a> web interface. The important thing about this SVG file is that every state path has the two-letter state postal code set as id attribute. And thankfully, Illustrator is preserving these ids as layer names during SVG import. <img src=\"/blog/images/old/1c57d21f9d116ded9e6e8f6193d3f352.png\" alt=\"\"></p><h2 id=\"step-3-setting-up-the-data-file\"><a class=\"anchor\" href=\"#step-3-setting-up-the-data-file\"><span class=\"header-anchor\">#</span></a> Step 3: Setting up the data file</h2><p>Now we’re going to need to create the JSON file needed by the colorizer-script. You can use whatever script or data source you want to create this file, as long as the final output looks similar to this:</p><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[&#123;</span><br><span class=\"line\">    <span class=\"attr\">\"file\"</span>: <span class=\"string\">\"results\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"data\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"AL\"</span>: &#123; <span class=\"attr\">\"fill\"</span>: <span class=\"string\">\"#cc3d3d\"</span>, <span class=\"attr\">\"stroke\"</span>: <span class=\"literal\">false</span> &#125;,</span><br><span class=\"line\">        <span class=\"attr\">\"AK\"</span>: &#123; <span class=\"attr\">\"fill\"</span>: <span class=\"string\">\"#cc3d3d\"</span>, <span class=\"attr\">\"stroke\"</span>: <span class=\"literal\">false</span> &#125;,</span><br><span class=\"line\">        <span class=\"attr\">\"AZ\"</span>: &#123; <span class=\"attr\">\"fill\"</span>: <span class=\"string\">\"#cc3d3d\"</span>, <span class=\"attr\">\"stroke\"</span>: <span class=\"literal\">false</span> &#125;,</span><br><span class=\"line\">        <span class=\"attr\">\"AR\"</span>: &#123; <span class=\"attr\">\"fill\"</span>: <span class=\"string\">\"#cc3d3d\"</span>, <span class=\"attr\">\"stroke\"</span>: <span class=\"literal\">false</span> &#125;,</span><br><span class=\"line\">        <span class=\"attr\">\"CA\"</span>: &#123; <span class=\"attr\">\"fill\"</span>: <span class=\"string\">\"#1a80c4\"</span>, <span class=\"attr\">\"stroke\"</span>: <span class=\"literal\">false</span> &#125;,</span><br><span class=\"line\">        <span class=\"attr\">\"CO\"</span>: &#123; <span class=\"attr\">\"fill\"</span>: <span class=\"string\">\"#1a80c4\"</span>, <span class=\"attr\">\"stroke\"</span>: <span class=\"literal\">false</span> &#125;,</span><br><span class=\"line\">        //...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;]</span><br></pre></td></tr></table></figure><p>The keys under the “data” section need to match the path names in the Illustrator document, in our case the two-letter state codes. If you set stroke or fill to false, Illustrator will remove the stroke/fill. If you want either stroke or fill to be left unchanged, simply leave them out in the data file. If you’re playing along with the tutorial you can <a href=\"https://gist.githubusercontent.com/gka/9864daba96d1111634d319ffe61137ee/raw/18e377f8253f32522f84e3bc310d07252881421b/colorizer.json\" target=\"_blank\" rel=\"noopener\">download the demo JSON file</a>.</p><h2 id=\"step-4-running-the-script\"><a class=\"anchor\" href=\"#step-4-running-the-script\"><span class=\"header-anchor\">#</span></a> Step 4: Running the script</h2><p>Finally you need to run the script. It will look for a data file called <code>colorizer.json</code> in the same folder where the Illustrator document is located. If it doesn’t find one, a file open dialog will pop up asking you for the location of the JSON data file. <img src=\"/blog/images/old/a1930760736ccab5ab3348d1a62ba2dd.png\" alt=\"\"> Just select the JSON file and click “Open”. Since the data file can contain multiple datasets, another dialog is popping up asking you to select a dataset from a list. This step also serves as final confirmation that you haven’t run the script accidentally. <img src=\"/blog/images/old/f6b2252af4873d5bad93ff947c76b2b8.png\" alt=\"\"> And boom: your map is now re-colored based on the colors from the data file. <img src=\"/blog/images/old/8a8db3e76bb924a540753af6afc5b194.png\" alt=\"\"></p>",
    "contentSnippet": "When working on choropleth maps or charts in Illustrator, sometimes the (final) data is not yet available by the time you’re designing the graphic. The typical work-around is to re-import the updated part of the graphic and align it with the rest of the artwork. But this is tedious work, especially if you’re dealing with multiple maps. To address this problem I wrote an Illustrator script that can re-color the artwork based on a JSON file. This blog post will walk you through how to use the script.\n# Step 1: Installing the colorizer script\nTo install the script you will need to clone our ai-scripts repository inside your Illustrator script folder. If you’re on macOS or OSX this would look something like this:\n❯ cd /Applications/Adobe\\ Illustrator\\ CC\\ 2017❯ cd Presets.localized/en_US/Scripts❯ git clone git@github.com:newsdev/ai-scripts.git\nDepending on what Illustrator version you’re using you might need to adjust the path in the first line. Alternative to cloning the repo, you can also just download the ai-scripts ZIP archive and unpack it into the Illustrator Scripts folder.  After restarting Illustrator you should see the new scripts in the File > Scripts menu: \n# Step 2: Setting up the Illustrator document\nIn this tutorial we’re going to create a map of U.S. states. Let’s download the us-states.svg file, which I created by throwing a US census shapefile into the mapshaper web interface. The important thing about this SVG file is that every state path has the two-letter state postal code set as id attribute. And thankfully, Illustrator is preserving these ids as layer names during SVG import. \n# Step 3: Setting up the data file\nNow we’re going to need to create the JSON file needed by the colorizer-script. You can use whatever script or data source you want to create this file, as long as the final output looks similar to this:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n[{\n    \"file\": \"results\",\n    \"data\": {\n        \"AL\": { \"fill\": \"#cc3d3d\", \"stroke\": false },\n        \"AK\": { \"fill\": \"#cc3d3d\", \"stroke\": false },\n        \"AZ\": { \"fill\": \"#cc3d3d\", \"stroke\": false },\n        \"AR\": { \"fill\": \"#cc3d3d\", \"stroke\": false },\n        \"CA\": { \"fill\": \"#1a80c4\", \"stroke\": false },\n        \"CO\": { \"fill\": \"#1a80c4\", \"stroke\": false },\n        //...\n    }\n}]\n\n\nThe keys under the “data” section need to match the path names in the Illustrator document, in our case the two-letter state codes. If you set stroke or fill to false, Illustrator will remove the stroke/fill. If you want either stroke or fill to be left unchanged, simply leave them out in the data file. If you’re playing along with the tutorial you can download the demo JSON file.\n# Step 4: Running the script\nFinally you need to run the script. It will look for a data file called colorizer.json in the same folder where the Illustrator document is located. If it doesn’t find one, a file open dialog will pop up asking you for the location of the JSON data file.  Just select the JSON file and click “Open”. Since the data file can contain multiple datasets, another dialog is popping up asking you to select a dataset from a list. This step also serves as final confirmation that you haven’t run the script accidentally.  And boom: your map is now re-colored based on the colors from the data file."
  },
  {
    "title": "In Defense of Interactive Graphics",
    "link": "https://vis4.net/blog/2017/03/in-defense-of-interactive-graphics/",
    "pubDate": "2017-03-31T11:36:41.000Z",
    "isoDate": "2017-03-31T11:36:41.000Z",
    "content": "<p>No, interactive graphics are <a href=\"https://medium.com/@dominikus/the-end-of-interactive-visualizations-52c585dcafcb\" target=\"_blank\" rel=\"noopener\">not dead</a>. It is also not true that “85% of the Times‘ page visitors online simply <a href=\"https://www.fastcodesign.com/3069008/the-problem-with-interactive-graphics\" target=\"_blank\" rel=\"noopener\">ignore interactive infographics altogether</a>”. But since I sort of helped creating<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup> this confusion, I think it’s time to set this straight:</p><blockquote><p>Interactive graphics are still great, and there are a lot of good reasons to make them!</p></blockquote><p>Knowing that the majority of readers doesn’t click buttons does not mean you shouldn’t use any buttons. Knowing that many many people will ignore your tooltips doesn’t mean you shouldn’t use any tooltips. All it means is that you should not <em>hide</em> <em>important</em> <em>content</em> behind interactions. If some information is crucial, don’t make the user click or hover to see it (unless you <a href=\"https://www.nytimes.com/interactive/2015/05/28/upshot/you-draw-it-how-family-income-affects-childrens-college-chances.html\" target=\"_blank\" rel=\"noopener\">really want to</a>). But not everything is crucial and 15% of readers isn’t nobody. So there is a lot we can do with interaction, and I am going to point out three examples below.</p><h2 id=\"1-tooltips-allow-your-most-interested-users-to-dig-deep\"><a class=\"anchor\" href=\"#1-tooltips-allow-your-most-interested-users-to-dig-deep\"><span class=\"header-anchor\">#</span></a> 1. Tooltips allow your most interested users to dig deep</h2><p>Take a look at the following graphic which summarized election results <a href=\"https://www.nytimes.com/interactive/2016/05/22/world/europe/europe-right-wing-austria-hungary.html\" target=\"_blank\" rel=\"noopener\">across 20 European countries</a>. Everything you need to see is shown right away. You see the country names, the years and the red bars representing results of right-wing and far-right parties. <img src=\"/blog/images/old/bf9c55c6acbccbc4fa7d44eb085bc77f.png\" alt=\"\"> Further down on the page we showed larger versions of the charts for a few countries which are more relevant because they either held elections recently or going to hold elections soon. In those charts we labels the parties, because they are important and thus should not be hidden behind tooltips. <img src=\"/blog/images/old/a5655d4fca5030afbb397499f674e7d9.png\" alt=\"\"> Now we could have just published this graphic as it is and nothing would be wrong with it. But we decided to add tooltips revealing the party names and vote percentages. <img src=\"/blog/images/old/b032048c776b8040e21936df99f572f4.png\" alt=\"\"> So did people use the tooltips? Oh, yes they did. Our most passionate readers from many different countries went deep into the numbers and fact-checked the party affiliations and elections results, leading to a sad record number of corrections we had to add after first publication. It’s safe to say that none of the errors would have been spotted without the tooltips, and I am happy we were able to correct them.</p><h2 id=\"2-interaction-allow-readers-to-discover-the-full-dataset\"><a class=\"anchor\" href=\"#2-interaction-allow-readers-to-discover-the-full-dataset\"><span class=\"header-anchor\">#</span></a> 2. Interaction allow readers to discover the full dataset</h2><p>There are cases when you have far more data than fit on a page, which means you have to select which charts to show and which to hide. To avoid cherry-picking we usually try to come up with a selection rule that we apply consistently throughout the piece. For instance, in a <a href=\"https://www.nytimes.com/interactive/2017/02/27/us/politics/most-important-problem-gallup-polling-question.html\" target=\"_blank\" rel=\"noopener\">recent graphic</a> we decided to show the first poll after the start of the term for each president. Deciding on such a rule is definitely better than just picking charts, but it can still feel arbitrary sometimes. Fortunately we had already set up the graphic in a way that the charts are rendered dynamically. So it didn’t cost us much to add in a little bonus feature that allows browsing through the entire dataset. <img src=\"/blog/images/old/treemap-gif.gif\" alt=\"\"> We didn’t expect that a lot of readers would use the navigation. In fact, readers even had to stumble across the gray arrows which were hidden by default so they wouldn’t distract from the linear reading flow<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup>. Again, the static version of the piece is great on its own, so missing out on the interaction doesn’t do any harm.</p><h2 id=\"3-interaction-can-help-build-trust-in-your-data-analysis\"><a class=\"anchor\" href=\"#3-interaction-can-help-build-trust-in-your-data-analysis\"><span class=\"header-anchor\">#</span></a> 3. Interaction can help build trust in your data analysis</h2><p>Early in 2016 there was still a field of five contenders in the Republican primaries. While Donald Trump had a slight advantage many political analysts remained skeptical about his chances of eventually securing enough delegates to become the nominee. So we decided to build a <a href=\"https://www.nytimes.com/interactive/2016/02/27/upshot/republican-delegate-calculator-how-trump-can-win.html\" target=\"_blank\" rel=\"noopener\">delegate calculator</a>. Given a baseline estimate of average results per candidate, the calculator simulated 100 random outcomes of the primaries. It took into account the exact rules for allocating delegates, which varied state by state from simple winner-takes-all to proportional allocation to a lot more complicated rules. The somewhat surprising result at the time was that Trump actually wouldn’t need crazy margins of victory to stay on track for the nomination. Of course we didn’t want to appear as if we <em>knew</em> the outcome of the primaries, so we decided to show a couple of mathematically possible scenarios. In one scenario Trump won, in another it was Rubio etc. And at the end of the piece we invited our readers to make up their own scenarios.</p><blockquote><p><em>Adjust the sliders in the chart below to explore more scenarios. The scenarios presented here do not aim to predict what will happen, because we don’t know what will. But they are theoretically and mathematically possible, and thus help us understand the dynamics of the race to the Republican nomination.</em></p></blockquote><p><img src=\"/blog/images/old/delegate-calc-1.gif\" alt=\"\"></p><h2 id=\"interaction-transparency-trust\"><a class=\"anchor\" href=\"#interaction-transparency-trust\"><span class=\"header-anchor\">#</span></a> Interaction→ transparency→ trust</h2><p>So, concluding this post, I hope it became clear that there are still a lot of good reasons for making graphics interactive, even if only a small share of the audience will dig into it. Tooltips make the presented data more accessible and invite readers to fact-check your numbers. Subtle navigation buttons can provide access to hundreds of interesting charts you otherwise would have to cut out of a piece. Sliders can invite readers to test your simulation code and try out their own scenarios. And if interactive graphics are not just a fun addition but can actually increase the transparency of our work, open us for criticism, and thereby, hopefully, help re-build some trust in journalism, I think it they’re worth it.</p><hr class=\"footnotes-sep\"><section class=\"footnotes\"><ol class=\"footnotes-list\"><li id=\"fn1\" class=\"footnote-item\"><p>The number comes from me measuring the percent of readers who clicked on a prominent button in a couple of graphics we published in 2015. I mentioned the number at a <a href=\"http://www.informationplusconference.com\" target=\"_blank\" rel=\"noopener\">conference</a>, from where it ended up in another talk, then on <a href=\"https://medium.com/@dominikus/the-end-of-interactive-visualizations-52c585dcafcb\" target=\"_blank\" rel=\"noopener\">Medium</a> and finally on <a href=\"https://www.fastcodesign.com/3069008/the-problem-with-interactive-graphics\" target=\"_blank\" rel=\"noopener\">FastCo.Design</a>. However, the 85% of readers didn't ignore the graphic, they just didn't click the button. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p></li><li id=\"fn2\" class=\"footnote-item\"><p>I later changed the behavior so that the arrows show up permanently as soon as you scroll down far enough into the graphic <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p></li></ol></section>",
    "contentSnippet": "No, interactive graphics are not dead. It is also not true that “85% of the Times‘ page visitors online simply ignore interactive infographics altogether”. But since I sort of helped creating[1] this confusion, I think it’s time to set this straight:\n\nInteractive graphics are still great, and there are a lot of good reasons to make them!\n\nKnowing that the majority of readers doesn’t click buttons does not mean you shouldn’t use any buttons. Knowing that many many people will ignore your tooltips doesn’t mean you shouldn’t use any tooltips. All it means is that you should not hide important content behind interactions. If some information is crucial, don’t make the user click or hover to see it (unless you really want to). But not everything is crucial and 15% of readers isn’t nobody. So there is a lot we can do with interaction, and I am going to point out three examples below.\n# 1. Tooltips allow your most interested users to dig deep\nTake a look at the following graphic which summarized election results across 20 European countries. Everything you need to see is shown right away. You see the country names, the years and the red bars representing results of right-wing and far-right parties.  Further down on the page we showed larger versions of the charts for a few countries which are more relevant because they either held elections recently or going to hold elections soon. In those charts we labels the parties, because they are important and thus should not be hidden behind tooltips.  Now we could have just published this graphic as it is and nothing would be wrong with it. But we decided to add tooltips revealing the party names and vote percentages.  So did people use the tooltips? Oh, yes they did. Our most passionate readers from many different countries went deep into the numbers and fact-checked the party affiliations and elections results, leading to a sad record number of corrections we had to add after first publication. It’s safe to say that none of the errors would have been spotted without the tooltips, and I am happy we were able to correct them.\n# 2. Interaction allow readers to discover the full dataset\nThere are cases when you have far more data than fit on a page, which means you have to select which charts to show and which to hide. To avoid cherry-picking we usually try to come up with a selection rule that we apply consistently throughout the piece. For instance, in a recent graphic we decided to show the first poll after the start of the term for each president. Deciding on such a rule is definitely better than just picking charts, but it can still feel arbitrary sometimes. Fortunately we had already set up the graphic in a way that the charts are rendered dynamically. So it didn’t cost us much to add in a little bonus feature that allows browsing through the entire dataset.  We didn’t expect that a lot of readers would use the navigation. In fact, readers even had to stumble across the gray arrows which were hidden by default so they wouldn’t distract from the linear reading flow[2]. Again, the static version of the piece is great on its own, so missing out on the interaction doesn’t do any harm.\n# 3. Interaction can help build trust in your data analysis\nEarly in 2016 there was still a field of five contenders in the Republican primaries. While Donald Trump had a slight advantage many political analysts remained skeptical about his chances of eventually securing enough delegates to become the nominee. So we decided to build a delegate calculator. Given a baseline estimate of average results per candidate, the calculator simulated 100 random outcomes of the primaries. It took into account the exact rules for allocating delegates, which varied state by state from simple winner-takes-all to proportional allocation to a lot more complicated rules. The somewhat surprising result at the time was that Trump actually wouldn’t need crazy margins of victory to stay on track for the nomination. Of course we didn’t want to appear as if we knew the outcome of the primaries, so we decided to show a couple of mathematically possible scenarios. In one scenario Trump won, in another it was Rubio etc. And at the end of the piece we invited our readers to make up their own scenarios.\n\nAdjust the sliders in the chart below to explore more scenarios. The scenarios presented here do not aim to predict what will happen, because we don’t know what will. But they are theoretically and mathematically possible, and thus help us understand the dynamics of the race to the Republican nomination.\n\n\n# Interaction→ transparency→ trust\nSo, concluding this post, I hope it became clear that there are still a lot of good reasons for making graphics interactive, even if only a small share of the audience will dig into it. Tooltips make the presented data more accessible and invite readers to fact-check your numbers. Subtle navigation buttons can provide access to hundreds of interesting charts you otherwise would have to cut out of a piece. Sliders can invite readers to test your simulation code and try out their own scenarios. And if interactive graphics are not just a fun addition but can actually increase the transparency of our work, open us for criticism, and thereby, hopefully, help re-build some trust in journalism, I think it they’re worth it.\n\n\nThe number comes from me measuring the percent of readers who clicked on a prominent button in a couple of graphics we published in 2015. I mentioned the number at a conference, from where it ended up in another talk, then on Medium and finally on FastCo.Design. However, the 85% of readers didn't ignore the graphic, they just didn't click the button. ↩︎\n\nI later changed the behavior so that the arrows show up permanently as soon as you scroll down far enough into the graphic ↩︎"
  },
  {
    "title": "Running elementary.os on a MacBook, what works well, and what does not.",
    "link": "https://vis4.net/blog/2017/04/elementary-os-macbook/",
    "pubDate": "2017-04-21T10:36:56.000Z",
    "isoDate": "2017-04-21T10:36:56.000Z",
    "content": "<p>After being disappointed with the direction in which macOS is heading, especially the continuous push into iCloud features that I refuse to use <sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>, I decided to say good-bye to macOS and try <a href=\"https://elementary.io/\" target=\"_blank\" rel=\"noopener\">elementary.os</a>. After testing it on a separate desktop computer for a couple of months I finally installed it on my MacBook<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup>. In this post I am documenting my experience for those who might be interested in switching as well.  <img src=\"//www.vis4.net/blog/wp-content/uploads/2017/04/47ef725da42ab0982de2f25440d04673.png\" alt=\"\"></p><h2 id=\"installation\"><a class=\"anchor\" href=\"#installation\"><span class=\"header-anchor\">#</span></a> Installation</h2><p>For the installation I was simply following the nice <a href=\"https://aroman.github.io/elementary-on-a-mac/\" target=\"_blank\" rel=\"noopener\">elementary-on-a-mac tutorial</a>. The installations of the rEFInd boot manager worked fine, so I can still boot into OSX if I want. After a couple of months the only times I found myself booting into OSX was when I wanted to edit photos in Lightroom. Everything else I do in Linux now.</p><h2 id=\"hardware-support\"><a class=\"anchor\" href=\"#hardware-support\"><span class=\"header-anchor\">#</span></a> Hardware support</h2><p>The <strong>wifi</strong> adapter did not work right out of the box, but after a bit of searching I managed to install the correct driver. You might need a USB ethernet dongle. Also it seems the macbook <strong>webcam</strong> is not supported out of the box, but since I am not using it anyway I didn’t bother to try to find a fix. Other than that, everything is working fine for me. elementary.os works great with the <strong>retina display</strong>. For the non-X terminal there is a way to set the font size. Unfortunately some apps that run on older GTK are still not supporting retina, such as Gimp and Inkscape, so working with those is almost impossible. elementary.os boots up in 5-7 seconds. however, due to a known bug sometimes the login freezes for 20 seconds after you logged in. it’s annoying, but not a showstopper for me. hope this is going to be fixed soon. <em>(update: it’s fixed)</em></p><h2 id=\"email-calendar-etc\"><a class=\"anchor\" href=\"#email-calendar-etc\"><span class=\"header-anchor\">#</span></a> Email, Calendar etc</h2><p><img src=\"//www.vis4.net/blog/wp-content/uploads/2017/04/91da23e2ff414e2fa599e5b8ef75e4f9.png\" alt=\"\"> I tried to use the new Calender and Mail apps but ran into all sorts of problems. I had trouble setting up my caldav calendars, and something else didn’t work with Mail. so I just unstalled the apps and started using Evolution, which is more like an all-in-one Outlook kind of software. And it works pretty good.</p><h2 id=\"browsing\"><a class=\"anchor\" href=\"#browsing\"><span class=\"header-anchor\">#</span></a> Browsing</h2><p>The built-in browser epiphany wasn’t for me, so I am using Chromium and Firefox. <del>To get Netflix working I had to install Google Chrome as well, but that’s pretty much is the only website I’m using it for.</del> Netflix is supported in Firefox once you <a href=\"https://linuxconfig.org/play-netflix-on-linux-with-firefox\" target=\"_blank\" rel=\"noopener\">enable the DRM</a> content setting. Sometimes I find some sites that use some Flash applets but those are getting rare.</p><h2 id=\"office\"><a class=\"anchor\" href=\"#office\"><span class=\"header-anchor\">#</span></a> Office</h2><p>I am a long-time user of LibreOffice and already learned to deal with it’s quirks. Unsurprisingly, LibreOffice works fine on Linux. But for those who can’t live without Microsoft Office, there seem to be <a href=\"https://www.howtogeek.com/171565/how-to-install-microsoft-office-on-linux/\" target=\"_blank\" rel=\"noopener\">ways to install it</a>.</p><h2 id=\"music-and-movies\"><a class=\"anchor\" href=\"#music-and-movies\"><span class=\"header-anchor\">#</span></a> Music and movies</h2><p>VLC works just fine on Linux. And in case you’re still using old fashioned mp3 music collections: I ended up using a program called <a href=\"https://wiki.gnome.org/Apps/Rhythmbox/Screenshots\" target=\"_blank\" rel=\"noopener\">Rhythmbox</a> as iTunes replacement. It works fine with my music stored on a Samba network share, can show album covers, and has plugins for podcasts and external music providers (which I mostly don’t use.)</p><h2 id=\"missing-apps\"><a class=\"anchor\" href=\"#missing-apps\"><span class=\"header-anchor\">#</span></a> Missing apps</h2><p>For some applications like Adobe Lightroom, Photoshop and Illustrator there are just no sufficient Linux replacements. There is a way to set up OSX in a VirtualBox, but it takes some time and then the integrations are not working as smooth as with windows VMs. So I ended up keeping OSX as backup system. I’ve been reluctant so far to I’ll install a windows vm, but that would certainly be another solution.</p><h2 id=\"coding\"><a class=\"anchor\" href=\"#coding\"><span class=\"header-anchor\">#</span></a> Coding</h2><p>SublimeText is running smooth on Linux. Python (via pyenv), Node, and RStudio are also working just as expected. RStudio has no problem more with retina screens than it has on OSX.</p><h2 id=\"database-gui\"><a class=\"anchor\" href=\"#database-gui\"><span class=\"header-anchor\">#</span></a> Database GUI</h2><p>I was using SequelPro on OSX which is not available on Linux. But <a href=\"http://dbeaver.jkiss.org/\" target=\"_blank\" rel=\"noopener\">DBeaver</a> is a full replacement, and to some extent even better<sup class=\"footnote-ref\"><a href=\"#fn3\" id=\"fnref3\">[3]</a></sup>. and it’s under active development: after being unable to connect to a mysql db via ssh tunnel I reported a bug on github and the bugfix was released just a few days later!</p><h2 id=\"package-manager\"><a class=\"anchor\" href=\"#package-manager\"><span class=\"header-anchor\">#</span></a> Package manager</h2><p>On OSX I was using a mix of manual installation and homebrew. For as long as I was using homebrew it was always broken (according to brew doctor). And on every major system update something changed in XCode that would break homebrew and the only way to fix it was to install 3GB of XCode update. Now I am using apt and I like it way better than homebrew. That’s it for now, but I intend to update this blog from time to time.</p><h1 id=\"comments\"><a class=\"anchor\" href=\"#comments\"><span class=\"header-anchor\">#</span></a> Comments</h1><p>Perugino Fortuna (Apr 21, 2017)</p><blockquote><p>sehr cool!<br>ein Bewunderer,  der nie wieder ein KernelUpdate machen wird…</p></blockquote><hr class=\"footnotes-sep\"><section class=\"footnotes\"><ol class=\"footnotes-list\"><li id=\"fn1\" class=\"footnote-item\"><p>most annoyingly the iCloud synchronization of sensitive files that is turned-on by default <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p></li><li id=\"fn2\" class=\"footnote-item\"><p>13-inch MacBook Pro from 2015 <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p></li><li id=\"fn3\" class=\"footnote-item\"><p>like, it automatically draws ORM diagrams based on your DB structure, how cool is that? <a href=\"#fnref3\" class=\"footnote-backref\">↩︎</a></p></li></ol></section>",
    "contentSnippet": "After being disappointed with the direction in which macOS is heading, especially the continuous push into iCloud features that I refuse to use [1], I decided to say good-bye to macOS and try elementary.os. After testing it on a separate desktop computer for a couple of months I finally installed it on my MacBook[2]. In this post I am documenting my experience for those who might be interested in switching as well.  \n# Installation\nFor the installation I was simply following the nice elementary-on-a-mac tutorial. The installations of the rEFInd boot manager worked fine, so I can still boot into OSX if I want. After a couple of months the only times I found myself booting into OSX was when I wanted to edit photos in Lightroom. Everything else I do in Linux now.\n# Hardware support\nThe wifi adapter did not work right out of the box, but after a bit of searching I managed to install the correct driver. You might need a USB ethernet dongle. Also it seems the macbook webcam is not supported out of the box, but since I am not using it anyway I didn’t bother to try to find a fix. Other than that, everything is working fine for me. elementary.os works great with the retina display. For the non-X terminal there is a way to set the font size. Unfortunately some apps that run on older GTK are still not supporting retina, such as Gimp and Inkscape, so working with those is almost impossible. elementary.os boots up in 5-7 seconds. however, due to a known bug sometimes the login freezes for 20 seconds after you logged in. it’s annoying, but not a showstopper for me. hope this is going to be fixed soon. (update: it’s fixed)\n# Email, Calendar etc\n I tried to use the new Calender and Mail apps but ran into all sorts of problems. I had trouble setting up my caldav calendars, and something else didn’t work with Mail. so I just unstalled the apps and started using Evolution, which is more like an all-in-one Outlook kind of software. And it works pretty good.\n# Browsing\nThe built-in browser epiphany wasn’t for me, so I am using Chromium and Firefox. To get Netflix working I had to install Google Chrome as well, but that’s pretty much is the only website I’m using it for. Netflix is supported in Firefox once you enable the DRM content setting. Sometimes I find some sites that use some Flash applets but those are getting rare.\n# Office\nI am a long-time user of LibreOffice and already learned to deal with it’s quirks. Unsurprisingly, LibreOffice works fine on Linux. But for those who can’t live without Microsoft Office, there seem to be ways to install it.\n# Music and movies\nVLC works just fine on Linux. And in case you’re still using old fashioned mp3 music collections: I ended up using a program called Rhythmbox as iTunes replacement. It works fine with my music stored on a Samba network share, can show album covers, and has plugins for podcasts and external music providers (which I mostly don’t use.)\n# Missing apps\nFor some applications like Adobe Lightroom, Photoshop and Illustrator there are just no sufficient Linux replacements. There is a way to set up OSX in a VirtualBox, but it takes some time and then the integrations are not working as smooth as with windows VMs. So I ended up keeping OSX as backup system. I’ve been reluctant so far to I’ll install a windows vm, but that would certainly be another solution.\n# Coding\nSublimeText is running smooth on Linux. Python (via pyenv), Node, and RStudio are also working just as expected. RStudio has no problem more with retina screens than it has on OSX.\n# Database GUI\nI was using SequelPro on OSX which is not available on Linux. But DBeaver is a full replacement, and to some extent even better[3]. and it’s under active development: after being unable to connect to a mysql db via ssh tunnel I reported a bug on github and the bugfix was released just a few days later!\n# Package manager\nOn OSX I was using a mix of manual installation and homebrew. For as long as I was using homebrew it was always broken (according to brew doctor). And on every major system update something changed in XCode that would break homebrew and the only way to fix it was to install 3GB of XCode update. Now I am using apt and I like it way better than homebrew. That’s it for now, but I intend to update this blog from time to time.\n# Comments\nPerugino Fortuna (Apr 21, 2017)\n\nsehr cool!\nein Bewunderer,  der nie wieder ein KernelUpdate machen wird…\n\n\n\nmost annoyingly the iCloud synchronization of sensitive files that is turned-on by default ↩︎\n\n13-inch MacBook Pro from 2015 ↩︎\n\nlike, it automatically draws ORM diagrams based on your DB structure, how cool is that? ↩︎"
  },
  {
    "title": "So long, Wordpress",
    "link": "https://vis4.net/blog/2017/10/goodbye-wordpress/",
    "pubDate": "2017-10-03T18:19:49.000Z",
    "isoDate": "2017-10-03T18:19:49.000Z",
    "content": "<p>That’s it, I finally killed my Wordpress installation and moved the entire blog over to a static website generator.</p><p>After looking at a few frameworks I decided to go with <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>. The config files took a few minutes to get used to, but thanks to the plugin system I managed to get almost everything I wanted.</p><h2 id=\"migrating-the-old-posts\"><a class=\"anchor\" href=\"#migrating-the-old-posts\"><span class=\"header-anchor\">#</span></a> Migrating the old posts</h2><p>To get my stuff out of Wordpress I used the XML export and a <a href=\"https://gist.github.com/gka/5476f61bfb311447eac237192d4f71da\" target=\"_blank\" rel=\"noopener\">Python script</a> I wrote<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup> that converts the posts into Hexo-compatible markdown files. It also downloaded the embedded images and created the redirect directives so the old urls would still work.</p><p>The import went mostly fine, but I still have to go through many of the older posts to clean up the markdown.</p><h2 id=\"theming\"><a class=\"anchor\" href=\"#theming\"><span class=\"header-anchor\">#</span></a> Theming</h2><p>I went with a lazy color and font variation of the excellent <a href=\"https://probberechts.github.io/cactus-dark/\" target=\"_blank\" rel=\"noopener\">Cactus Dark</a> theme by Pieter Robberechts.</p><h2 id=\"plugins-plugins-plugins\"><a class=\"anchor\" href=\"#plugins-plugins-plugins\"><span class=\"header-anchor\">#</span></a> Plugins, plugins, plugins</h2><p>The great thing about Hexo is how easy it is to add more features to it. I ended up swapping the default markdown renderer for <a href=\"https://github.com/CHENXCHEN/hexo-renderer-markdown-it-plus\" target=\"_blank\" rel=\"noopener\">markdown-it-plus</a> to get everything I need.</p><p><strong>Syntax highlighting:</strong> just works!</p><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">foo</span><span class=\"params\">(syntax, highlighting=<span class=\"string\">'great'</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">yield</span> <span class=\"string\">'yay'</span></span><br></pre></td></tr></table></figure><p><strong>Footnotes:</strong> couldn’t live without them<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup>.</p><p><strong>Custom HTML:</strong> Writing markdown is great and saves a lot of time, but sometimes you just need <span style=\"color:magenta; display:inline-block; transform: rotate(-20deg);\">something</span> <span style=\"color:cyan; display:inline-block; transform: rotate(20deg);\">more</span>.</p><p><strong>Latex math expressions:</strong> To be fair, I only used them in one blog post in 2009 and I honestly have no idea how to write them anymore, but it’s good that the blog supports it!</p><p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>!</mo><mrow><mo fence=\"true\">(</mo><mtable><mtr><mtd><mrow><msup><mi>r</mi><mrow><mi mathvariant=\"normal\">′</mi></mrow></msup></mrow></mtd></mtr><mtr><mtd><mrow><mtext> </mtext><msup><mi>g</mi><mrow><mi mathvariant=\"normal\">′</mi></mrow></msup></mrow></mtd></mtr><mtr><mtd><mrow><mtext> </mtext><msup><mi>b</mi><mrow><mi mathvariant=\"normal\">′</mi></mrow></msup></mrow></mtd></mtr></mtable><mo fence=\"true\">)</mo></mrow><mo>=</mo><mi>α</mi><mo>⋅</mo><mrow><mo fence=\"true\">(</mo><mtable><mtr><mtd><mrow><mi>r</mi></mrow></mtd></mtr><mtr><mtd><mrow><mtext> </mtext><mi>g</mi></mrow></mtd></mtr><mtr><mtd><mrow><mtext> </mtext><mi>b</mi></mrow></mtd></mtr></mtable><mo fence=\"true\">)</mo></mrow><mo separator=\"true\">;</mo><mn>0</mn><mo>&lt;</mo><mi>α</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">!\\left(\\begin{array}{c} r&#x27;\\\\\\ g&#x27;\\\\\\ b&#x27;\\end{array}\\right)=\\alpha\\cdot\\left(\\begin{array}{c} r\\\\\\ g\\\\\\ b\\end{array}\\right);0&lt;\\alpha&lt;1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:2.05002em;\"></span><span class=\"strut bottom\" style=\"height:3.60004em;vertical-align:-1.55002em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mclose\">!</span><span class=\"minner displaystyle textstyle uncramped\"><span class=\"style-wrap reset-textstyle textstyle uncramped\"><span class=\"delimsizing mult\"><span class=\"vlist\"><span style=\"top:0.9049999999999999em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"delimsizinginner delim-size4\"><span>⎝</span></span></span><span style=\"top:-0.89502em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"delimsizinginner delim-size4\"><span>⎛</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist\"><span style=\"top:-1.2099999999999997em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mord mathrm\">′</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span><span style=\"top:-0.00999999999999951em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle uncramped\"><span class=\"mord mspace\"> </span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mord mathrm\">′</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span><span style=\"top:1.1900000000000006em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle uncramped\"><span class=\"mord mspace\"> </span><span class=\"mord\"><span class=\"mord mathit\">b</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mord mathrm\">′</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span></span></span><span class=\"style-wrap reset-textstyle textstyle uncramped\"><span class=\"delimsizing mult\"><span class=\"vlist\"><span style=\"top:0.9049999999999999em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"delimsizinginner delim-size4\"><span>⎠</span></span></span><span style=\"top:-0.89502em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"delimsizinginner delim-size4\"><span>⎞</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.0037em;\">α</span><span class=\"mbin\">⋅</span><span class=\"minner displaystyle textstyle uncramped\"><span class=\"style-wrap reset-textstyle textstyle uncramped\"><span class=\"delimsizing mult\"><span class=\"vlist\"><span style=\"top:0.9049999999999999em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"delimsizinginner delim-size4\"><span>⎝</span></span></span><span style=\"top:-0.89502em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"delimsizinginner delim-size4\"><span>⎛</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist\"><span style=\"top:-1.2099999999999997em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span></span></span><span style=\"top:-0.00999999999999951em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle uncramped\"><span class=\"mord mspace\"> </span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span></span></span><span style=\"top:1.1900000000000006em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle uncramped\"><span class=\"mord mspace\"> </span><span class=\"mord mathit\">b</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span></span></span><span class=\"style-wrap reset-textstyle textstyle uncramped\"><span class=\"delimsizing mult\"><span class=\"vlist\"><span style=\"top:0.9049999999999999em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"delimsizinginner delim-size4\"><span>⎠</span></span></span><span style=\"top:-0.89502em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"delimsizinginner delim-size4\"><span>⎞</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span><span class=\"mpunct\">;</span><span class=\"mord mathrm\">0</span><span class=\"mrel\">&lt;</span><span class=\"mord mathit\" style=\"margin-right:0.0037em;\">α</span><span class=\"mrel\">&lt;</span><span class=\"mord mathrm\">1</span></span></span></span></span></p><h2 id=\"what-about-comments\"><a class=\"anchor\" href=\"#what-about-comments\"><span class=\"header-anchor\">#</span></a> What about comments?</h2><p>That’s the only sad part so far. A static website has no commenting system. I thought about using Disqus, but since they now seem to run ads in the free plan, I decided against it. Maybe I’ll come up with a solution later.<sup class=\"footnote-ref\"><a href=\"#fn3\" id=\"fnref3\">[3]</a></sup></p><p>However, since the XML file included the old comments I was able to just bake them into the static markdown files, so they’re at least preserved for the future.</p><h2 id=\"why-spend-all-this-effort\"><a class=\"anchor\" href=\"#why-spend-all-this-effort\"><span class=\"header-anchor\">#</span></a> Why spend all this effort</h2><p>Guess it’s the same reason why everyone moves away from Wordpress in the end: maintainance just takes up too much work. Every other month you need to upgrade the CMS or the plugins to make sure all security holes are fixed. After a couple years you accumulate too many plugins so eventually something will break.</p><p>For me it was the login to the Wordpress admin. A while ago I was freaking out about someone breaking into my Wordpress – automatic bots try to login all the time – so I installed plugins protecting the login. I was even able to integrate my YubiKey! But last week, I guess some automatic update of something must have caused the whole setup to fail and I wasn’t able to get back in. It took me a couple hours to re-gain access to my own CMS<sup class=\"footnote-ref\"><a href=\"#fn4\" id=\"fnref4\">[4]</a></sup>.</p><hr class=\"footnotes-sep\"><section class=\"footnotes\"><ol class=\"footnotes-list\"><li id=\"fn1\" class=\"footnote-item\"><p>Well, I started using <a href=\"https://github.com/dreikanter/wp2md\" target=\"_blank\" rel=\"noopener\">wp2md</a> but then I ended up rewriting everything except Aaron Swartz'(!) html2text script <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p></li><li id=\"fn2\" class=\"footnote-item\"><p>maybe without this one <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p></li><li id=\"fn3\" class=\"footnote-item\"><p>for now, if you feel like getting in touch, shoot me a message on <a href=\"https://keybase.io/\" target=\"_blank\" rel=\"noopener\">Keybase.io</a> <a href=\"#fnref3\" class=\"footnote-backref\">↩︎</a></p></li><li id=\"fn4\" class=\"footnote-item\"><p>which was very frustrating: 99% of the tutorials for resetting a wordpress password directly in the database are plain wrong (my blog wasn't using MD5 for hashing passwords) and I had to figure out how to disable all plugins <a href=\"#fnref4\" class=\"footnote-backref\">↩︎</a></p></li></ol></section>",
    "contentSnippet": "That’s it, I finally killed my Wordpress installation and moved the entire blog over to a static website generator.\nAfter looking at a few frameworks I decided to go with Hexo. The config files took a few minutes to get used to, but thanks to the plugin system I managed to get almost everything I wanted.\n# Migrating the old posts\nTo get my stuff out of Wordpress I used the XML export and a Python script I wrote[1] that converts the posts into Hexo-compatible markdown files. It also downloaded the embedded images and created the redirect directives so the old urls would still work.\nThe import went mostly fine, but I still have to go through many of the older posts to clean up the markdown.\n# Theming\nI went with a lazy color and font variation of the excellent Cactus Dark theme by Pieter Robberechts.\n# Plugins, plugins, plugins\nThe great thing about Hexo is how easy it is to add more features to it. I ended up swapping the default markdown renderer for markdown-it-plus to get everything I need.\nSyntax highlighting: just works!\n\n\n1\n2\n\ndef foo(syntax, highlighting='great'):\n    yield 'yay'\n\n\nFootnotes: couldn’t live without them[2].\nCustom HTML: Writing markdown is great and saves a lot of time, but sometimes you just need something more.\nLatex math expressions: To be fair, I only used them in one blog post in 2009 and I honestly have no idea how to write them anymore, but it’s good that the blog supports it!\n!(r′ g′ b′)=α⋅(r g b);0<α<1!\\left(\\begin{array}{c} r'\\\\\\ g'\\\\\\ b'\\end{array}\\right)=\\alpha\\cdot\\left(\\begin{array}{c} r\\\\\\ g\\\\\\ b\\end{array}\\right);0<\\alpha<1!​⎝​⎛​​​r​′​​​ g​′​​​ b​′​​​​​⎠​⎞​​=α⋅​⎝​⎛​​​r​ g​ b​​​⎠​⎞​​;0<α<1\n# What about comments?\nThat’s the only sad part so far. A static website has no commenting system. I thought about using Disqus, but since they now seem to run ads in the free plan, I decided against it. Maybe I’ll come up with a solution later.[3]\nHowever, since the XML file included the old comments I was able to just bake them into the static markdown files, so they’re at least preserved for the future.\n# Why spend all this effort\nGuess it’s the same reason why everyone moves away from Wordpress in the end: maintainance just takes up too much work. Every other month you need to upgrade the CMS or the plugins to make sure all security holes are fixed. After a couple years you accumulate too many plugins so eventually something will break.\nFor me it was the login to the Wordpress admin. A while ago I was freaking out about someone breaking into my Wordpress – automatic bots try to login all the time – so I installed plugins protecting the login. I was even able to integrate my YubiKey! But last week, I guess some automatic update of something must have caused the whole setup to fail and I wasn’t able to get back in. It took me a couple hours to re-gain access to my own CMS[4].\n\n\nWell, I started using wp2md but then I ended up rewriting everything except Aaron Swartz'(!) html2text script ↩︎\n\nmaybe without this one ↩︎\n\nfor now, if you feel like getting in touch, shoot me a message on Keybase.io ↩︎\n\nwhich was very frustrating: 99% of the tutorials for resetting a wordpress password directly in the database are plain wrong (my blog wasn't using MD5 for hashing passwords) and I had to figure out how to disable all plugins ↩︎"
  },
  {
    "title": "Say hello to Schnack.js: A new Disqus-like commenting drop-in for static websites",
    "link": "https://vis4.net/blog/2017/10/hello-schnack/",
    "pubDate": "2017-10-08T20:54:30.000Z",
    "isoDate": "2017-10-08T20:54:30.000Z",
    "content": "<p><strong>tl;dr:</strong> <a href=\"https://github.com/gka/schnack\" target=\"_blank\" rel=\"noopener\">Schnack.js</a> is an awesome, ad-free, open source Disqus clone that comes with a very minimal, hackable code base.</p><h2 id=\"how-to-embed-schnackjs\"><a class=\"anchor\" href=\"#how-to-embed-schnackjs\"><span class=\"header-anchor\">#</span></a> How to embed Schnack.js</h2><p>It works pretty much like other drop-in commenting systems. You drop a script tag under your blog posts, pass a few parameters and Schnack is doing the rest for you.</p><figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">\"comments-go-here\"</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">script</span> <span class=\"attr\">type</span>=<span class=\"string\">\"text/javascript\"</span></span></span><br><span class=\"line\"><span class=\"tag\">   <span class=\"attr\">src</span>=<span class=\"string\">\"https://schnack.youdomain.com/embed.js\"</span></span></span><br><span class=\"line\"><span class=\"tag\">   <span class=\"attr\">data-schnack-target</span>=<span class=\"string\">\".comments-go-here\"</span></span></span><br><span class=\"line\"><span class=\"tag\">   <span class=\"attr\">data-schnack-slug</span>=<span class=\"string\">\"my-blogpost-slug\"</span>&gt;</span><span class=\"undefined\"></span><span class=\"tag\">&lt;/<span class=\"name\">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Thanks to the <a href=\"#standing-on-the-shoulders-of-giants\">wonders</a> of modern science the <code>embed.js</code> comes at 2KB gzipped and minified. Your site has full control of the styling of the comments.</p><h2 id=\"how-schnack-is-protecting-against-spam\"><a class=\"anchor\" href=\"#how-schnack-is-protecting-against-spam\"><span class=\"header-anchor\">#</span></a> How Schnack is protecting against spam</h2><p>Schnack asks comment authors to sign in via Twitter or Github<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>. New comments need to be approved by the site owner before they show up. To save you some work you can also “trust” comment authors, in which case their comments are being approved automatically.</p><h2 id=\"how-to-install-schnack-on-your-server\"><a class=\"anchor\" href=\"#how-to-install-schnack-on-your-server\"><span class=\"header-anchor\">#</span></a> How to install Schnack on your server</h2><p>Schnack is a Node app and uses a SQLite database to store the comments. So all you really need to do is to clone the repo, run <code>npm install</code> and start the app.</p><p>To hook up the authentication you need to go to <a href=\"http://apps.twitter.com\" target=\"_blank\" rel=\"noopener\">apps.twitter.com</a>, create a new app and copy the api keys over to your <a href=\"https://github.com/gka/schnack/blob/master/config.tpl.json\" target=\"_blank\" rel=\"noopener\">config file</a>.</p><p>Schnack needs to run on a subdomain of the domain your blog is running on, mainly because that’s what cookie authentication is limited to<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup>.</p><h2 id=\"why-did-we-decide-to-work-on-schnack\"><a class=\"anchor\" href=\"#why-did-we-decide-to-work-on-schnack\"><span class=\"header-anchor\">#</span></a> Why did  we decide to work on Schnack?</h2><p>Ok, here’s a little backstory:</p><p><a href=\"/blog/2017/10/goodbye-wordpress/\">Last week</a> I replaced my old Wordpress blog with a static <a href=\"https://hexo.io\" target=\"_blank\" rel=\"noopener\">Hexo</a> website. Hexo gave me everything I wanted, except for one thing: I missed the comment box below my posts.</p><p>So I scouted out my options. <a href=\"https://disqus.com/\" target=\"_blank\" rel=\"noopener\">Disqus</a> was my first consideration, but I was put off by the ads that come with the free “Basic” plan, aside from <a href=\"https://blog.disqus.com/security-alert-user-info-breach?utm_source=motd_marketing&amp;utm_medium=web\" target=\"_blank\" rel=\"noopener\">privacy considerations</a>.</p><p>Then I thought there must be an app for this, but when I didn’t find one that fit my needs (see next section) I decided it’s time to do some hacking<sup class=\"footnote-ref\"><a href=\"#fn3\" id=\"fnref3\">[3]</a></sup>!</p><p>The next step was to put up a readme on Github, just to keep notes of what this app might look like and how it should be organized. My curious friends over at <a href=\"https://webkid.io\" target=\"_blank\" rel=\"noopener\">Webkid</a> found the readme while Github-lurking on my repositories and that’s how we got started!</p><h2 id=\"prior-art\"><a class=\"anchor\" href=\"#prior-art\"><span class=\"header-anchor\">#</span></a> Prior art</h2><p>Of course, Schnack is not the first of its kind. There are several open source alternatives that I could’ve used but I decided not to.</p><p>The new flag ship amonst open source commenting systems comes from a well-funded Mozilla-project named <a href=\"https://coralproject.net/\" target=\"_blank\" rel=\"noopener\">Coral Project</a> in collaboration with the Washington Post and New York Times. They have recently launched their comment software <a href=\"https://coralproject.net/products/talk.html\" target=\"_blank\" rel=\"noopener\">Talk</a> which is now <a href=\"https://www.washingtonpost.com/pr/wp/2017/09/06/the-washington-post-launches-talk-commenting-platform/\" target=\"_blank\" rel=\"noopener\">in use on washingtonpost.com</a>.</p><p>But after looking into Talk’s <a href=\"https://github.com/coralproject/talk\" target=\"_blank\" rel=\"noopener\">code base</a> I felt it was just way too complex for what I needed.</p><p>There also a few other open source commenting tools:</p><ul><li><a href=\"https://github.com/discourse/discourse\" target=\"_blank\" rel=\"noopener\">Discourse</a> - looks pretty complex as well, and is written in Ruby on Rails which I refuse to even look at</li><li><a href=\"https://github.com/adtac/commento\" target=\"_blank\" rel=\"noopener\">Commento</a> - looks clean and simple, but it’s written in Go, which is too fancy for me</li><li><a href=\"https://posativ.org/isso/\" target=\"_blank\" rel=\"noopener\">Isso</a> - I only saw this halfway through the coding. Looks actually very similar to Schnack, except it’s written in Python and doesn’t have third-party site authentication (but allows anonymous commenting).</li></ul><p>But I wanted something for Node and haven’t found it so far. Let me know in the comments below if I missed another project!</p><h2 id=\"whats-up-with-that-weird-name\"><a class=\"anchor\" href=\"#whats-up-with-that-weird-name\"><span class=\"header-anchor\">#</span></a> What’s up with that weird name?</h2><p>For non-Germans out there: It’s called <em>Schnack</em>, not <em>snack</em>, and it has nothing to do with <a href=\"http://www.urbandictionary.com/define.php?term=schnack\" target=\"_blank\" rel=\"noopener\">receiving sexual favors</a>.</p><p><a href=\"https://dict.leo.org/englisch-deutsch/schnack\" target=\"_blank\" rel=\"noopener\">Schnack</a> is simply a nice northern-German word for chatter that wasn’t already taken by other projects.</p><h2 id=\"can-i-try-schnack-now\"><a class=\"anchor\" href=\"#can-i-try-schnack-now\"><span class=\"header-anchor\">#</span></a> Can I try Schnack now?</h2><p>Yes, just leave a comment under this blog post!</p><h2 id=\"standing-on-the-shoulders-of-giants\"><a class=\"anchor\" href=\"#standing-on-the-shoulders-of-giants\"><span class=\"header-anchor\">#</span></a> Standing on the shoulders of giants</h2><p>It wouldn’t be possible to write an app like this in a few days without the amazing libraries it was built upon:</p><ul><li>Thanks to <a href=\"https://github.com/Rich-Harris\" target=\"_blank\" rel=\"noopener\">Rich Harris</a> for <a href=\"https://rollupjs.org/\" target=\"_blank\" rel=\"noopener\">Rollup</a> and <a href=\"https://buble.surge.sh/guide/\" target=\"_blank\" rel=\"noopener\">Bublé</a>, which we’re using to compile the super tiny Schnack comment renderer</li><li>Thanks to <a href=\"https://github.com/developit\" target=\"_blank\" rel=\"noopener\">Jason Miller</a> for <a href=\"https://github.com/developit/unfetch\" target=\"_blank\" rel=\"noopener\">unfetch</a> - the “500 bytes” fetch polyfil we’re using for embed-Schnack communication</li><li>Thanks to <a href=\"https://twitter.com/jaredhanson\" target=\"_blank\" rel=\"noopener\">Jared Hanson</a> for making <a href=\"http://passportjs.org/\" target=\"_blank\" rel=\"noopener\">PassportJS</a>, which we’re using to deal with Twitter and Facebook authentication</li><li>Also thanks to all the other open source contributors in the Node community. It’s been a lot of fun hacking Schnack.js together, mainly because of you!</li></ul><h2 id=\"its-2017-are-people-still-leaving-comments-under-blog-posts\"><a class=\"anchor\" href=\"#its-2017-are-people-still-leaving-comments-under-blog-posts\"><span class=\"header-anchor\">#</span></a> It’s 2017, are people still leaving comments under blog posts?</h2><p>Well, you tell me!</p><p>But reading through some of my older blog posts reminded my that a good blog is never just a one-way broadcast. I couldn’t bring it over myself to dump all the comments, and I definitly don’t want to miss out on future discussions that may arise.</p><p><a href=\"https://github.com/gka/schnack\" class=\"github-fork-ribbon\" style=\"position:fixed;padding:0px 35px;width:128px;top:40px;left:-50px;-webkit-transform:rotate(315deg);-moz-transform:rotate(315deg);-ms-transform:rotate(315deg);transform:rotate(315deg);box-shadow:0 0 0 3px #cc3300, 0 0 20px -3px rgba(0, 0, 0, 0.5);text-shadow:0 0 0 #ffffff, 0 0 5px rgba(0, 0, 0, 0.3);background-color:#cc3300;color:#ffffff;font-size:12px;text-decoration:none;font-weight:bold;border:2px dotted rgba(255,255,255,0.5);-webkit-backface-visibility:hidden;letter-spacing:.5px;background:#c30;box-sizing: content-box; text-align: center;\" target=\"_blank\" rel=\"noopener\">Fork on GitHub</a></p><style type=\"text/css\">@media (max-width:760px) {.github-fork-ribbon { display: none }}</style><hr class=\"footnotes-sep\"><section class=\"footnotes\"><ol class=\"footnotes-list\"><li id=\"fn1\" class=\"footnote-item\"><p>Other authentication provider like Facebook or Google can be added easily <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p></li><li id=\"fn2\" class=\"footnote-item\"><p>We could've also used an iframe but I kind of didn't want to <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p></li><li id=\"fn3\" class=\"footnote-item\"><p>Back when I made my first websites, almost all websites used to be static, hand-written HTML files, and you would write or download some CGI or PHP script to add a &quot;guest book&quot; to your page. This problem is basically how I learned programming. <a href=\"#fnref3\" class=\"footnote-backref\">↩︎</a></p></li></ol></section>",
    "contentSnippet": "tl;dr: Schnack.js is an awesome, ad-free, open source Disqus clone that comes with a very minimal, hackable code base.\n# How to embed Schnack.js\nIt works pretty much like other drop-in commenting systems. You drop a script tag under your blog posts, pass a few parameters and Schnack is doing the rest for you.\n\n\n1\n2\n3\n4\n5\n\n<div class=\"comments-go-here\"></div>\n<script type=\"text/javascript\"\n   src=\"https://schnack.youdomain.com/embed.js\"\n   data-schnack-target=\".comments-go-here\"\n   data-schnack-slug=\"my-blogpost-slug\"></script>\n\n\nThanks to the wonders of modern science the embed.js comes at 2KB gzipped and minified. Your site has full control of the styling of the comments.\n# How Schnack is protecting against spam\nSchnack asks comment authors to sign in via Twitter or Github[1]. New comments need to be approved by the site owner before they show up. To save you some work you can also “trust” comment authors, in which case their comments are being approved automatically.\n# How to install Schnack on your server\nSchnack is a Node app and uses a SQLite database to store the comments. So all you really need to do is to clone the repo, run npm install and start the app.\nTo hook up the authentication you need to go to apps.twitter.com, create a new app and copy the api keys over to your config file.\nSchnack needs to run on a subdomain of the domain your blog is running on, mainly because that’s what cookie authentication is limited to[2].\n# Why did  we decide to work on Schnack?\nOk, here’s a little backstory:\nLast week I replaced my old Wordpress blog with a static Hexo website. Hexo gave me everything I wanted, except for one thing: I missed the comment box below my posts.\nSo I scouted out my options. Disqus was my first consideration, but I was put off by the ads that come with the free “Basic” plan, aside from privacy considerations.\nThen I thought there must be an app for this, but when I didn’t find one that fit my needs (see next section) I decided it’s time to do some hacking[3]!\nThe next step was to put up a readme on Github, just to keep notes of what this app might look like and how it should be organized. My curious friends over at Webkid found the readme while Github-lurking on my repositories and that’s how we got started!\n# Prior art\nOf course, Schnack is not the first of its kind. There are several open source alternatives that I could’ve used but I decided not to.\nThe new flag ship amonst open source commenting systems comes from a well-funded Mozilla-project named Coral Project in collaboration with the Washington Post and New York Times. They have recently launched their comment software Talk which is now in use on washingtonpost.com.\nBut after looking into Talk’s code base I felt it was just way too complex for what I needed.\nThere also a few other open source commenting tools:\n\nDiscourse - looks pretty complex as well, and is written in Ruby on Rails which I refuse to even look at\nCommento - looks clean and simple, but it’s written in Go, which is too fancy for me\nIsso - I only saw this halfway through the coding. Looks actually very similar to Schnack, except it’s written in Python and doesn’t have third-party site authentication (but allows anonymous commenting).\n\nBut I wanted something for Node and haven’t found it so far. Let me know in the comments below if I missed another project!\n# What’s up with that weird name?\nFor non-Germans out there: It’s called Schnack, not snack, and it has nothing to do with receiving sexual favors.\nSchnack is simply a nice northern-German word for chatter that wasn’t already taken by other projects.\n# Can I try Schnack now?\nYes, just leave a comment under this blog post!\n# Standing on the shoulders of giants\nIt wouldn’t be possible to write an app like this in a few days without the amazing libraries it was built upon:\n\nThanks to Rich Harris for Rollup and Bublé, which we’re using to compile the super tiny Schnack comment renderer\nThanks to Jason Miller for unfetch - the “500 bytes” fetch polyfil we’re using for embed-Schnack communication\nThanks to Jared Hanson for making PassportJS, which we’re using to deal with Twitter and Facebook authentication\nAlso thanks to all the other open source contributors in the Node community. It’s been a lot of fun hacking Schnack.js together, mainly because of you!\n\n# It’s 2017, are people still leaving comments under blog posts?\nWell, you tell me!\nBut reading through some of my older blog posts reminded my that a good blog is never just a one-way broadcast. I couldn’t bring it over myself to dump all the comments, and I definitly don’t want to miss out on future discussions that may arise.\nFork on GitHub\n@media (max-width:760px) {.github-fork-ribbon { display: none }}\n\n\nOther authentication provider like Facebook or Google can be added easily ↩︎\n\nWe could've also used an iframe but I kind of didn't want to ↩︎\n\nBack when I made my first websites, almost all websites used to be static, hand-written HTML files, and you would write or download some CGI or PHP script to add a \"guest book\" to your page. This problem is basically how I learned programming. ↩︎"
  },
  {
    "title": "Moving on... to Datawrapper",
    "link": "https://vis4.net/blog/2017/10/moving-on/",
    "pubDate": "2017-10-23T23:32:07.000Z",
    "isoDate": "2017-10-23T23:32:07.000Z",
    "content": "<p>Some personal news.</p><p>Almost four years ago, in January 2014, I <a href=\"https://nytimes.com/by/gregor-aisch\" target=\"_blank\" rel=\"noopener\">joined</a> the New York Times Graphics department. Now I decided that it’s time to move on.</p><p>It’s been an incredible experience and honor to work with some of the smartest journalists, cartographers, and data visualizers in the world. Who, by the way, are also the nicest folks you can imagine, and who went our of their way to make the German immigrant feel at home. I’ll miss you guys!</p><p><a href=\"https://twitter.com/driven_by_data/status/746392404660490240\" target=\"_blank\" rel=\"noopener\">Last summer</a> we had already moved back to Germany for family reasons, and for the last 15 months I enjoyed the benefits and challenges of working remotely for a team on a different continent. But in the end, this arrangement just didn’t work out for me.</p><h2 id=\"returning-to-my-old-love-datawrapper\"><a class=\"anchor\" href=\"#returning-to-my-old-love-datawrapper\"><span class=\"header-anchor\">#</span></a> Returning to my old love Datawrapper</h2><p>So, starting next month I am returning to work on <a href=\"https://www.datawrapper.de\" target=\"_blank\" rel=\"noopener\">Datawrapper</a>, the project I helped <a href=\"https://blog.datawrapper.de/hello-world-hello-datawrapper-1-0-37810cac1a89\" target=\"_blank\" rel=\"noopener\">bring to life in 2012</a> and also the one I cold-heartedly abondened for the exciting job in New York.</p><p>During my absence, a lot has happened to Datawrapper! What started as a quickly hacked together proof-of-concept with a few but enthusiastic users has since turned into a real business, with customers and some revenue. Datawrapper is serving around 2-3 million chart views per day, and it is used by data journalists in many many countries around the world.</p><p><img src=\"/blog/images/2017/datawrapper-usage.png\" alt=\"\"></p><p>We just opened a small office in Berlin to accommodate our team, and in some ways I look forward to exchange a busy newsroom of a thousand for a startup of six.</p><p>So while this means I am leaving journalism (for now), I will still be working on tools to help newsrooms on their way into the future (something I <a href=\"/blog/2015/03/seven-features-youll-wantin-your-next-charting-tool/\">enjoyed doing</a> at the NYT as well). My role at Datawrapper will be some combination of CTO, product development and lead programmer (small company), and I am very excited about our roadmap!</p><p>And, while we’re here, I also hope to find more time for this blog and some <a href=\"https://github.com/gka/schnack\" target=\"_blank\" rel=\"noopener\">fun</a> <a href=\"http://gka.github.io/chroma.js/\" target=\"_blank\" rel=\"noopener\">side</a> <a href=\"https://github.com/gka/canvid\" target=\"_blank\" rel=\"noopener\">projects</a>.</p><p>I kind of missed it!</p>",
    "contentSnippet": "Some personal news.\nAlmost four years ago, in January 2014, I joined the New York Times Graphics department. Now I decided that it’s time to move on.\nIt’s been an incredible experience and honor to work with some of the smartest journalists, cartographers, and data visualizers in the world. Who, by the way, are also the nicest folks you can imagine, and who went our of their way to make the German immigrant feel at home. I’ll miss you guys!\nLast summer we had already moved back to Germany for family reasons, and for the last 15 months I enjoyed the benefits and challenges of working remotely for a team on a different continent. But in the end, this arrangement just didn’t work out for me.\n# Returning to my old love Datawrapper\nSo, starting next month I am returning to work on Datawrapper, the project I helped bring to life in 2012 and also the one I cold-heartedly abondened for the exciting job in New York.\nDuring my absence, a lot has happened to Datawrapper! What started as a quickly hacked together proof-of-concept with a few but enthusiastic users has since turned into a real business, with customers and some revenue. Datawrapper is serving around 2-3 million chart views per day, and it is used by data journalists in many many countries around the world.\n\nWe just opened a small office in Berlin to accommodate our team, and in some ways I look forward to exchange a busy newsroom of a thousand for a startup of six.\nSo while this means I am leaving journalism (for now), I will still be working on tools to help newsrooms on their way into the future (something I enjoyed doing at the NYT as well). My role at Datawrapper will be some combination of CTO, product development and lead programmer (small company), and I am very excited about our roadmap!\nAnd, while we’re here, I also hope to find more time for this blog and some fun side projects.\nI kind of missed it!"
  },
  {
    "title": "I wrote some code that automatically checks visualizations for non-colorblind safe colors. Here's how it works\n",
    "link": "https://vis4.net/blog/2018/02/automate-colorblind-checking/",
    "pubDate": "2018-02-08T14:09:30.000Z",
    "isoDate": "2018-02-08T14:09:30.000Z",
    "content": "<p>Earlier this week we released a new feature at Datawrapper that <a href=\"https://blog.datawrapper.de/colorblind-check/\" target=\"_blank\" rel=\"noopener\"><strong>checks colors</strong> used in charts or maps <strong>for problems with colorblind readers</strong></a>. If problematic colors are found, a warning sign is shown that leads to a colorblind simulation view that helps our users find better colors.</p><p>This blog post explains how the code works. Perhaps this can open a discussion for future improvement. I also hope that other tools will follow Darawrapper on this path towards more automated accessibility testing.</p><p>To see how the algorithm work we’ll look at an example map showing population growth in the US between 2000 and 2010, based on <a href=\"https://www.texastribune.org/2011/03/25/maps-visualize-us-population-growth-by-county/\" target=\"_blank\" rel=\"noopener\">this map made by Matt Stiles</a> in 2011:</p><div class=\"us-map poster poster-945\" style=\"position: relative;\"></div><input type=\"color\" class=\"map-gradient\" value=\"#E52000\"><input type=\"color\" class=\"map-gradient\" value=\"#FFF2C3\"><input type=\"color\" class=\"map-gradient\" value=\"#eeeeee\"><input type=\"color\" class=\"map-gradient\" value=\"#DEE6D0\"><input type=\"color\" class=\"map-gradient\" value=\"#03934C\"><script type=\"text/javascript\">function makeMap() {    var base = \"/blog/interactives/usmap/\";    d3.loadData(base+'us-counties-10m.json', base+\"popchange.csv\", function(error, res) {        var sel = d3.select('.us-map').html(''),            pad = 10,            margin = {top:0,bottom:0,left:pad,right:pad},            totalWidth = sel.node().clientWidth,            totalHeight = totalWidth * 0.6;        var c = d3.conventions({sel, margin, totalWidth, totalHeight}),            [svg] = c.layers,            proj = d3.geoIdentity(),            path = d3.geoPath().projection(proj);        // read colors from inputs        var colors = [];        d3.selectAll('.map-gradient').each(function() {            colors.push(d3.select(this).prop('value'));        });        var scale = chroma            .scale(colors)            .domain([-50,-1,0,1,60]).out('hex');        var colors = d3.set();        // load data        if (error) throw error;        var [us, popchange] = res;        var popchange = d3.nest()            .key(d3.f('FIPS'))            .rollup(d3.f(0))            .object(popchange);        var nation = topojson.feature(us, us.objects.nation),            counties = topojson.feature(us, us.objects.counties).features;        proj.fitSize([totalWidth-pad*2, totalHeight], nation);        svg.append('g.counties')            .appendMany('path', counties)            .attr('d', path)            .style('fill', function(cty) {                var d = popchange[cty.id] || {},                    col = scale(+d.popchange||0);                colors.add(col);                return col;            });        svg.append(\"path\")            .st({fill:'none', stroke:'white'})            .attr(\"d\", path(topojson.mesh(us, us.objects.states, function(a, b) { return a !== b; })));        svg.append(\"path\")            .st({fill:'none',stroke:'black'})            .attr(\"d\", path(nation));        // run the rest of the demos        updateColors(colors);    });}function deltaE(col1, col2) {    return 0.5*(chroma.deltaE(col1,col2)+chroma.deltaE(col2,col1));}d3.selectAll('.map-gradient').on('change', makeMap);</script><p>So here’s what we’re going to do</p><ol><li>We need to extract a representative color sample</li><li>Then look for color-pairs that are distinguishable under normal vision and</li><li>check if any of them turn indistinguishable in a color blindness simulated view</li><li>Finally we need to decide whether or not to show a warning.</li></ol><h2 id=\"extracting-a-representative-color-sample\"><a class=\"anchor\" href=\"#extracting-a-representative-color-sample\"><span class=\"header-anchor\">#</span></a> Extracting a representative color sample</h2><p>Now, to check if this map is colorblind-friendly or not, we need to look at the colors used in it. In some graphics there might be just a handful of colors, but since this is a continuous US county choropleth map, we have a lot of them (<strong><span class=\"num-colors\">TK</span></strong>, to be exact).</p><div class=\"all-colors colors\"></div><p>Since the next step will involve checking each combination of these colors, it would result in a lot of permutations (<strong><span class=\"num-pairs\">TK</span></strong>, to be exact). So in order to save some work we need to reduce the number of colors.</p><p>My first approach was to use <strong>random sampling</strong>. To illustrate this, let’s look at 20 random colors from the palette above:</p><div class=\"random-colors colors large\"></div><p>This might look like a nice sample at first glance, but due to the is random sampling we might end up with more colors from either spectrum of the palette. Click the button below to try a few more samples.</p><p><button class=\"resample-random\">Repeat random sampling</button></p><p>To get around this problem I tried to find a more deterministic sampling method that gives us a representative collection of colors. Since color blindness affects hue perception I thought it made sense to get a good sample of the entire hue-spectrum. So I sorted all colors by hue…</p><div class=\"all-colors-sorted colors\"></div><p>…and then pick 20 evenly spaced colors along this spectrum. Like taking a sample every 5th percentile. To deal with interpolation (in case a percentile falls exactly between two colors) I used the <a href=\"https://gka.github.io/chroma.js/#scale-colors\" target=\"_blank\" rel=\"noopener\">scale.colors()</a> method in chroma.js.</p><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> sample = chroma.scale(sorted_colors).colors(<span class=\"number\">20</span>);</span><br></pre></td></tr></table></figure><p>And this is what the sample looks like:</p><div class=\"hue-sample colors large\"></div><script type=\"text/javascript\">function updateColors(colors) {    d3.select('.all-colors').html('')        .appendMany('div.color', colors.values())        .st('background', d3.f());    var numColors = colors.size();    // update color counts    d3.select('.num-colors').text(numColors);    d3.select('.num-pairs').text(d3.format(',')((numColors * (numColors-1))/2));    // update random sample    randomSample();    hueSample();    d3.select('.resample-random').on('click', randomSample);    // random sampling    function randomSample() {        var sample = _.sample(colors.values(), 20);;        d3.select('.random-colors').html('')            .appendMany('div.color', sample)            .st('background', d3.f());    }    // hue sorted percentiles    function hueSample() {        var sorted = colors.values().sort(d3.ascendingKey(function(col) {            return chroma(col).get('lch.h') || 1000;        }));        // show all sorted colors        d3.select('.all-colors-sorted').html('')            .appendMany('div.color', sorted)            .attr('title', function(c) { return chroma(c).get('lch.h'); })            .st('background', d3.f());        // show sample        var sample = chroma.scale(sorted).colors(20);        d3.select('.hue-sample').html('')            .appendMany('div.color', sample)            .st('background', d3.f())            .attr('title', function(c) { return chroma(c).get('lch.h'); });        // compute distance matrix        colorDistanceMatrix('.cdm-normal', sample);        cbsimTable(sample);    }}</script><p>We could experiment with other sampling methods, but for now these look good enough, so let’s move on. Next we want to find out which of the resulting <strong>190</strong> color pairs are actually distinguishable from another. To do that we need to find a method to compute differences between colors.</p><h2 id=\"how-to-compute-color-differences\"><a class=\"anchor\" href=\"#how-to-compute-color-differences\"><span class=\"header-anchor\">#</span></a> How to compute color differences</h2><p>There are a couple of ways to do this. For instance, a color can be represented as three-dimensional coordinates in a <span style=\"color:red\">R</span>-<span style=\"color:#0c0\">G</span>-<span style=\"color:blue\">B</span> color space, so the color difference can be defined as the Euclidean distance between the two points.</p><p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>d</mi><mi>i</mi><mi>s</mi><msub><mi>t</mi><mrow><mi>R</mi><mi>G</mi><mi>B</mi></mrow></msub><mo>=</mo><msqrt><mrow><mo>(</mo><msub><mi>R</mi><mn>2</mn></msub><mo>−</mo><msub><mi>R</mi><mn>1</mn></msub><msup><mo>)</mo><mrow><mn>2</mn></mrow></msup><mo>+</mo><mo>(</mo><msub><mi>G</mi><mn>2</mn></msub><mo>−</mo><msub><mi>G</mi><mn>1</mn></msub><msup><mo>)</mo><mrow><mn>2</mn></mrow></msup><mo>+</mo><mo>(</mo><msub><mi>B</mi><mn>2</mn></msub><mo>−</mo><msub><mi>B</mi><mn>1</mn></msub><msup><mo>)</mo><mrow><mn>2</mn></mrow></msup></mrow></msqrt></mrow><annotation encoding=\"application/x-tex\">dist_{RGB}=\\sqrt{(R_2-R_1)^{2} + (G_2-G_1)^{2} + (B_2-B_1)^{2}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.9838800000000001em;\"></span><span class=\"strut bottom\" style=\"height:1.24001em;vertical-align:-0.2561299999999999em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\">d</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">s</span><span class=\"mord\"><span class=\"mord mathit\">t</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathit\">G</span><span class=\"mord mathit\" style=\"margin-right:0.05017em;\">B</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mrel\">=</span><span class=\"sqrt mord\"><span class=\"sqrt-sign\" style=\"top:-0.09388000000000007em;\"><span class=\"style-wrap reset-textstyle textstyle uncramped\"><span class=\"delimsizing size1\">√</span></span></span><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:1em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.00773em;\">R</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.00773em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">−</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.00773em;\">R</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.00773em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">1</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">+</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\">G</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">−</span><span class=\"mord\"><span class=\"mord mathit\">G</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">1</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">+</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.05017em;\">B</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.05017em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">−</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.05017em;\">B</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.05017em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">1</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span><span style=\"top:-0.90388em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:1em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped sqrt-line\"></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:1em;\">​</span></span>​</span></span></span></span></span></span></span></p><p>However, after playing around with RGB distances a bit I noticed a problem: Look at the yellow/green and pink/blue combinations below. From eyeballing the colors I had expected the distance between yellow and green to be smaller than the pink/blue combination.</p><div class=\"colors xlarge\">    <div class=\"color\" style=\"background: #ff0\"></div><div class=\"color\" style=\"background: #0f0\"></div></div> vs <div class=\"colors xlarge\">    <div class=\"color\" style=\"background: #f0f\"></div><div class=\"color\" style=\"background: #00f\"></div></div><p>But it turns out, in RGB the distance is exactly 255 in both cases. This makes sense mathematically, since it takes 255 “steps” from <span style=\"border-bottom:2px solid rgb(255,255,0)\">(255,255,0)</span> to <span style=\"border-bottom:2px solid rgb(0,255,0)\">(0,255,0)</span> as well as from <span style=\"border-bottom:2px solid rgb(255,0,255)\">(255,0,255)</span> to <span style=\"border-bottom:2px solid rgb(0,0,255)\">(0,0,255)</span>. But it doesn’t make sense visually.</p><p>Of course, the same Euclidean distances can be computed in any other color space, so perhaps a perceptual color space like <code>Lab</code> or <code>Lch</code> makes more sense. Sadly, that’s not the case.</p><table style=\"max-width: 350px\">    <tr><th></th><th>RGB</th><th>Lch</th><th>Lab</th><th>deltaE</th></tr>    <tr>        <td><div class=\"colors xlarge\">        <div class=\"color\" style=\"background: #ff0\"></div><div class=\"color\" style=\"background: #0f0\"></div></div></td>        <td>255</td><td>41.4</td><td>66.3</td><td>26.9</td>    </tr>    <tr>        <td><div class=\"colors xlarge\">        <div class=\"color\" style=\"background: #f0f\"></div><div class=\"color\" style=\"background: #00f\"></div></div></td>        <td>255</td><td>40.0</td><td>58.0</td><td>34.7</td>    </tr></table><p>In Lch, like RGB, both color pairs are almost equally distant. In Lab, the distance between yellow and green is even larger (66.3) than pink/blue (58.0).</p><p>So I ended up using a more fancy algorithm called <a href=\"https://en.wikipedia.org/wiki/Color_difference#CMC_l:c_.281984.29\" target=\"_blank\" rel=\"noopener\"><strong>deltaE</strong> or CMC l:c</a>. It’s based on the Lch color model, but appears to work better.</p><p>One minor problem with deltaE is that it’s not symmetrical, meaning that the difference between yellow and green differs slightly from the difference between green and yellow. To get around this problem I am using the mean of the distances in both direction:</p><p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo>(</mo><msub><mi>c</mi><mrow><mn>1</mn></mrow></msub><mo separator=\"true\">,</mo><msub><mi>c</mi><mrow><mn>2</mn></mrow></msub><mo>)</mo><mo>=</mo><mfrac><mrow><mi>d</mi><mi>e</mi><mi>l</mi><mi>t</mi><mi>a</mi><mi>E</mi><mo>(</mo><msub><mi>c</mi><mrow><mn>1</mn></mrow></msub><mo separator=\"true\">,</mo><msub><mi>c</mi><mrow><mn>2</mn></mrow></msub><mo>)</mo><mo>+</mo><mi>d</mi><mi>e</mi><mi>l</mi><mi>t</mi><mi>a</mi><mi>E</mi><mo>(</mo><msub><mi>c</mi><mrow><mn>2</mn></mrow></msub><mo separator=\"true\">,</mo><msub><mi>c</mi><mrow><mn>1</mn></mrow></msub><mo>)</mo></mrow><mrow><mn>2</mn></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">dist(c_{1},c_{2})=\\frac{deltaE(c_{1}, c_{2}) + deltaE(c_{2}, c_{1})}{2}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.427em;\"></span><span class=\"strut bottom\" style=\"height:2.113em;vertical-align:-0.686em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\">d</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">s</span><span class=\"mord mathit\">t</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\">c</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mpunct\">,</span><span class=\"mord\"><span class=\"mord mathit\">c</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\">2</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathit\">d</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\">c</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mpunct\">,</span><span class=\"mord\"><span class=\"mord mathit\">c</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">)</span><span class=\"mbin\">+</span><span class=\"mord mathit\">d</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\">c</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mpunct\">,</span><span class=\"mord\"><span class=\"mord mathit\">c</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">)</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p><p>Now let’s move from the difference between two colors to the difference between <em>all</em> the colors!</p><h2 id=\"compute-the-color-distance-matrix\"><a class=\"anchor\" href=\"#compute-the-color-distance-matrix\"><span class=\"header-anchor\">#</span></a> Compute the color distance matrix</h2><p>To do that we compute the color distance matrix. Which is a fancy word for a table with rows and columns for each of our sample colors, and the distance between each color combination shown in the table cells.</p><table class=\"color-distance-matrix cdm-normal\"></table><script type=\"text/javascript\">function colorDistanceMatrix(cont, sample, baseline) {    var hm = chroma.scale(['#eee','#ddd', '#666']).domain(baseline ? [1,3,10] :[0,10,70]).out('hex');    var table = d3.select(cont).html('');    var tr = table.append('tr');    if (baseline) {        tr.append('td');        tr.append('td');        tr.appendMany('th.color', baseline).st('background', d3.f());        tr = table.append('tr');        tr.append('td');    }    tr.append('td');    tr.appendMany('th.color', sample).st('background', d3.f());    tr = table.appendMany('tr', sample);    if (baseline) {        tr.append('th.color').st('background', (d,i) => baseline[i]);    }    tr.append('th.color').st('background', d3.f());    tr.appendMany('td', function(a, ia) {            return sample.map(function(b, ib) {                var diff = deltaE(a,b);                return {                    a, ia, b, ib,                    diff: baseline ?                        // difference ratio                        (deltaE(baseline[ia],baseline[ib])/diff).toFixed(1)                        : diff.toFixed(0)                };            });        })        .style('background-color', d3.f('diff', hm))        .text(d3.f('diff'))        .classed('relevant', (d) => d.ib > d.ia && d.diff >= (baseline ? 2 : 8))        .classed('strong', (d) => d.diff >= (baseline ? 5 : 40))        .classed('lower', (d) => d.ib <= d.ia);}</script><p>We only need to look at the upper half of the matrix – the lower half is just an exact mirror. I highlighted all values above 45 so the combinations with the largest color differences pop out a bit more.</p><p>Now we need to compute the same matrix with the colorblind-simulated versions of our colors.</p><h2 id=\"simulating-color-blindness\"><a class=\"anchor\" href=\"#simulating-color-blindness\"><span class=\"header-anchor\">#</span></a> Simulating color blindness</h2><p>Color blindness simulation is done by mapping colors from RGB to a reduced color space. It means that you have a function that gets one color as input and returns a new color.</p><p>After googling around a bit I settled on a NPM package <a href=\"https://www.npmjs.com/package/color-blind\" target=\"_blank\" rel=\"noopener\">color-blind</a>, which provides a fairly simple API:</p><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> blinder = <span class=\"built_in\">require</span>(<span class=\"string\">'color-blind'</span>);</span><br><span class=\"line\">blinder.protanopia(<span class=\"string\">'#42dead'</span>); <span class=\"comment\">// result: \"#d1c4a0\"</span></span><br></pre></td></tr></table></figure><p>Here is our color sample mapped through three kinds of color blindness:</p><table class=\"simulated\"></table><script type=\"text/javascript\">function cbsimTable(sample) {    var table = d3.select('.simulated').html('');    var tr = table.appendMany('tr', ['normal','deuteranopia','protanopia','tritanopia']);    tr.append('td').append('b').text(d3.f());    var div = tr.append('td').append('div.colors.large');    div.appendMany('div.color', function(t) {            return t == 'normal' ? sample :                sample.map(function(c) { return blinder[t](c); })        })        .style('background', d3.f());    simColorMatrix(sample);    simColorRatioMatrix(sample);    summaryTable(sample);}</script><p>Now we can just repeat the color difference matrix for the converted sample colors:</p><table class=\"color-distance-matrix cdm-sim\"></table><p>You can click through the buttons below to see the color difference matrix for each color blindness simulation:</p><div class=\"btn-group select-simulation\">    <button>normal</button>    <button class=\"selected\">deuteranopia</button>    <button>protanopia</button>    <button>tritanopia</button></div><script type=\"text/javascript\">function simColorMatrix(sample) {    var btns = d3.selectAll('.select-simulation button'),        typ = d3.select('.select-simulation button.selected').text();    btns.on('click.vis4', function() {        btns.classed('selected', false);        d3.select(this).classed('selected', true);        simColorMatrix(sample);    });    var colors = typ == 'normal' ? sample :        sample.map(function(c) { return blinder[typ](c); });    colorDistanceMatrix('.cdm-sim', colors);}</script><h2 id=\"computing-difference-ratios-between-normal-and-colorblind-vision\"><a class=\"anchor\" href=\"#computing-difference-ratios-between-normal-and-colorblind-vision\"><span class=\"header-anchor\">#</span></a> Computing difference ratios between normal and colorblind vision</h2><p>We’re getting closer to the finish line! The only thing that’s left to do now is to <strong>compare the differences</strong> under normal vision with the differences under a color blindness simulation.</p><p>One way to do this is to look at the ratio between the two differences:</p><p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>r</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mo>(</mo><msub><mi>c</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>c</mi><mn>2</mn></msub><mo>)</mo><mo>=</mo><mfrac><mrow><mi>d</mi><mi>i</mi><mi>s</mi><msub><mi>t</mi><mrow><mi>n</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>a</mi><mi>l</mi></mrow></msub><mo>(</mo><msub><mi>c</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>c</mi><mn>2</mn></msub><mo>)</mo></mrow><mrow><mi>d</mi><mi>i</mi><mi>s</mi><msub><mi>t</mi><mrow><mi>c</mi><mi>o</mi><mi>l</mi><mi>o</mi><mi>r</mi><mi>b</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>d</mi></mrow></msub><mo>(</mo><msub><mi>c</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>c</mi><mn>2</mn></msub><mo>)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">ratio(c_1,c_2)=\\frac{dist_{normal}(c_1,c_2)}{dist_{colorblind}(c_1,c_2)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.427em;\"></span><span class=\"strut bottom\" style=\"height:2.363em;vertical-align:-0.936em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">o</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\">c</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">1</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mpunct\">,</span><span class=\"mord\"><span class=\"mord mathit\">c</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathit\">d</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">s</span><span class=\"mord\"><span class=\"mord mathit\">t</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">c</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">b</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">d</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\">c</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">1</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mpunct\">,</span><span class=\"mord\"><span class=\"mord mathit\">c</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">)</span></span></span></span><span style=\"top:-0.2300000000000001em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathit\">d</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">s</span><span class=\"mord\"><span class=\"mord mathit\">t</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">n</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\">c</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">1</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mpunct\">,</span><span class=\"mord\"><span class=\"mord mathit\">c</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">)</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p><p>To illustrate the ratio, let’s look at the notorious <span style=\"border-bottom:2px solid #54ae49\">green</span>/<span style=\"border-bottom:2px solid #f06c62\">red</span> color pair and compute the differences between them after applying different colorblindness simulations.</p><table class=\"color-ratios\"></table><script type=\"text/javascript\">function colorRatios(col1, col2) {    var table = d3.select('.color-ratios').html('');    var modes = ['normal','deuteranopia','protanopia','tritanopia'];    table.append('tr').appendMany('th', [''].concat(modes)).text(d3.f());    var dist_norm = deltaE(col1,col2);    var columns = modes.map(function(mode) {        var c1 = mode == 'normal' ? col1 : blinder[mode](col1);        var c2 = mode == 'normal' ? col2 : blinder[mode](col2);        var dist = deltaE(c1,c2);        return {            mode: mode,            colors: [c1,c2],            distance: dist,            ratio: dist_norm.toFixed()+'/'+dist.toFixed()+' = <b>'+ (dist_norm / dist).toFixed(1)+'</b>'        };    });    // colors    var tr = table.append('tr');    tr.append('td').text('Colors');    tr.appendMany('td.color', columns)        .append('div.colors.xlarge')        .appendMany('div.color', (col) => col.colors)        .style('background', d3.f());    // distance    tr = table.append('tr');    tr.append('td').text('Distance');    tr.appendMany('td', columns)        .text((col) => col.distance.toFixed(0));    // ratio    tr = table.append('tr');    tr.append('td').text('Ratio').style('font-weight','bold');    tr.appendMany('td', columns)        .html((col) => col.ratio);}colorRatios('#54ae49', '#f06c62');</script><p>A ratio of 17.3 means that a color pair is <strong>seventeen times more differentiable</strong> under normal vision than under this type of color blindness. The higher the ratio the more information is “lost” for a colorblind person. This is what we’ll use to decide what colors are ok or not.</p><p>Now, to <strong>look at all the ratios at once</strong>, let’s make another matrix. The table looks just like the matrices we’ve seen before, except now the values in the cells show the ratio between the normal distance and the colorblind distance for each color pair.</p><table class=\"color-distance-matrix cdm-sim-ratios\"></table><div class=\"btn-group select-simulation2\">    <button>normal</button>    <button>deuteranopia</button>    <button class=\"selected\">protanopia</button>    <button>tritanopia</button></div><script type=\"text/javascript\">function simColorRatioMatrix(sample) {    var btns = d3.selectAll('.select-simulation2 button'),        typ = d3.select('.select-simulation2 button.selected').text();    btns.on('click.vis4', function() {        btns.classed('selected', false);        d3.select(this).classed('selected', true);        simColorRatioMatrix(sample);    });    var colors = typ == 'normal' ? sample :        sample.map(function(c) { return blinder[typ](c); });    colorDistanceMatrix('.cdm-sim-ratios', colors, sample);}</script><p>You can use the buttons to cycle through the different simulations.</p><p>All that’s left to do now is to decide when to show a warning.</p><h2 id=\"when-to-show-a-color-warning\"><a class=\"anchor\" href=\"#when-to-show-a-color-warning\"><span class=\"header-anchor\">#</span></a> When to show a color warning</h2><p>One criteria to check for I came up with is to check how many of the color pairs that were differentiable under normal vision turn into non-differentiable color pairs under colorblind vision. I also found that difference ratios above five signal a significant loss of information. So I ended up using a mix of both to decide when to trigger warnings.</p><p>As you can see in the table, it all depends highly on where I set the thresholds. You can tweak the sliders below to change them and see the results changing.</p><table class=\"summary\"></table><div class=\"summary-ctrls\">    <label>Difference threshold: <b class=\"diff-thresh\"></b><br><input class=\"diff-input\" type=\"range\" value=\"5\" min=\"4\" max=\"20\" step=\"0.1\"></label>    <label>Ratio threshold: <b class=\"ratio-thresh\"></b><br><input class=\"ratio-input\" type=\"range\" value=\"5\" min=\"1\" max=\"10\" step=\"0.1\"></label>    <label>Warning threshold: <b class=\"warning-thresh\"></b>%<br><input class=\"warning-input\" type=\"range\" value=\"4\" min=\"0.1\" max=\"40\" step=\"0.1\"></label></div><script type=\"text/javascript\">    function summaryTable(colors) {        d3.selectAll('.summary-ctrls input').on('input', function() {            summaryTable(colors);        });        var ratio_thresh = +d3.select('.ratio-input').prop('value');        var diff_thresh = +d3.select('.diff-input').prop('value');        var warning_thresh = +d3.select('.warning-input').prop('value');        console.log('rrr', ratio_thresh, ratio_thresh.toFixed(1));        d3.select('.ratio-thresh').html(ratio_thresh.toFixed(1));        d3.select('.diff-thresh').html(diff_thresh.toFixed(1));        d3.select('.warning-thresh').html(warning_thresh.toFixed(1));        var table = d3.select('.summary').html('');        var modes = ['normal', 'deuteranopia','protanopia','tritanopia'];        table.append('tr').appendMany('th', [''].concat(modes)).text(d3.f());        var num_colors = colors.length,            num_perm = (colors.length*(colors.length-1))/2;        // var dist_ratio = (chroma.deltaE(col1,col2)+chroma.deltaE(col2,col1))*0.5;        var columns = modes.map(function(mode) {            // var c1 = mode == 'normal' ? col1 : blinder[mode](col1);            // var c2 = mode == 'normal' ? col2 : blinder[mode](col2);            // var dist = (chroma.deltaE(c1,c2)+chroma.deltaE(c2,c1))*0.5;            var diff_colors = 0,                cnt_ratio = 0,                cnt_sim_diff = 0,                max_ratio = 0,                sum_ratio = 0;            colors.forEach((col_a, i) => {                colors.forEach((col_b, j) => {                    if (j>i) {                        var dist_norm = deltaE(col_a, col_b);                        if (dist_norm >= diff_thresh) {                            diff_colors++;                            if (mode != 'normal') {                                var sim_a = blinder[mode](col_a),                                    sim_b = blinder[mode](col_b);                                var dist_sim = deltaE(sim_a, sim_b);                                if (dist_sim < diff_thresh) {                                    cnt_sim_diff++;                                }                                var ratio = dist_norm / dist_sim;                                sum_ratio+= ratio;                                if (ratio > max_ratio) max_ratio = ratio;                                if (ratio >= ratio_thresh) cnt_ratio++;                            }                        }                    }                });            });            var ratio_pct = 100*(cnt_ratio/diff_colors);            var diff_pct = 100*(cnt_sim_diff/diff_colors);            return {                mode: mode,                colors: mode == 'normal' ? num_colors : '-',                permutations: mode == 'normal' ? num_perm : '-',                differentiable: mode == 'normal' ? diff_colors : '-',                sim_not_diff: mode == 'normal' ? '' : cnt_sim_diff + ' ('+diff_pct.toFixed(1)+'%)',                ratio: mode == 'normal' ? '' : cnt_ratio+' ('+ratio_pct.toFixed(1)+'%)',                ratio_avg: mode == 'normal' ? '' : (sum_ratio / diff_colors).toFixed(1),                ratio_max: mode == 'normal' ? '' : (max_ratio).toFixed(1),                warning: mode== 'normal' ? '' : ratio_pct >= warning_thresh || diff_pct > warning_thresh ? '<b>WARNING</b>' : 'ok'                // colors: [c1,c2],                // distance: dist,                // ratio: dist_norm / dist            };        });        // number of colors        var tr = table.append('tr');        tr.append('td').text('Colors');        tr.appendMany('td', columns).text(d3.f('colors'));        // number of color permutations        tr = table.append('tr');        tr.append('td').text('Color pairs');        tr.appendMany('td', columns).text(d3.f('permutations'));        // of those, differentiable        tr = table.append('tr');        tr.append('td').html('Differentiable color pairs');        tr.appendMany('td', columns).text(d3.f('differentiable'));        // hr        // tr = table.append('tr');        // tr.append('td').attr('colspan', 4)        //     .st({textAlign:'center', color: '#999'})        //     .text('of those...');        // non differentiable        tr = table.append('tr');        tr.append('td').html('..that turn into non-differentiable<br>pairs under colorblind simulation');        tr.appendMany('td', columns).text(d3.f('sim_not_diff'));        // of those, differentiable        // tr = table.append('tr');        // tr.append('td').text('Avg. ratio');        // tr.appendMany('td', columns).text(d3.f('ratio_avg'));        // // of those, differentiable        // tr = table.append('tr');        // tr.append('td').text('Max. ratio');        // tr.appendMany('td', columns).text(d3.f('ratio_max'));        // of those, differentiable        tr = table.append('tr');        tr.append('td').text('Ratio > '+(ratio_thresh.toFixed(1)));        tr.appendMany('td', columns).text(d3.f('ratio'));        // of those, differentiable        tr = table.append('tr');        tr.append('td').text('Result');        tr.appendMany('td', columns).html(d3.f('warning'));        // of those, differentiable    }</script><p>Feel free to scroll back to the top and try out different colors in the map. All the examples and matrices in this post will change accordingly. Let me know if you find bugs along the way.</p><h2 id=\"some-ideas-for-future-improvements\"><a class=\"anchor\" href=\"#some-ideas-for-future-improvements\"><span class=\"header-anchor\">#</span></a> Some ideas for future improvements</h2><p>This algorithm was hacked together in a few days, so obviously there are possible improvements to be made. Here are a few ideas that might be worth exploring:</p><ul><li>Instead of the hue-percentiles we could try different methods to find the most “representative” colors, such as k-means clustering.</li><li>One could look out for different color difference metrics, ideally by having (non-colorblind) people “guess” color differences and compare these results with the various color difference formulas.</li><li>Maybe there are smarter ways to compare the color differences between normal and colorblind vision than looking at the ratios?</li><li>Maybe there are smarter ways to decide how many “problematic” colors are enough to trigger the color blindness warning.</li><li>Clearly, it would help testing a larger set of charts and maps with this algorithm to measure its real performance</li></ul><p>As always I’m happy to hear what you think. Feel free to drop a comment below or send me an email at <a href=\"mailto:gregor@datawrapper.de\" target=\"_blank\" rel=\"noopener\">gregor@datawrapper.de</a>.</p><script type=\"text/javascript\">    makeMap();</script><style type=\"text/css\">    svg,canvas {top:0;}    button { font-size: 15px; padding: 5px 15px;  }    .colors { line-height: 0;  padding: 2px 0 0 2px; display: inline-block;}    .colors .color { box-sizing: border-box; border: 2px solid white; display: inline-block;        width: 16px; height: 16px; margin: -2px 0 0 -2px; }    .colors.large .color { width: 28px; height: 28px;  }    td .colors.large .color { width: 24px; height: 24px;  }    .colors.xlarge .color { width: 38px; height: 38px;  }    .colors.xlarge .color+.color{ border-left: 0 }    .all-colors-sorted { border-left: 2px solid white }    .all-colors-sorted .color{ border-left: 0 }    .content table.color-distance-matrix { cursor:pointer;overflow: hidden; z-index: 1; border-spacing: 0; border-collapse: separate; border: 1px solid white;}    .color-distance-matrix th,    .color-distance-matrix td { border:1px solid white; padding:0; color: #888;}    .color-distance-matrix th { width:16px; height:21px; }    .color-distance-matrix td { font-size: 11px; text-align: center;vertical-align: middle; position: relative;}    .color-distance-matrix tr:hover { background: #e0e0e0 }    .color-distance-matrix td:hover::after {        background-color: #e0e0e0; content: '\\00a0';height: 10000px;left: 0;position: absolute;        top: -5000px;width: 100%;z-index: -1;    }    .summary-ctrls label { display: inline-block; font-size: 14px; margin-right: 15px }    label input[type=range] { vertical-align: middle; margin: 0;}    .color-distance-matrix td:hover { background: #ccc; }    .color-distance-matrix td.relevant { color: #333; }    .color-distance-matrix td.strong { font-weight: bold; color:#fff;}    .color-distance-matrix td.lower { color: #d7d7d7; background: transparent!important;}</style>",
    "contentSnippet": "Earlier this week we released a new feature at Datawrapper that checks colors used in charts or maps for problems with colorblind readers. If problematic colors are found, a warning sign is shown that leads to a colorblind simulation view that helps our users find better colors.\nThis blog post explains how the code works. Perhaps this can open a discussion for future improvement. I also hope that other tools will follow Darawrapper on this path towards more automated accessibility testing.\nTo see how the algorithm work we’ll look at an example map showing population growth in the US between 2000 and 2010, based on this map made by Matt Stiles in 2011:\n\nfunction makeMap() {    var base = \"/blog/interactives/usmap/\";    d3.loadData(base+'us-counties-10m.json', base+\"popchange.csv\", function(error, res) {        var sel = d3.select('.us-map').html(''),            pad = 10,            margin = {top:0,bottom:0,left:pad,right:pad},            totalWidth = sel.node().clientWidth,            totalHeight = totalWidth * 0.6;        var c = d3.conventions({sel, margin, totalWidth, totalHeight}),            [svg] = c.layers,            proj = d3.geoIdentity(),            path = d3.geoPath().projection(proj);        // read colors from inputs        var colors = [];        d3.selectAll('.map-gradient').each(function() {            colors.push(d3.select(this).prop('value'));        });        var scale = chroma            .scale(colors)            .domain([-50,-1,0,1,60]).out('hex');        var colors = d3.set();        // load data        if (error) throw error;        var [us, popchange] = res;        var popchange = d3.nest()            .key(d3.f('FIPS'))            .rollup(d3.f(0))            .object(popchange);        var nation = topojson.feature(us, us.objects.nation),            counties = topojson.feature(us, us.objects.counties).features;        proj.fitSize([totalWidth-pad*2, totalHeight], nation);        svg.append('g.counties')            .appendMany('path', counties)            .attr('d', path)            .style('fill', function(cty) {                var d = popchange[cty.id] || {},                    col = scale(+d.popchange||0);                colors.add(col);                return col;            });        svg.append(\"path\")            .st({fill:'none', stroke:'white'})            .attr(\"d\", path(topojson.mesh(us, us.objects.states, function(a, b) { return a !== b; })));        svg.append(\"path\")            .st({fill:'none',stroke:'black'})            .attr(\"d\", path(nation));        // run the rest of the demos        updateColors(colors);    });}function deltaE(col1, col2) {    return 0.5*(chroma.deltaE(col1,col2)+chroma.deltaE(col2,col1));}d3.selectAll('.map-gradient').on('change', makeMap);\nSo here’s what we’re going to do\n\nWe need to extract a representative color sample\nThen look for color-pairs that are distinguishable under normal vision and\ncheck if any of them turn indistinguishable in a color blindness simulated view\nFinally we need to decide whether or not to show a warning.\n\n# Extracting a representative color sample\nNow, to check if this map is colorblind-friendly or not, we need to look at the colors used in it. In some graphics there might be just a handful of colors, but since this is a continuous US county choropleth map, we have a lot of them (TK, to be exact).\n\nSince the next step will involve checking each combination of these colors, it would result in a lot of permutations (TK, to be exact). So in order to save some work we need to reduce the number of colors.\nMy first approach was to use random sampling. To illustrate this, let’s look at 20 random colors from the palette above:\n\nThis might look like a nice sample at first glance, but due to the is random sampling we might end up with more colors from either spectrum of the palette. Click the button below to try a few more samples.\nRepeat random sampling\nTo get around this problem I tried to find a more deterministic sampling method that gives us a representative collection of colors. Since color blindness affects hue perception I thought it made sense to get a good sample of the entire hue-spectrum. So I sorted all colors by hue…\n\n…and then pick 20 evenly spaced colors along this spectrum. Like taking a sample every 5th percentile. To deal with interpolation (in case a percentile falls exactly between two colors) I used the scale.colors() method in chroma.js.\n\n\n1\n\nvar sample = chroma.scale(sorted_colors).colors(20);\n\n\nAnd this is what the sample looks like:\n\nfunction updateColors(colors) {    d3.select('.all-colors').html('')        .appendMany('div.color', colors.values())        .st('background', d3.f());    var numColors = colors.size();    // update color counts    d3.select('.num-colors').text(numColors);    d3.select('.num-pairs').text(d3.format(',')((numColors * (numColors-1))/2));    // update random sample    randomSample();    hueSample();    d3.select('.resample-random').on('click', randomSample);    // random sampling    function randomSample() {        var sample = _.sample(colors.values(), 20);;        d3.select('.random-colors').html('')            .appendMany('div.color', sample)            .st('background', d3.f());    }    // hue sorted percentiles    function hueSample() {        var sorted = colors.values().sort(d3.ascendingKey(function(col) {            return chroma(col).get('lch.h') || 1000;        }));        // show all sorted colors        d3.select('.all-colors-sorted').html('')            .appendMany('div.color', sorted)            .attr('title', function(c) { return chroma(c).get('lch.h'); })            .st('background', d3.f());        // show sample        var sample = chroma.scale(sorted).colors(20);        d3.select('.hue-sample').html('')            .appendMany('div.color', sample)            .st('background', d3.f())            .attr('title', function(c) { return chroma(c).get('lch.h'); });        // compute distance matrix        colorDistanceMatrix('.cdm-normal', sample);        cbsimTable(sample);    }}\nWe could experiment with other sampling methods, but for now these look good enough, so let’s move on. Next we want to find out which of the resulting 190 color pairs are actually distinguishable from another. To do that we need to find a method to compute differences between colors.\n# How to compute color differences\nThere are a couple of ways to do this. For instance, a color can be represented as three-dimensional coordinates in a R-G-B color space, so the color difference can be defined as the Euclidean distance between the two points.\ndistRGB=(R2−R1)2+(G2−G1)2+(B2−B1)2dist_{RGB}=\\sqrt{(R_2-R_1)^{2} + (G_2-G_1)^{2} + (B_2-B_1)^{2}}dist​RGB​​=√​(R​2​​−R​1​​)​2​​+(G​2​​−G​1​​)​2​​+(B​2​​−B​1​​)​2​​​​​\nHowever, after playing around with RGB distances a bit I noticed a problem: Look at the yellow/green and pink/blue combinations below. From eyeballing the colors I had expected the distance between yellow and green to be smaller than the pink/blue combination.\n    \n\n\n vs \n    \n\n\nBut it turns out, in RGB the distance is exactly 255 in both cases. This makes sense mathematically, since it takes 255 “steps” from (255,255,0) to (0,255,0) as well as from (255,0,255) to (0,0,255). But it doesn’t make sense visually.\nOf course, the same Euclidean distances can be computed in any other color space, so perhaps a perceptual color space like Lab or Lch makes more sense. Sadly, that’s not the case.\n    \nRGBLchLabdeltaE\n    \n        \n        \n\n\n        25541.466.326.9    \n    \n        \n        \n\n\n        25540.058.034.7    \n\nIn Lch, like RGB, both color pairs are almost equally distant. In Lab, the distance between yellow and green is even larger (66.3) than pink/blue (58.0).\nSo I ended up using a more fancy algorithm called deltaE or CMC l:c. It’s based on the Lch color model, but appears to work better.\nOne minor problem with deltaE is that it’s not symmetrical, meaning that the difference between yellow and green differs slightly from the difference between green and yellow. To get around this problem I am using the mean of the distances in both direction:\ndist(c1,c2)=deltaE(c1,c2)+deltaE(c2,c1)2dist(c_{1},c_{2})=\\frac{deltaE(c_{1}, c_{2}) + deltaE(c_{2}, c_{1})}{2}dist(c​1​​,c​2​​)=​2​​deltaE(c​1​​,c​2​​)+deltaE(c​2​​,c​1​​)​​\nNow let’s move from the difference between two colors to the difference between all the colors!\n# Compute the color distance matrix\nTo do that we compute the color distance matrix. Which is a fancy word for a table with rows and columns for each of our sample colors, and the distance between each color combination shown in the table cells.\n\nfunction colorDistanceMatrix(cont, sample, baseline) {    var hm = chroma.scale(['#eee','#ddd', '#666']).domain(baseline ? [1,3,10] :[0,10,70]).out('hex');    var table = d3.select(cont).html('');    var tr = table.append('tr');    if (baseline) {        tr.append('td');        tr.append('td');        tr.appendMany('th.color', baseline).st('background', d3.f());        tr = table.append('tr');        tr.append('td');    }    tr.append('td');    tr.appendMany('th.color', sample).st('background', d3.f());    tr = table.appendMany('tr', sample);    if (baseline) {        tr.append('th.color').st('background', (d,i) => baseline[i]);    }    tr.append('th.color').st('background', d3.f());    tr.appendMany('td', function(a, ia) {            return sample.map(function(b, ib) {                var diff = deltaE(a,b);                return {                    a, ia, b, ib,                    diff: baseline ?                        // difference ratio                        (deltaE(baseline[ia],baseline[ib])/diff).toFixed(1)                        : diff.toFixed(0)                };            });        })        .style('background-color', d3.f('diff', hm))        .text(d3.f('diff'))        .classed('relevant', (d) => d.ib > d.ia && d.diff >= (baseline ? 2 : 8))        .classed('strong', (d) => d.diff >= (baseline ? 5 : 40))        .classed('lower', (d) => d.ib \nWe only need to look at the upper half of the matrix – the lower half is just an exact mirror. I highlighted all values above 45 so the combinations with the largest color differences pop out a bit more.\nNow we need to compute the same matrix with the colorblind-simulated versions of our colors.\n# Simulating color blindness\nColor blindness simulation is done by mapping colors from RGB to a reduced color space. It means that you have a function that gets one color as input and returns a new color.\nAfter googling around a bit I settled on a NPM package color-blind, which provides a fairly simple API:\n\n\n1\n2\n\nvar blinder = require('color-blind');\nblinder.protanopia('#42dead'); // result: \"#d1c4a0\"\n\n\nHere is our color sample mapped through three kinds of color blindness:\n\nfunction cbsimTable(sample) {    var table = d3.select('.simulated').html('');    var tr = table.appendMany('tr', ['normal','deuteranopia','protanopia','tritanopia']);    tr.append('td').append('b').text(d3.f());    var div = tr.append('td').append('div.colors.large');    div.appendMany('div.color', function(t) {            return t == 'normal' ? sample :                sample.map(function(c) { return blinder[t](c); })        })        .style('background', d3.f());    simColorMatrix(sample);    simColorRatioMatrix(sample);    summaryTable(sample);}\nNow we can just repeat the color difference matrix for the converted sample colors:\n\nYou can click through the buttons below to see the color difference matrix for each color blindness simulation:\n    normal    deuteranopia    protanopia    tritanopia\nfunction simColorMatrix(sample) {    var btns = d3.selectAll('.select-simulation button'),        typ = d3.select('.select-simulation button.selected').text();    btns.on('click.vis4', function() {        btns.classed('selected', false);        d3.select(this).classed('selected', true);        simColorMatrix(sample);    });    var colors = typ == 'normal' ? sample :        sample.map(function(c) { return blinder[typ](c); });    colorDistanceMatrix('.cdm-sim', colors);}\n# Computing difference ratios between normal and colorblind vision\nWe’re getting closer to the finish line! The only thing that’s left to do now is to compare the differences under normal vision with the differences under a color blindness simulation.\nOne way to do this is to look at the ratio between the two differences:\nratio(c1,c2)=distnormal(c1,c2)distcolorblind(c1,c2)ratio(c_1,c_2)=\\frac{dist_{normal}(c_1,c_2)}{dist_{colorblind}(c_1,c_2)}ratio(c​1​​,c​2​​)=​dist​colorblind​​(c​1​​,c​2​​)​​dist​normal​​(c​1​​,c​2​​)​​\nTo illustrate the ratio, let’s look at the notorious green/red color pair and compute the differences between them after applying different colorblindness simulations.\n\nfunction colorRatios(col1, col2) {    var table = d3.select('.color-ratios').html('');    var modes = ['normal','deuteranopia','protanopia','tritanopia'];    table.append('tr').appendMany('th', [''].concat(modes)).text(d3.f());    var dist_norm = deltaE(col1,col2);    var columns = modes.map(function(mode) {        var c1 = mode == 'normal' ? col1 : blinder[mode](col1);        var c2 = mode == 'normal' ? col2 : blinder[mode](col2);        var dist = deltaE(c1,c2);        return {            mode: mode,            colors: [c1,c2],            distance: dist,            ratio: dist_norm.toFixed()+'/'+dist.toFixed()+' = '+ (dist_norm / dist).toFixed(1)+''        };    });    // colors    var tr = table.append('tr');    tr.append('td').text('Colors');    tr.appendMany('td.color', columns)        .append('div.colors.xlarge')        .appendMany('div.color', (col) => col.colors)        .style('background', d3.f());    // distance    tr = table.append('tr');    tr.append('td').text('Distance');    tr.appendMany('td', columns)        .text((col) => col.distance.toFixed(0));    // ratio    tr = table.append('tr');    tr.append('td').text('Ratio').style('font-weight','bold');    tr.appendMany('td', columns)        .html((col) => col.ratio);}colorRatios('#54ae49', '#f06c62');\nA ratio of 17.3 means that a color pair is seventeen times more differentiable under normal vision than under this type of color blindness. The higher the ratio the more information is “lost” for a colorblind person. This is what we’ll use to decide what colors are ok or not.\nNow, to look at all the ratios at once, let’s make another matrix. The table looks just like the matrices we’ve seen before, except now the values in the cells show the ratio between the normal distance and the colorblind distance for each color pair.\n\n    normal    deuteranopia    protanopia    tritanopia\nfunction simColorRatioMatrix(sample) {    var btns = d3.selectAll('.select-simulation2 button'),        typ = d3.select('.select-simulation2 button.selected').text();    btns.on('click.vis4', function() {        btns.classed('selected', false);        d3.select(this).classed('selected', true);        simColorRatioMatrix(sample);    });    var colors = typ == 'normal' ? sample :        sample.map(function(c) { return blinder[typ](c); });    colorDistanceMatrix('.cdm-sim-ratios', colors, sample);}\nYou can use the buttons to cycle through the different simulations.\nAll that’s left to do now is to decide when to show a warning.\n# When to show a color warning\nOne criteria to check for I came up with is to check how many of the color pairs that were differentiable under normal vision turn into non-differentiable color pairs under colorblind vision. I also found that difference ratios above five signal a significant loss of information. So I ended up using a mix of both to decide when to trigger warnings.\nAs you can see in the table, it all depends highly on where I set the thresholds. You can tweak the sliders below to change them and see the results changing.\n\n    Difference threshold: \n    Ratio threshold: \n    Warning threshold: %\n\n    function summaryTable(colors) {        d3.selectAll('.summary-ctrls input').on('input', function() {            summaryTable(colors);        });        var ratio_thresh = +d3.select('.ratio-input').prop('value');        var diff_thresh = +d3.select('.diff-input').prop('value');        var warning_thresh = +d3.select('.warning-input').prop('value');        console.log('rrr', ratio_thresh, ratio_thresh.toFixed(1));        d3.select('.ratio-thresh').html(ratio_thresh.toFixed(1));        d3.select('.diff-thresh').html(diff_thresh.toFixed(1));        d3.select('.warning-thresh').html(warning_thresh.toFixed(1));        var table = d3.select('.summary').html('');        var modes = ['normal', 'deuteranopia','protanopia','tritanopia'];        table.append('tr').appendMany('th', [''].concat(modes)).text(d3.f());        var num_colors = colors.length,            num_perm = (colors.length*(colors.length-1))/2;        // var dist_ratio = (chroma.deltaE(col1,col2)+chroma.deltaE(col2,col1))*0.5;        var columns = modes.map(function(mode) {            // var c1 = mode == 'normal' ? col1 : blinder[mode](col1);            // var c2 = mode == 'normal' ? col2 : blinder[mode](col2);            // var dist = (chroma.deltaE(c1,c2)+chroma.deltaE(c2,c1))*0.5;            var diff_colors = 0,                cnt_ratio = 0,                cnt_sim_diff = 0,                max_ratio = 0,                sum_ratio = 0;            colors.forEach((col_a, i) => {                colors.forEach((col_b, j) => {                    if (j>i) {                        var dist_norm = deltaE(col_a, col_b);                        if (dist_norm >= diff_thresh) {                            diff_colors++;                            if (mode != 'normal') {                                var sim_a = blinder[mode](col_a),                                    sim_b = blinder[mode](col_b);                                var dist_sim = deltaE(sim_a, sim_b);                                if (dist_sim  max_ratio) max_ratio = ratio;                                if (ratio >= ratio_thresh) cnt_ratio++;                            }                        }                    }                });            });            var ratio_pct = 100*(cnt_ratio/diff_colors);            var diff_pct = 100*(cnt_sim_diff/diff_colors);            return {                mode: mode,                colors: mode == 'normal' ? num_colors : '-',                permutations: mode == 'normal' ? num_perm : '-',                differentiable: mode == 'normal' ? diff_colors : '-',                sim_not_diff: mode == 'normal' ? '' : cnt_sim_diff + ' ('+diff_pct.toFixed(1)+'%)',                ratio: mode == 'normal' ? '' : cnt_ratio+' ('+ratio_pct.toFixed(1)+'%)',                ratio_avg: mode == 'normal' ? '' : (sum_ratio / diff_colors).toFixed(1),                ratio_max: mode == 'normal' ? '' : (max_ratio).toFixed(1),                warning: mode== 'normal' ? '' : ratio_pct >= warning_thresh || diff_pct > warning_thresh ? 'WARNING' : 'ok'                // colors: [c1,c2],                // distance: dist,                // ratio: dist_norm / dist            };        });        // number of colors        var tr = table.append('tr');        tr.append('td').text('Colors');        tr.appendMany('td', columns).text(d3.f('colors'));        // number of color permutations        tr = table.append('tr');        tr.append('td').text('Color pairs');        tr.appendMany('td', columns).text(d3.f('permutations'));        // of those, differentiable        tr = table.append('tr');        tr.append('td').html('Differentiable color pairs');        tr.appendMany('td', columns).text(d3.f('differentiable'));        // hr        // tr = table.append('tr');        // tr.append('td').attr('colspan', 4)        //     .st({textAlign:'center', color: '#999'})        //     .text('of those...');        // non differentiable        tr = table.append('tr');        tr.append('td').html('..that turn into non-differentiable\npairs under colorblind simulation');        tr.appendMany('td', columns).text(d3.f('sim_not_diff'));        // of those, differentiable        // tr = table.append('tr');        // tr.append('td').text('Avg. ratio');        // tr.appendMany('td', columns).text(d3.f('ratio_avg'));        // // of those, differentiable        // tr = table.append('tr');        // tr.append('td').text('Max. ratio');        // tr.appendMany('td', columns).text(d3.f('ratio_max'));        // of those, differentiable        tr = table.append('tr');        tr.append('td').text('Ratio > '+(ratio_thresh.toFixed(1)));        tr.appendMany('td', columns).text(d3.f('ratio'));        // of those, differentiable        tr = table.append('tr');        tr.append('td').text('Result');        tr.appendMany('td', columns).html(d3.f('warning'));        // of those, differentiable    }\nFeel free to scroll back to the top and try out different colors in the map. All the examples and matrices in this post will change accordingly. Let me know if you find bugs along the way.\n# Some ideas for future improvements\nThis algorithm was hacked together in a few days, so obviously there are possible improvements to be made. Here are a few ideas that might be worth exploring:\n\nInstead of the hue-percentiles we could try different methods to find the most “representative” colors, such as k-means clustering.\nOne could look out for different color difference metrics, ideally by having (non-colorblind) people “guess” color differences and compare these results with the various color difference formulas.\nMaybe there are smarter ways to compare the color differences between normal and colorblind vision than looking at the ratios?\nMaybe there are smarter ways to decide how many “problematic” colors are enough to trigger the color blindness warning.\nClearly, it would help testing a larger set of charts and maps with this algorithm to measure its real performance\n\nAs always I’m happy to hear what you think. Feel free to drop a comment below or send me an email at gregor@datawrapper.de.\n    makeMap();    svg,canvas {top:0;}    button { font-size: 15px; padding: 5px 15px;  }    .colors { line-height: 0;  padding: 2px 0 0 2px; display: inline-block;}    .colors .color { box-sizing: border-box; border: 2px solid white; display: inline-block;        width: 16px; height: 16px; margin: -2px 0 0 -2px; }    .colors.large .color { width: 28px; height: 28px;  }    td .colors.large .color { width: 24px; height: 24px;  }    .colors.xlarge .color { width: 38px; height: 38px;  }    .colors.xlarge .color+.color{ border-left: 0 }    .all-colors-sorted { border-left: 2px solid white }    .all-colors-sorted .color{ border-left: 0 }    .content table.color-distance-matrix { cursor:pointer;overflow: hidden; z-index: 1; border-spacing: 0; border-collapse: separate; border: 1px solid white;}    .color-distance-matrix th,    .color-distance-matrix td { border:1px solid white; padding:0; color: #888;}    .color-distance-matrix th { width:16px; height:21px; }    .color-distance-matrix td { font-size: 11px; text-align: center;vertical-align: middle; position: relative;}    .color-distance-matrix tr:hover { background: #e0e0e0 }    .color-distance-matrix td:hover::after {        background-color: #e0e0e0; content: '\\00a0';height: 10000px;left: 0;position: absolute;        top: -5000px;width: 100%;z-index: -1;    }    .summary-ctrls label { display: inline-block; font-size: 14px; margin-right: 15px }    label input[type=range] { vertical-align: middle; margin: 0;}    .color-distance-matrix td:hover { background: #ccc; }    .color-distance-matrix td.relevant { color: #333; }    .color-distance-matrix td.strong { font-weight: bold; color:#fff;}    .color-distance-matrix td.lower { color: #d7d7d7; background: transparent!important;}"
  },
  {
    "title": "Not My Society\n",
    "link": "https://vis4.net/blog/2019/03/not-my-society/",
    "pubDate": "2019-03-20T09:09:30.000Z",
    "isoDate": "2019-03-20T09:09:30.000Z",
    "content": "<p>A few weeks ago we saw the foundation of a new group called the <a href=\"https://www.datavisualizationsociety.com/\" target=\"_blank\" rel=\"noopener\">Data Visualization Society</a>. Since its launch some 4,000 “members” have joined. There’s a Slack channel for discussion, a <a href=\"http://medium.com/data-visualization-society\" target=\"_blank\" rel=\"noopener\">Medium publication</a> and a collection of links to useful resources.</p><img alt=\"logo of the &quot;Data Visualization Society&quot;\" src=\"/blog/images/2019/datavisualizationsociety-logo.png\" style=\"background: white; max-width: 25em\"><span class=\"image-caption\">logo of the &quot;Data Visualization Society&quot;</span><p>It’s main goal is described as supporting its member in developing their data visualization skills:</p><blockquote><p>The <strong>Data Visualization Society</strong> aims to collect and establish best practices and foster community to support its members as they develop their data visualization skills.</p></blockquote><p>In general I support the idea of a community of visualization professionals getting together to share their knowledge and engage in critical discussions about the field, and to make this clear: I wish the DVS (as I am going to abreviate the group for the rest of this text) and its members all the best in achieving their goals.</p><p>However, I do have some issues in <em>how</em> the DVS has decided to build this community, and I want to share my thoughts in this blog post.</p><h2 id=\"1-talk-to-the-community-before-launching-your-community-project\"><a class=\"anchor\" href=\"#1-talk-to-the-community-before-launching-your-community-project\"><span class=\"header-anchor\">#</span></a> 1. Talk to the community before launching your community project</h2><p>If there is one thing I know about community building it is that reaching out and talking to people is perhaps the most essential part. Like many people from the field I was taken by surprise when I learned about the launch of a new group that seemingly aimed to represent data visualization.</p><p>If the DVS tries to be something like the new home for the datavis community, it certainly would have been nice to ask the community just how this home should look like <sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>, how it should be called, what its mission should or could be, etc.</p><p>While this kind of “building in secret” probably helps to maximize the launch impact, it leaves me wondering why so many people who are already working in the same area where not given a chance to get involved earlier in the process. There could’ve been 15 founding members instead of just three.</p><h2 id=\"2-put-yourself-behind-the-community-not-in-front-of-it\"><a class=\"anchor\" href=\"#2-put-yourself-behind-the-community-not-in-front-of-it\"><span class=\"header-anchor\">#</span></a> 2. Put yourself behind the community, not in front of it</h2><p>When I first learned about the DVS I checked out the website and was a little bit put off by the <a href=\"https://www.datavisualizationsociety.com/team\" target=\"_blank\" rel=\"noopener\">team page</a>  that lists the three “founding members”, complete with professional photos and short biographies.</p><p>This might be just a matter of taste, but I couldn’t help but thinking that this page is more about establishing leadership than it is about welcoming people. Working in the startup myself – where the differentiation between “founders” and “non-founders” is usually about ownership and company control – this team page probably triggered the wrong impulses with me.</p><p>Or to stick with that community home metaphor: if you are trying to build a house where everyone feels welcome, you should perhaps resist the temptation of putting your statues out on the front lawn, no matter how proud you are of what you achieved. Communicating that this is about the community and not about you.</p><h2 id=\"3-keep-the-door-open-and-the-blinds-up\"><a class=\"anchor\" href=\"#3-keep-the-door-open-and-the-blinds-up\"><span class=\"header-anchor\">#</span></a> 3. Keep the door open and the blinds up</h2><p>Now that the house has been built it would be nice to see what’s going on inside, who’s already there, what’s being discussed, what the atmosphere is like etc. People in the community might wonder whether this is is a place for them or not. So it seems like a good idea to leave the front door wide open and pull up the windows blinds.</p><p>But the DVS decided to do the opposite. The main forum is a Slack channel and there’s no way of looking inside before you commit to “join” the society. And “joining a society” certainly is a bigger commitment than for instance signing up for a newsletter or logging in with your Twitter account.</p><p>And in case with the DVS, it involves answering a rather awkward questionnaire about your self-described visualization skills, and a note about potential membership fees in the future.</p><img alt=\"The Data Visualization Society Membership form\" src=\"/blog/images/2019/datavisualizationsociety-survey.png\"><span class=\"image-caption\">The Data Visualization Society Membership form</span><h2 id=\"4-give-before-you-take\"><a class=\"anchor\" href=\"#4-give-before-you-take\"><span class=\"header-anchor\">#</span></a> 4. Give before you take</h2><p>Community engagement should be mostly about giving instead of taking. So many people in this field have dedicated years (or even decades) of their lives to the data visualization community. They’ve written blog posts, published tutorials, made podcasts, and most of them have never asked for anything in return.</p><p>Again, with the DVS for things look a little different. Other than a Slack channel I still don’t really know just <em>how</em> they are planning to help people improve their visualization skills. The only thing they made clear from the start is that at some point there’s going to be a membership fee (once they figured out “how to provide value”). And indeed, just weeks after launch they started a <a href=\"https://www.patreon.com/datavizsociety\" target=\"_blank\" rel=\"noopener\">Patreon campaign</a>.</p><p>For instance, the first $1000 per month (which they already reached) are dedicated to covering “recurring expenses” like “website hosting, domain name, email” and “website design”. At the same time hundreds of people in the field, including myself, have designed and hosted many, many websites for the data visualization community without ever asking for money<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup>.</p><h2 id=\"the-society-id-like-to-join\"><a class=\"anchor\" href=\"#the-society-id-like-to-join\"><span class=\"header-anchor\">#</span></a> The society I’d like to join</h2><p>To end this post  on a more positive note, here’s a description of a society I’d like to join:</p><ul><li>It’s an <strong>open society</strong> that everyone can be a part of (official “members” and non-members)</li><li>It’s an <strong>egalitarian society</strong> that makes no distinction between “founders” and members who joined at a later point.</li><li>It’s a <strong>free society</strong> in which there are no membership fees, and any contributions from its members are voluntary</li><li>It’s an <strong>integrative society</strong> that is not trying to replace communities and organizations that already exist but tries to integrate them.</li></ul><p>Fortunately <strong>there is already an absolutely fantastic community around data visualization</strong>! There are lots of interesting discussions on <a href=\"https://twitter.com/search?q=%23datavis&amp;src=typd\" target=\"_blank\" rel=\"noopener\">Twitter</a> and the <a href=\"https://vis.social\" target=\"_blank\" rel=\"noopener\">Fediverse</a>, there are great many Slack channels for people to meet, there are many blog authors who write about data visualization topics (and engage in reader discouse in their comment sections), there are thousands of tutorials and <a href=\"https://bl.ocks.org/\" target=\"_blank\" rel=\"noopener\">free code examples</a> that help practitioners to get started, there are free and open source <a href=\"https://d3js.org/\" target=\"_blank\" rel=\"noopener\">visualization</a> libraries.</p><p>There are also great datavis <strong>podcasts</strong> like <a href=\"http://datastori.es/\" target=\"_blank\" rel=\"noopener\">Data Stories</a> and <a href=\"https://policyviz.com/podcast/\" target=\"_blank\" rel=\"noopener\">PolicyViz</a> which regularly invite people from the vis field to discuss interesting topics, there are <a href=\"https://www.getrevue.co/profile/FairWarning\" target=\"_blank\" rel=\"noopener\">data vis newsletters</a>, there’s an amazing number of great data visualization books, and now there’s even a <a href=\"https://twitter.com/datavisclub\" target=\"_blank\" rel=\"noopener\">book club</a> where you can meet the authors of these books.</p><p>There are over <strong>13 million people</strong> in the <a href=\"https://www.reddit.com/r/dataisbeautiful/\" target=\"_blank\" rel=\"noopener\">r/dataisbeautiful</a> Reddit subchannel (which is also organizing <a href=\"https://www.reddit.com/r/dataisbeautiful/comments/axknia/battle_dataviz_battle_for_the_month_of_march_2019/\" target=\"_blank\" rel=\"noopener\">monthly visualization challenges</a> btw), there are thousands of <a href=\"https://www.meetup.com/find/?allMeetups=false&amp;keywords=data+visualization&amp;radius=Infinity&amp;userFreeform=Berlin%2C+Germany&amp;mcId=z1007698&amp;mcName=Berlin%2C+DE&amp;sort=recommended&amp;eventFilter=mysugg\" target=\"_blank\" rel=\"noopener\">datavis</a> and <a href=\"http://maptime.io/\" target=\"_blank\" rel=\"noopener\">mapping</a> meetups around the world where people get together, there are <a href=\"https://www.informationisbeautifulawards.com/\" target=\"_blank\" rel=\"noopener\">datavis awards</a>, and there are <a href=\"http://www.openvisconf.com/\" target=\"_blank\" rel=\"noopener\">datavis conferences</a> that give people in the field a chance to speak (and record their talks and make them available for free to the many people who couldn’t afford conference tickets).</p><p>Isn’t all that amazing? I am really curious to see how and where the Data Visualization Society is going to fit into this field, and what additional “value” it is going to provide.</p><p>Finally, I’d like to ask you not to mistake the critical thoughts I offered here for bitterness or malevolence. I am happy to see the DVS get this much support, and I’ve <a href=\"https://twitter.com/FILWD/status/1107005564612198400\" target=\"_blank\" rel=\"noopener\">read really good things</a> about the discussions in the Slack channel, and I genuinely wish this project all the best for future! 👋</p><hr class=\"footnotes-sep\"><section class=\"footnotes\"><ol class=\"footnotes-list\"><li id=\"fn1\" class=\"footnote-item\"><p>I've heard multiple people complaining about the logo typeface already 😉 <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p></li><li id=\"fn2\" class=\"footnote-item\"><p>I am on the internet long enough to know that a .COM domain costs $7.99 a year and website hosting is basically free, so I did wonder what exactly these $1000 dollars a month are going to be spent on <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p></li></ol></section>",
    "contentSnippet": "A few weeks ago we saw the foundation of a new group called the Data Visualization Society. Since its launch some 4,000 “members” have joined. There’s a Slack channel for discussion, a Medium publication and a collection of links to useful resources.\nlogo of the \"Data Visualization Society\"\nIt’s main goal is described as supporting its member in developing their data visualization skills:\n\nThe Data Visualization Society aims to collect and establish best practices and foster community to support its members as they develop their data visualization skills.\n\nIn general I support the idea of a community of visualization professionals getting together to share their knowledge and engage in critical discussions about the field, and to make this clear: I wish the DVS (as I am going to abreviate the group for the rest of this text) and its members all the best in achieving their goals.\nHowever, I do have some issues in how the DVS has decided to build this community, and I want to share my thoughts in this blog post.\n# 1. Talk to the community before launching your community project\nIf there is one thing I know about community building it is that reaching out and talking to people is perhaps the most essential part. Like many people from the field I was taken by surprise when I learned about the launch of a new group that seemingly aimed to represent data visualization.\nIf the DVS tries to be something like the new home for the datavis community, it certainly would have been nice to ask the community just how this home should look like [1], how it should be called, what its mission should or could be, etc.\nWhile this kind of “building in secret” probably helps to maximize the launch impact, it leaves me wondering why so many people who are already working in the same area where not given a chance to get involved earlier in the process. There could’ve been 15 founding members instead of just three.\n# 2. Put yourself behind the community, not in front of it\nWhen I first learned about the DVS I checked out the website and was a little bit put off by the team page  that lists the three “founding members”, complete with professional photos and short biographies.\nThis might be just a matter of taste, but I couldn’t help but thinking that this page is more about establishing leadership than it is about welcoming people. Working in the startup myself – where the differentiation between “founders” and “non-founders” is usually about ownership and company control – this team page probably triggered the wrong impulses with me.\nOr to stick with that community home metaphor: if you are trying to build a house where everyone feels welcome, you should perhaps resist the temptation of putting your statues out on the front lawn, no matter how proud you are of what you achieved. Communicating that this is about the community and not about you.\n# 3. Keep the door open and the blinds up\nNow that the house has been built it would be nice to see what’s going on inside, who’s already there, what’s being discussed, what the atmosphere is like etc. People in the community might wonder whether this is is a place for them or not. So it seems like a good idea to leave the front door wide open and pull up the windows blinds.\nBut the DVS decided to do the opposite. The main forum is a Slack channel and there’s no way of looking inside before you commit to “join” the society. And “joining a society” certainly is a bigger commitment than for instance signing up for a newsletter or logging in with your Twitter account.\nAnd in case with the DVS, it involves answering a rather awkward questionnaire about your self-described visualization skills, and a note about potential membership fees in the future.\nThe Data Visualization Society Membership form\n# 4. Give before you take\nCommunity engagement should be mostly about giving instead of taking. So many people in this field have dedicated years (or even decades) of their lives to the data visualization community. They’ve written blog posts, published tutorials, made podcasts, and most of them have never asked for anything in return.\nAgain, with the DVS for things look a little different. Other than a Slack channel I still don’t really know just how they are planning to help people improve their visualization skills. The only thing they made clear from the start is that at some point there’s going to be a membership fee (once they figured out “how to provide value”). And indeed, just weeks after launch they started a Patreon campaign.\nFor instance, the first $1000 per month (which they already reached) are dedicated to covering “recurring expenses” like “website hosting, domain name, email” and “website design”. At the same time hundreds of people in the field, including myself, have designed and hosted many, many websites for the data visualization community without ever asking for money[2].\n# The society I’d like to join\nTo end this post  on a more positive note, here’s a description of a society I’d like to join:\n\nIt’s an open society that everyone can be a part of (official “members” and non-members)\nIt’s an egalitarian society that makes no distinction between “founders” and members who joined at a later point.\nIt’s a free society in which there are no membership fees, and any contributions from its members are voluntary\nIt’s an integrative society that is not trying to replace communities and organizations that already exist but tries to integrate them.\n\nFortunately there is already an absolutely fantastic community around data visualization! There are lots of interesting discussions on Twitter and the Fediverse, there are great many Slack channels for people to meet, there are many blog authors who write about data visualization topics (and engage in reader discouse in their comment sections), there are thousands of tutorials and free code examples that help practitioners to get started, there are free and open source visualization libraries.\nThere are also great datavis podcasts like Data Stories and PolicyViz which regularly invite people from the vis field to discuss interesting topics, there are data vis newsletters, there’s an amazing number of great data visualization books, and now there’s even a book club where you can meet the authors of these books.\nThere are over 13 million people in the r/dataisbeautiful Reddit subchannel (which is also organizing monthly visualization challenges btw), there are thousands of datavis and mapping meetups around the world where people get together, there are datavis awards, and there are datavis conferences that give people in the field a chance to speak (and record their talks and make them available for free to the many people who couldn’t afford conference tickets).\nIsn’t all that amazing? I am really curious to see how and where the Data Visualization Society is going to fit into this field, and what additional “value” it is going to provide.\nFinally, I’d like to ask you not to mistake the critical thoughts I offered here for bitterness or malevolence. I am happy to see the DVS get this much support, and I’ve read really good things about the discussions in the Slack channel, and I genuinely wish this project all the best for future! 👋\n\n\nI've heard multiple people complaining about the logo typeface already 😉 ↩︎\n\nI am on the internet long enough to know that a .COM domain costs $7.99 a year and website hosting is basically free, so I did wonder what exactly these $1000 dollars a month are going to be spent on ↩︎"
  },
  {
    "creator": "Maarten Lambrechts",
    "title": "The essential lies in news maps",
    "link": "https://datajournalism.com/read/longreads/the-essential-lies-in-news-maps",
    "pubDate": "Thu, 04 Apr 2019 10:29:00 +0200",
    "isoDate": "2019-04-04T08:29:00.000Z",
    "content": "\n                                                                        <p>\"Not only is it easy to lie with maps, it is essential.\" </p>\n<p>While this may seem to be a bold and surprising statement, it's a <a href=\"http://www.markmonmonier.com/how_to_lie_with_maps_14880.htm\">long held view</a> of renowned geography professor, Mark Monmonier. </p>\n<p>And, of course, Monmonier is right. In order to display the big, three-dimensional, and complex world we live in on a small piece of two-dimensional paper or on a screen with a limited number of pixels, we are forced to distort reality. As you'll soon see, every map does so in its own way. So how can maps, which distort reality, be married with journalism, which tries to paint an objective and accurate image of the world?</p>\n<p>In the context of news, maps are an intuitive way to show the location of where events took place, but they can be so much more than this. Maps can also explain how things happened, they can be the canvas on which a story is told, they can put the size and extent of things in context, and they can be used to show geographical patterns hidden in data. </p>\n<p>So, are all maps in the news lying? Are all news maps 'fake news'? If done well, they are not. But it is quite easy to produce misleading maps, even with the best intentions. Because there are plenty of reasons to use maps in the newsroom anyway, let's look at some commonly used maps and learn how to avoid being misled by them.</p>\n\n                                                                                                                                <p>Maarten Lambrechts is a data journalist, data designer, and visualization consultant. Follow on Twitter: \n<a href=\"https://twitter.com/maartenzam\">@maartenzam</a>.</p>\n\n                                                                                                                              \n                            <h2>The locator map</h2>\n<p>A good news story answers the '5 W' questions: the who, what, when, where and why of something that happened. When an article only mentions the where of a story in the text, many people will not be able to really connect to it. A lot of readers simply lack the geographical knowledge to pinpoint Lombok, Lithuania, Luanda or Leicester Square, and to relate these locations to the places where they are living themselves, or to other places they are familiar with.</p>\n<p>The visualisation of a location through a locator map can overcome this problem. This type of map helps the reader to contextualise a news story geographically; it shows the location of an event in the context of the surrounding geography, offering many entry points to the map for the reader to connect with. Locator maps in the news show where that earthquake happened, where that exotic tax haven is located, where in my city that bank was robbed and where exactly in the world that ongoing violent conflict is taking place.</p>\n\n                                                                      \n                            <p><strong>Devastation in Lombok</strong></p>\n<p>A locator map showing the location of Lombok, part of the introduction of <a href=\"https://fingfx.thomsonreuters.com/gfx/rngs/INDONESIA-QUAKE/010071NH35P/index.html\">Devastation in Lombok</a>, by <a href=\"https://twitter.com/ReutersGraphics\">Reuters Graphics</a>.</p>\n\n                                                                      \n                            <p>Locator maps let people assess how a news story is related to their own life. Did something happen close by? Did something happen in a country they've visited or where people they know live? Or did something happen in a country neighbouring a country they know or have some kind of connection to? Based on these questions, readers can quickly evaluate how relevant a story is to them. And a locator map makes this evaluation easier than providing a description in text only.</p>\n\n                                                                      \n                            <p>So, in what sense does the humble locator map lie to the reader? Well, locator maps usually are very small in order to be readable. This means that these maps leave out many details: sinuous roads become straight lines, smaller roads are left out (or roads are left out altogether) and a group of mountain peaks can be represented by a single symbol for a whole mountain ridge. In other words, these maps are heavily <a href=\"https://en.wikipedia.org/wiki/Cartographic_generalization\">generalised</a>, which limits their accuracy and broader use cases. Don't use them for navigational purposes, for example. You will get lost.</p>\n\n                                                                      \n                            <h3>The breaking news map</h3>\n<p>Maps can do more than show the ‘where’ of a story. When news breaks, the big challenge for journalists is to explain to their audiences <em>how</em> events unfolded. In many cases, the best way to do so is by using an annotated ‘breaking news’ map.</p>\n\n                                                                      \n                            <p>A breaking news map showing what happened in Nice on 14 of July 2016, from <a href=\"http://graphics.wsj.com/what-we-know-about-the-nice-attack/\">What We Know After Terror Attack</a> in Nice, France by the Wall Street Journal. Notice how the annotated map at the top is accompanied by two locator maps: the first one to situate Nice in France and the second one to situate the attack in the city of Nice.</p>\n\n                                                                      \n                            <p>While locator maps only communicate 'something happened here', annotated maps can show a sequence of events and convey other information relevant to the story. People familiar with the location can mentally replay what happened by connecting what’s on the map with how they know the place.</p>\n<p>Often these maps use oblique, 3D-views of a city, so people unfamiliar with the location can still get a good sense of exactly what happened and how things looked on the ground. <a href=\"https://support.google.com/earth/answer/21955?hl=en\">With Google Earth Pro</a> you can generate these very detailed, oblique views for free.</p>\n\n                                                                      \n                            <p>An annotated map describing the events in Berlin on 19 December 2016 by <a href=\"https://spiegel.de\">Spiegel Online</a>.</p>\n\n                                                                      \n                            <p>But be careful. Some people might think these 3D images are real pictures, taken at oblique angles from airplanes or helicopters. Remind them that they are not; instead, they are generated by Google Earth by 'draping' satellite images over a detailed 3D model of the Earth. In some cases this process leads to glitches, as the below ferris wheel in Scheveningen, the Netherlands, clearly demonstrates.</p>\n\n                                                                                                  \n                                <p>The Scheveningen ferris wheel, a glitch in Google’s 3D model of the earth.</p>\n\n                                                                                        <p>It’s also important that readers and visual journalists remember that Google Earth images are typically a few months to a few years old. Suggesting that these images are 'live' or taken after the breaking news event took place would be lying.</p>\n\n                                                                                                                              \n                                <h3>The extent map</h3>\n<p>Let’s move along to a map type that is definitely lying to the reader: the extent map. In order to communicate the size and magnitude of things around the world, these maps cut them out of their real geographical location and paste them into a foreign one.</p>\n<p>For example, during the 2014 Winter Olympics in Sochi, the New York Times produced an extent map to illustrate the size of the site’s infrastructure, by copy-pasting these into the streets of Manhattan.</p>\n\n                                                                                        <p><strong>Luge, Bobsled and Skeleton</strong></p>\n<p>Racers might begin their starting sprints 40 stories up and several blocks north of Times Square for the run down the city’s own version of the Sanki Sliding Center’s track, finishing in a big turn on the plaza in front of the Armed Services Recruiting Center. Credit: <a href=\"https://www.nytimes.com/interactive/2014/02/04/sports/olympic-venues.html\">Is That a Luge in Times Square?</a> by the New York Times.</p>\n\n                                                                                                                                             \n                            <p>Many readers lack good reference points for assessing how long a 400 meter ship really is, or how far 5.800 square kilometers really stretch out. By showing these objects and areas on maps that the reader can relate to, a direct connection can be made to their reference frames.</p>\n<p>When the iceberg A-68 broke away from the Larsen C ice shelf in Antarctica, many media <a href=\"https://www.google.com/search?source=hp&amp;ei=1sPFW8uAF8aZsAfo94aYAw&amp;q=larsen+c+ice+shelf+delaware&amp;oq=larsen+c+ice+shelf+delaware&amp;gs_l=psy-ab.3...1389.5855.0.5969.29.22.0.6.6.0.136.1643.18j2.21.0....0...1c.1.64.psy-ab..2.26.1719.6..0j35i39k1j0i131k1j0i203k1j0i10i203k1j0i22i30k1j33i160k1j33i21k1.116.q3k7UYLIOXo\">compared it</a> to the size of Delaware. For Americans familiar with the size of this state, this comparison was probably helpful. But for many readers the size of Delaware is just as abstract as 5.800 square kilometres, the actual size of the iceberg. That's why the Berliner Morgenpost built <a href=\"https://interaktiv.morgenpost.de/eisberg-groessenvergleich/\">a little interactive extent map</a>, which the reader can use to copy-paste the iceberg's silhouette to any number of familiar places.</p>\n<p>Extent maps are only useful when the area used for comparison is familiar to the reader. Otherwise the question of 'How big is that iceberg?' remains unanswered and the reader ends up with more questions, like 'How big is Delaware?'. Some readers may even be confused and think the iceberg (which is in a place far away from them) is located near Delaware (an equally remote place for some).</p>\n\n                                                                      \n                            <h3>The before-after map</h3>\n<p>A map type that is becoming increasingly popular in news stories today is the before-after map. While before-after images have a long history in the news, journalists have previously been limited to photographs taken from the ground. Before and after images are now mostly taken from space, by satellites circling the globe.</p>\n<p>Until recently, detailed satellite images with a high temporal resolution were simply not available and as a result details were too blurry or time intervals between pictures were too wide to be useful. Today, a range of satellites with high resolution cameras fly over the same location with a frequency of once every week and sometimes even once per day. This allows for the detection and reconstruction of events like like deforestation, floods, droughts and the construction of buildings.</p>\n<p>NASA's <a href=\"https://climate.nasa.gov/images-of-change\">Images of Change</a> is an example of the powerful impact of before-after maps. Included in the gallery are images highlighting <a href=\"https://climate.nasa.gov/images-of-change?id=657#657-heat-wave-turns-europe-brown\">the impact of drought on Europe</a>, <a href=\"https://climate.nasa.gov/images-of-change?id=654#654-county-fire-lights-up-northern-california-night\">forest fires in California</a>, and <a href=\"https://climate.nasa.gov/images-of-change?id=655#655-hurricane-marias-damage-to-puerto-rico%E2%80%99s-forests\">hurricane damage in Puerto Rico</a>.</p>\n\n                                                                                                                  \n                            <p>Newsrooms have also discovered the power of before-after satellite images. Often, the supplier of these images is <a href=\"https://www.planet.com/\">Planet</a>, a Silicon Valley satellite imaging company that offers daily high resolution pictures with its 'flock' of shoebox sized satellites. The company has already provided images for news stories about <a href=\"https://www.chinadialogue.net/blog/10761-China-is-building-coal-power-again/en\">the construction of Chinese coal plants</a>, <a href=\"https://www.zeit.de/wissen/umwelt/2018-08/duerre-deutschland-hitze-klima-wald-forst\">the effect of drought on German vegetation</a>, and <a href=\"https://www.washingtonpost.com/world/national-security/us-spy-agencies-north-korea-is-working-on-new-missiles/2018/07/30/b3542696-940d-11e8-a679-b09212fb69c2_story.html?utm_term=.a1a371eab928\">the development of North Korean missiles</a>.</p>\n<p>Before-after maps are usually very explicit about the date images were taken, so there is no room for misleading there. But like all aerial imagery, these images are still flawed. For example, images are only useful for before-after maps when they are taken during daytime and on cloud-free days. Ever noticed that the sun is always shining in these images? For this reason, photos showing destruction after a big storm usually take some time to become available after the clouds have disappeared. Some parts of the world are also much cloudier than others, so clear satellite pictures for these regions are rare.</p>\n<p>And that's not all. All of these raw images need to be corrected with good colour correcting algorithms. Differences in the angle at which the sun is illuminating the scene, differences in atmospheric conditions, and the variations between cameras on board different satellites introduce biases and glitches in satellite images. Ignoring these differences, or trying to remove them with badly designed algorithms, will lead to misleading before-after images. Brown areas 'affected by drought' could well be looking a lot greener when processed incorrectly, for example.</p>\n\n                                                                      \n                            <h3>Numbers on maps</h3>\n<p>'There are lies, damn lies, and statistics,' the saying goes. So what happens when you mix statistics with maps, which are distortions of reality by definition? You get numbers on maps, and those are really easy to screw up and can very easily mislead.</p>\n<p>These maps don’t serve as general reference maps, using data to show geographical patterns about a certain topic instead. Most commonly, they are used by journalists during election times, to show where people voted and for whom.</p>\n\n                                                                      \n                            <p>A 2016 election map showing voting patterns in Berlin. <a href=\"http://berlinwahlkarte2016.morgenpost.de/\">Interactive version</a> by <a href=\"https://twitter.com/BMgrafik\">Berliner Morgenpost</a>.</p>\n\n                                                                      \n                            <p>Maps that show administrative areas shaded according to some data value are called choropleth maps. These are useful for showing geographic patterns in statistics that are collected at the level of administrative units; for example, the average age of the population for each country, or the share of impoverished population living in the municipalities of a country. </p>\n<p>One common mistake when making choropleth maps is using absolute numbers instead of relative (or 'normalised') numbers. Values need to be scaled to the population inhabiting every administrative unit. If numbers are not scaled, the result is a map like the below (which I understand is a favourite of President Trump’s):</p>\n\n                                                                                                                                 <p>Tweeted on 11 May 2017: \"Spotted: A map to be hung somewhere in the West Wing\".</p>\n\n                                                                                                                              \n                            <p>On this map, population density is not taken into account and, as a result, a lot of the Republican red dominates the map (that’s why President Trump likes this map so much). The millions of blue Democratic voters concentrated in the big cities on the eastern and western coast are not well represented on this map, because they live in a relatively small geographical area.</p>\n<p><a href=\"https://www.nytimes.com/interactive/2016/11/01/upshot/many-ways-to-map-election-results.html?mtrref=undefined&amp;gwh=CDCA877105D162703FCC9945F96470FD&amp;gwt=pay\">Many techniques</a> exist to overcome this problem, one of these solutions is called a cartogram.</p>\n\n                                                                      \n                            <p>A US election map that scales the states to account for the number of voters living in it. Credit: The New York Times.</p>\n\n                                                                      \n                            <p>With this technique, densely populated areas are assigned more visual space on the map. But cartograms have their own downsides: they distort the geography considerably, as is clear in the example above.</p>\n\n                                                                      \n                            <h3>Lying world maps</h3>\n<p>Probably the biggest and most frequent lie in mapping refers to world maps that use the <a href=\"https://en.wikipedia.org/wiki/Mercator_projection\">Mercator projection</a>. Almost all online maps use the Mercator projection, but the projection is used in many static and offline world maps too.</p>\n<p>The problem with the Mercator projection is that it distorts areas close to the poles enormously. Did you know Greenland isn’t really the same size of Africa? It’s actually 14 times smaller.</p>\n\n                                                                                                                  \n                            <p>Because of these distortions, it is better to avoid the Mercator projection for maps showing areas close to the poles and world maps. This is the reason why Google Maps <a href=\"https://twitter.com/googlemaps/status/1025130620471656449?ref_src=twsrc%5Etfw\">decided</a> to switch to an <a href=\"https://en.wikipedia.org/wiki/Orthographic_projection_in_cartography\">orthographic projection</a> when zooming out to a world view.</p>\n<p>As a rule of thumb, journalists should try to avoid using the Mercator projection when making world maps. Good alternatives are the <a href=\"https://en.wikipedia.org/wiki/Robinson_projection\">Robinson</a> and <a href=\"https://en.wikipedia.org/wiki/Winkel_tripel_projection\">Winkel-Tripel</a> projections, or the recently developed <a href=\"https://en.wikipedia.org/wiki/Equal_Earth_projection\">Equal Earth</a> projection, which respects areas throughout the whole map.</p>\n\n                                                                      \n                            <h3>WebGL-based maps</h3>\n<p>Today's modern web browsers support <a href=\"https://en.wikipedia.org/wiki/WebGL\">WebGL</a>, a technology that allows browsers to tap into a computer's graphics card, opening up a whole new range of mapping possibilities.</p>\n<p>WebGL makes it possible to rotate maps, tilt the camera view, and visualise data in three dimensions -- it's an exciting new toy. But all these fancy features make it harder for readers to assess the numbers behind a visualisation. Tilted views make features in the back look smaller, and even hidden or obscured by other features in front of them. And because the camera view can be rotated and flown around, the north is not always up in these maps. This may confuse readers.</p>\n<p>Let's look at an application of this technology in <a href=\"https://ig.ft.com/sites/special-reports/one-belt-one-road/\">One belt, one road</a> by The Financial Times. This story uses an animated map of the new Chinese Silk Road, which reacts as the user scrolls through the text. In this way, different features on the map can be highlighted by zooming and rotating the map to get the best view of each section of the new Silk Road. Although this feature helps step the reader through the story, it also means that North is not always up, which can be confusing for readers unfamiliar with the countries and cities shown on the map.</p>\n\n                                                                      \n                            <p>The ‘One belt, one road’ project by the Financial Times.</p>\n\n                                                                      \n                            <p>The adoption of WebGL by mapping tools like Mapbox and more recently <a href=\"https://kepler.gl/#/\">kepler.gl</a> has opened the door for WebGL driven maps that show numbers. These have already found their way into the media, as <a href=\"https://www.nytimes.com/interactive/2018/upshot/election-2016-voting-precinct-maps.html\">An Extremely Detailed Map of the 2016 Election</a> by The New York Times shows.</p>\n\n                                                                      \n                            <p>An extremely detailed map of the 2016 election by The New York Times.</p>\n\n                                                                      \n                            <h3>Every map is a lie</h3>\n<p>Mapmakers make a lot of design decisions in order to produce clear and useful maps. They leave things out, simplify things, highlight elements and put other elements in the background. Areas, shapes and lines are distorted and geographical features may be shifted out of place. Sometimes old imagery is used to show where recent events took place, and unlike in the real world the sun is always shining in satellite images.</p>\n<p>The degrees of freedom in the design of a map are infinite and by changing the size of features, the colours, the layering, the composition and the projection of a map, a different story is told and other distortions are introduced.</p>\n<p>This illustrates perfectly another point made by Professor Monmonier: \"A single map is but one of an indefinitely large number of maps that might be produced for the same situation or from the same data.\"</p>\n<p>Maps are powerful, but sometimes a misleading picture is generated. Mapmakers, as well as map readers, should be aware of their limitations. So, remember that all maps are a lie. But these are necessary lies.</p>\n\n                                                                      \n                            <p>Now that you've learnt how maps can lie, expand your mapping skills by:</p>\n<ul>\n<li>taking Maarten's video course <a href=\"https://datajournalism.com/watch/mapping-for-journalists\">Mapping for Journalism</a> to create both static and interactive maps for your stories</li>\n<li>learning <a href=\"https://datajournalism.com/read/longreads/data-visualisation-trick\">a data visualisation trick</a>, which allows you to represent tiny and bigger countries on the same chart</li>\n<li>exploring the community's favourite maps in <a href=\"https://datajournalism.com/read/newsletters/favourite-maps\">this edition</a> of Conversations with Data.</li>\n</ul>\n\n                                              \n                ",
    "contentSnippet": "\"Not only is it easy to lie with maps, it is essential.\" \nWhile this may seem to be a bold and surprising statement, it's a long held view of renowned geography professor, Mark Monmonier. \nAnd, of course, Monmonier is right. In order to display the big, three-dimensional, and complex world we live in on a small piece of two-dimensional paper or on a screen with a limited number of pixels, we are forced to distort reality. As you'll soon see, every map does so in its own way. So how can maps, which distort reality, be married with journalism, which tries to paint an objective and accurate image of the world?\nIn the context of news, maps are an intuitive way to show the location of where events took place, but they can be so much more than this. Maps can also explain how things happened, they can be the canvas on which a story is told, they can put the size and extent of things in context, and they can be used to show geographical patterns hidden in data. \nSo, are all maps in the news lying? Are all news maps 'fake news'? If done well, they are not. But it is quite easy to produce misleading maps, even with the best intentions. Because there are plenty of reasons to use maps in the newsroom anyway, let's look at some commonly used maps and learn how to avoid being misled by them.\nMaarten Lambrechts is a data journalist, data designer, and visualization consultant. Follow on Twitter: \n@maartenzam.\nThe locator map\nA good news story answers the '5 W' questions: the who, what, when, where and why of something that happened. When an article only mentions the where of a story in the text, many people will not be able to really connect to it. A lot of readers simply lack the geographical knowledge to pinpoint Lombok, Lithuania, Luanda or Leicester Square, and to relate these locations to the places where they are living themselves, or to other places they are familiar with.\nThe visualisation of a location through a locator map can overcome this problem. This type of map helps the reader to contextualise a news story geographically; it shows the location of an event in the context of the surrounding geography, offering many entry points to the map for the reader to connect with. Locator maps in the news show where that earthquake happened, where that exotic tax haven is located, where in my city that bank was robbed and where exactly in the world that ongoing violent conflict is taking place.\nDevastation in Lombok\nA locator map showing the location of Lombok, part of the introduction of Devastation in Lombok, by Reuters Graphics.\nLocator maps let people assess how a news story is related to their own life. Did something happen close by? Did something happen in a country they've visited or where people they know live? Or did something happen in a country neighbouring a country they know or have some kind of connection to? Based on these questions, readers can quickly evaluate how relevant a story is to them. And a locator map makes this evaluation easier than providing a description in text only.\nSo, in what sense does the humble locator map lie to the reader? Well, locator maps usually are very small in order to be readable. This means that these maps leave out many details: sinuous roads become straight lines, smaller roads are left out (or roads are left out altogether) and a group of mountain peaks can be represented by a single symbol for a whole mountain ridge. In other words, these maps are heavily generalised, which limits their accuracy and broader use cases. Don't use them for navigational purposes, for example. You will get lost.\nThe breaking news map\nMaps can do more than show the ‘where’ of a story. When news breaks, the big challenge for journalists is to explain to their audiences how events unfolded. In many cases, the best way to do so is by using an annotated ‘breaking news’ map.\nA breaking news map showing what happened in Nice on 14 of July 2016, from What We Know After Terror Attack in Nice, France by the Wall Street Journal. Notice how the annotated map at the top is accompanied by two locator maps: the first one to situate Nice in France and the second one to situate the attack in the city of Nice.\nWhile locator maps only communicate 'something happened here', annotated maps can show a sequence of events and convey other information relevant to the story. People familiar with the location can mentally replay what happened by connecting what’s on the map with how they know the place.\nOften these maps use oblique, 3D-views of a city, so people unfamiliar with the location can still get a good sense of exactly what happened and how things looked on the ground. With Google Earth Pro you can generate these very detailed, oblique views for free.\nAn annotated map describing the events in Berlin on 19 December 2016 by Spiegel Online.\nBut be careful. Some people might think these 3D images are real pictures, taken at oblique angles from airplanes or helicopters. Remind them that they are not; instead, they are generated by Google Earth by 'draping' satellite images over a detailed 3D model of the Earth. In some cases this process leads to glitches, as the below ferris wheel in Scheveningen, the Netherlands, clearly demonstrates.\nThe Scheveningen ferris wheel, a glitch in Google’s 3D model of the earth.\nIt’s also important that readers and visual journalists remember that Google Earth images are typically a few months to a few years old. Suggesting that these images are 'live' or taken after the breaking news event took place would be lying.\nThe extent map\nLet’s move along to a map type that is definitely lying to the reader: the extent map. In order to communicate the size and magnitude of things around the world, these maps cut them out of their real geographical location and paste them into a foreign one.\nFor example, during the 2014 Winter Olympics in Sochi, the New York Times produced an extent map to illustrate the size of the site’s infrastructure, by copy-pasting these into the streets of Manhattan.\nLuge, Bobsled and Skeleton\nRacers might begin their starting sprints 40 stories up and several blocks north of Times Square for the run down the city’s own version of the Sanki Sliding Center’s track, finishing in a big turn on the plaza in front of the Armed Services Recruiting Center. Credit: Is That a Luge in Times Square? by the New York Times.\nMany readers lack good reference points for assessing how long a 400 meter ship really is, or how far 5.800 square kilometers really stretch out. By showing these objects and areas on maps that the reader can relate to, a direct connection can be made to their reference frames.\nWhen the iceberg A-68 broke away from the Larsen C ice shelf in Antarctica, many media compared it to the size of Delaware. For Americans familiar with the size of this state, this comparison was probably helpful. But for many readers the size of Delaware is just as abstract as 5.800 square kilometres, the actual size of the iceberg. That's why the Berliner Morgenpost built a little interactive extent map, which the reader can use to copy-paste the iceberg's silhouette to any number of familiar places.\nExtent maps are only useful when the area used for comparison is familiar to the reader. Otherwise the question of 'How big is that iceberg?' remains unanswered and the reader ends up with more questions, like 'How big is Delaware?'. Some readers may even be confused and think the iceberg (which is in a place far away from them) is located near Delaware (an equally remote place for some).\nThe before-after map\nA map type that is becoming increasingly popular in news stories today is the before-after map. While before-after images have a long history in the news, journalists have previously been limited to photographs taken from the ground. Before and after images are now mostly taken from space, by satellites circling the globe.\nUntil recently, detailed satellite images with a high temporal resolution were simply not available and as a result details were too blurry or time intervals between pictures were too wide to be useful. Today, a range of satellites with high resolution cameras fly over the same location with a frequency of once every week and sometimes even once per day. This allows for the detection and reconstruction of events like like deforestation, floods, droughts and the construction of buildings.\nNASA's Images of Change is an example of the powerful impact of before-after maps. Included in the gallery are images highlighting the impact of drought on Europe, forest fires in California, and hurricane damage in Puerto Rico.\nNewsrooms have also discovered the power of before-after satellite images. Often, the supplier of these images is Planet, a Silicon Valley satellite imaging company that offers daily high resolution pictures with its 'flock' of shoebox sized satellites. The company has already provided images for news stories about the construction of Chinese coal plants, the effect of drought on German vegetation, and the development of North Korean missiles.\nBefore-after maps are usually very explicit about the date images were taken, so there is no room for misleading there. But like all aerial imagery, these images are still flawed. For example, images are only useful for before-after maps when they are taken during daytime and on cloud-free days. Ever noticed that the sun is always shining in these images? For this reason, photos showing destruction after a big storm usually take some time to become available after the clouds have disappeared. Some parts of the world are also much cloudier than others, so clear satellite pictures for these regions are rare.\nAnd that's not all. All of these raw images need to be corrected with good colour correcting algorithms. Differences in the angle at which the sun is illuminating the scene, differences in atmospheric conditions, and the variations between cameras on board different satellites introduce biases and glitches in satellite images. Ignoring these differences, or trying to remove them with badly designed algorithms, will lead to misleading before-after images. Brown areas 'affected by drought' could well be looking a lot greener when processed incorrectly, for example.\nNumbers on maps\n'There are lies, damn lies, and statistics,' the saying goes. So what happens when you mix statistics with maps, which are distortions of reality by definition? You get numbers on maps, and those are really easy to screw up and can very easily mislead.\nThese maps don’t serve as general reference maps, using data to show geographical patterns about a certain topic instead. Most commonly, they are used by journalists during election times, to show where people voted and for whom.\nA 2016 election map showing voting patterns in Berlin. Interactive version by Berliner Morgenpost.\nMaps that show administrative areas shaded according to some data value are called choropleth maps. These are useful for showing geographic patterns in statistics that are collected at the level of administrative units; for example, the average age of the population for each country, or the share of impoverished population living in the municipalities of a country. \nOne common mistake when making choropleth maps is using absolute numbers instead of relative (or 'normalised') numbers. Values need to be scaled to the population inhabiting every administrative unit. If numbers are not scaled, the result is a map like the below (which I understand is a favourite of President Trump’s):\nTweeted on 11 May 2017: \"Spotted: A map to be hung somewhere in the West Wing\".\nOn this map, population density is not taken into account and, as a result, a lot of the Republican red dominates the map (that’s why President Trump likes this map so much). The millions of blue Democratic voters concentrated in the big cities on the eastern and western coast are not well represented on this map, because they live in a relatively small geographical area.\nMany techniques exist to overcome this problem, one of these solutions is called a cartogram.\nA US election map that scales the states to account for the number of voters living in it. Credit: The New York Times.\nWith this technique, densely populated areas are assigned more visual space on the map. But cartograms have their own downsides: they distort the geography considerably, as is clear in the example above.\nLying world maps\nProbably the biggest and most frequent lie in mapping refers to world maps that use the Mercator projection. Almost all online maps use the Mercator projection, but the projection is used in many static and offline world maps too.\nThe problem with the Mercator projection is that it distorts areas close to the poles enormously. Did you know Greenland isn’t really the same size of Africa? It’s actually 14 times smaller.\nBecause of these distortions, it is better to avoid the Mercator projection for maps showing areas close to the poles and world maps. This is the reason why Google Maps decided to switch to an orthographic projection when zooming out to a world view.\nAs a rule of thumb, journalists should try to avoid using the Mercator projection when making world maps. Good alternatives are the Robinson and Winkel-Tripel projections, or the recently developed Equal Earth projection, which respects areas throughout the whole map.\nWebGL-based maps\nToday's modern web browsers support WebGL, a technology that allows browsers to tap into a computer's graphics card, opening up a whole new range of mapping possibilities.\nWebGL makes it possible to rotate maps, tilt the camera view, and visualise data in three dimensions -- it's an exciting new toy. But all these fancy features make it harder for readers to assess the numbers behind a visualisation. Tilted views make features in the back look smaller, and even hidden or obscured by other features in front of them. And because the camera view can be rotated and flown around, the north is not always up in these maps. This may confuse readers.\nLet's look at an application of this technology in One belt, one road by The Financial Times. This story uses an animated map of the new Chinese Silk Road, which reacts as the user scrolls through the text. In this way, different features on the map can be highlighted by zooming and rotating the map to get the best view of each section of the new Silk Road. Although this feature helps step the reader through the story, it also means that North is not always up, which can be confusing for readers unfamiliar with the countries and cities shown on the map.\nThe ‘One belt, one road’ project by the Financial Times.\nThe adoption of WebGL by mapping tools like Mapbox and more recently kepler.gl has opened the door for WebGL driven maps that show numbers. These have already found their way into the media, as An Extremely Detailed Map of the 2016 Election by The New York Times shows.\nAn extremely detailed map of the 2016 election by The New York Times.\nEvery map is a lie\nMapmakers make a lot of design decisions in order to produce clear and useful maps. They leave things out, simplify things, highlight elements and put other elements in the background. Areas, shapes and lines are distorted and geographical features may be shifted out of place. Sometimes old imagery is used to show where recent events took place, and unlike in the real world the sun is always shining in satellite images.\nThe degrees of freedom in the design of a map are infinite and by changing the size of features, the colours, the layering, the composition and the projection of a map, a different story is told and other distortions are introduced.\nThis illustrates perfectly another point made by Professor Monmonier: \"A single map is but one of an indefinitely large number of maps that might be produced for the same situation or from the same data.\"\nMaps are powerful, but sometimes a misleading picture is generated. Mapmakers, as well as map readers, should be aware of their limitations. So, remember that all maps are a lie. But these are necessary lies.\nNow that you've learnt how maps can lie, expand your mapping skills by:\ntaking Maarten's video course Mapping for Journalism to create both static and interactive maps for your stories\nlearning a data visualisation trick, which allows you to represent tiny and bigger countries on the same chart\nexploring the community's favourite maps in this edition of Conversations with Data."
  },
  {
    "creator": "Walid Al-Saqaf",
    "title": "Data journalism on the blockchain",
    "link": "https://datajournalism.com/read/longreads/data-journalism-on-the-blockchain",
    "pubDate": "Thu, 04 Apr 2019 16:29:00 +0200",
    "isoDate": "2019-04-04T14:29:00.000Z",
    "content": "\n                                                                        <p>When we talk about blockchain and journalism, the focus is often on trust and sustainability. Micro-cryptocurrency payments, powered by blockchain, are being <a href=\"https://www.pbs.org/newshour/show/can-blockchain-help-fill-journalisms-funding-gaps\">explored</a> as a potential solution to the journalism industry's declining revenue streams. At the same time, the technology's ability to protect and verify information has been <a href=\"https://medium.com/s/welcome-to-blockchain/what-could-blockchain-do-for-journalism-dfd054beb197\">touted</a> as an antidote to censorship and fake news -- but is this really all blockchain offers journalists?</p>\n<p>Although it hasn't received as much attention, the transaction data that blockchain applications leave behind provides a potential goldmine for investigative journalists. Uses of blockchain range far and wide, including by <a href=\"https://www.scmp.com/business/companies/article/2147477/chinese-wine-connoisseurs-wondering-whats-their-bottle-its\">premium wine makers</a>, the <a href=\"https://theconversation.com/how-blockchain-is-strengthening-tuna-traceability-to-combat-illegal-fishing-89965\">tuna industry</a>, and as a means to combat <a href=\"https://www.newamerica.org/bretton-woods-ii/blockchain-trust-accelerator/around-the-blockchain-blog/fighting-fake-drugs-blockchain/\">counterfeit pharmaceuticals</a>, opening up a whole new world of data for journalists to explore.</p>\n<p>In academia, blockchain has already been used to <a href=\"https://arxiv.org/pdf/1801.07501.pdf\">identify</a> the people behind bitcoin transactions, <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3102645\">research illegal activities</a>, and more. Surely, if researchers can use blockchain as an investigative tool, data journalists can too.</p>\n<p><a href=\"/people/wsaqaf\">Walid Al-Saqaf</a> agrees. Previously a journalist, turned academic, Walid brings these two worlds together through his work as a Senior Lecturer in Media Technology and Journalism at Södertörn University in Stockholm, Sweden. He is also a Co-Founder and Vice President of the <a href=\"https://isoc-bsig.org/\">Internet Society Blockchain Special Interest Group</a>. We spoke to him to find out more about how data journalists can use blockchain in their reporting.</p>\n\n                                                                                                                                                                                                  \n                            <p><strong>In your own words, can you provide us with an overview of what blockchain technologies do, as well as some prominent applications of them? </strong></p>\n<p>As the underlying technology used by the bitcoin cryptocurrency, blockchain technology is a decentralised and distributed database system. I personally think that knowing the internal under-the-hood mechanism of how a blockchain works is not necessary to understand its applications and possible benefits. It may, in fact, be sufficient to say that blockchain is a new type of database that has three characteristics: </p>\n<ol>\n<li>It is distributed, meaning that a copy of the data contained on the blockchain is cloned on thousands of nodes. </li>\n<li>It is transparent, allowing you to see all transactions that have occurred since the blockchain was created and making any transaction traceable over time. </li>\n<li>It is immutable, meaning that it is not possible to change what is written on the blockchain. This is because every transaction is chained together with strong cryptography (hence the name 'blockchain'), creating a permanent archive.</li>\n</ol>\n<p>Blockchains, such as <a href=\"https://www.ethereum.org/\">ethereum</a>, allow the creation and execution of advanced smart contracts that automate processes based on predefined rules, effectively ending the need for intermediaries to handle the execution manually. For example, you can use a smart contract to have an automatically executed crowdfunding campaign. In this instance, the blockchain would continuously monitor for incoming donations, automatically forwarding the total to the beneficiary once the goal is met. If the goal is not met, it would allow donors to reclaim their donations by calling the smart contract after the deadline.</p>\n<p>But this is just one example. The applications of blockchain are many and range from use for exchange of digital assets to crowdfunding, storing and tracking real estate ownership information, electronic voting, securing healthcare records, preserving intellectual property rights, and much more.</p>\n\n                                                                      \n                            \n                                                                                                                  \n                            <p><strong>Aside from funding, what are the various use cases for blockchain in data journalism?</strong></p>\n<p>Blockchain technology can be used to combat fake news, preserving intellectual rights of content providers, investigating and tracking transactions, limiting bias and external influence, combating censorship, protecting whistleblowers, enhancing citizen journalism. </p>\n<p>To use blockchain technology effectively, it is necessary to have the technology diffused and adopted widely, which may take years. Nonetheless, I believe it is just a matter of time until we get to the point of having blockchain accepted in the mainstream.</p>\n<p><strong>On this last point, why are journalistic uses of blockchain contingent on it becoming more widespread?</strong></p>\n<p>In order to understand and deal with blockchains, journalists have to invest time and energy by getting the training, education and expertise necessary to use and analyse blockchain data. However, since blockchains remain at the very early stage of development, they are not yet scalable and certainly not yet ready for mass adoption. In order to reach critical mass, the technology needs to go through some structural changes and enhancements. Until that happens, journalists can still benefit from exploring some blockchains (like bitcoin and ethereum), but they may have to wait until blockchains become more widely spread to effectively use it for day-to-day activities. </p>\n<p><strong>What kind of stories would most benefit from leveraging blockchain technologies?</strong></p>\n<p>Data contained in blockchains is well-structured and relatively easy to access through a number of APIs and open-source tools. This means that all kinds of stories could benefit from using blockchains as a data resource. Once journalists have extracted this data, they can use it to undertake investigative journalism in the traditional sense. </p>\n<p>This means that it is possible for data journalists to extract and analyse blockchain data for many different stories that may be relevant to the public. For example, journalists could look at the rise in the number of cryptocurrency purchases in countries suffering from economic turmoil. Since cryptocurrencies are not linked to any particular government or central bank, they can theoretically survive a global financial crisis. So, an increase in purchases may indicate that people are purchasing bitcoin as a store of value or that people's trust in government is diminishing. </p>\n<p>Another scenario worth considering is the anticipated wider application of smart contracts to facilitate a plethora of automated services. Such a development can theoretically lead to the replacement of centralised platforms, such as Facebook and Airbnb with alternative blockchain-based systems that use smart contracts instead.</p>\n<p>Data journalists can also write stories describing overall activities of certain bitcoin wallet addresses by aggregating the information obtained and analysing thousands of transactions recorded on the public blockchains -- these are open and easily accessible for free via services, such as <a href=\"https://www.blockchain.com/explorer\">blockchain.info</a>.</p>\n\n                                                                      \n                            \n                                                                      \n                            <p><strong>Can you list some examples of data journalism that have already used blockchain technologies as a reporting tool?</strong></p>\n<p>Instead of giving a list of examples, I'd rather give an example of the two main types of data journalism stories that one can do by using blockchain as a reporting tool: micro and macro.</p>\n<p>In the micro case, a journalist would zoom in and follow the trail of transactions sent from or received by a particular wallet address. One example of such an approach is manifested in the <a href=\"https://qz.com/search/collins+wannacry/\">reporting</a> done for Quartz by Keith Collins on the WannaCry ransomeware attack.</p>\n\n                                                                                                  \n                                <p>A screenshot from Keith Collins’ blockchain-powered investigation into ransomware attacks. Live version <a href=\"https://qz.com/1016525/the-petya-ransomware-cyberattack-has-earned-hackers-20k-less-than-wannacry-in-its-first-24-hours/\">here</a>.</p>\n\n                                                                                        <p>Collins was able to know when any bitcoin address sent an amount to the three WannaCry addresses, which received more than $140,000 in bitcoin. He also identified when those hackers started cashing out by forwarding the received funds to other accounts. The fact that public blockchains are trackable makes it possible to trace back any payment to its original source address.</p>\n\n                                                                                                  \n                            <p>This is a feature that makes investigative journalism quite exciting, and also has the advantage of making it possible to prove the findings reached in the investigative story.</p>\n<p>The other type of reporting is macro, which tries to provide readers with an overview of aggregated information, instead of doing a deep analysis focused on only a few addresses. An example of this type is a report by Camila Russo for Bloomberg, which <a href=\"https://www.bloomberg.com/news/articles/2018-08-07/bitcoin-speculators-not-drug-dealers-dominate-crypto-use-now\">wrote</a> that the criminal activity on bitcoin in 2018 constituted roughly 10 percent compared to 90 percent in 2013. The report noted that this drop may have been due to the realisation that criminal bitcoin transactions can be tracked (as was the case when US law enforcement units succeeded in tracking and taking down the dark web marketplace called 'Silk Road'). </p>\n<p><strong>Going to the micro use case, what else can transaction data be used to reveal? It is limited to cryptocurrencies and 'follow the money' investigations, or can journalists use it as a source for a wider array of stories?</strong></p>\n<p>Depending on the blockchain and the type of data stored in the transaction, it can be both financial and non-financial information. Initially, it is logical to use bitcoin for tracking financial transactions and relationships between various nodes in the network since that is the most dominant use case for that blockchain.</p>\n\n                                                                      \n                            \n                                                                                                                  \n                            <p>There is also potential to explore smart contracts created on Ethereum to identify the level of activity and progress achieved for a particular crowdfunding campaign, for example. Looking at the utility of a <a href=\"https://forum.ethereum.org/discussion/18480/what-are-erc20-tokens\">Ethereum ERC20 token</a> more broadly, it's possible to know how many smart contracts were created and how they were used over time simply by accessing the blockchain data. This can provide journalists with very useful insights on usage patterns regardless of the amounts being sent or received.</p>\n<p>Furthermore, the bitcoin blockchain allows you to embed a small piece of text (maximum 40-bytes) using the OP_RETURN operator. This OP_RETURN operator is used to add text related to a particular transaction that could point to a URL, for example, or provide some clues or meaningful information as a permanent note connected to the transaction. In some <a href=\"https://docs.google.com/spreadsheets/d/1x8ipepeGTNWq8TacgB7tCeuFjH_NSVKvMOYWl3B19rs/\">research</a> that I am doing to understand the breadth and reach of the WannaCry malware attack, for example, I am investigating all the transactions that involved sending funds to three WannaCry addresses. To this end, I looked for any leads in the OP_RETURN values to identify if there were any unusual messages. Sure enough, I discovered one of the transactions had the OP_RETURN string \"Caution! WannaCry Address!!\", which indicated that this particular sender, who sent a very small amount, wanted to warn other potential victims to not send money to the wallet address and keep a permanent timestamped record on the blockchain with this information. You can find that transaction directly on the blockchain <a href=\"https://www.blockchain.com/btc/tx/f093bf69f5e85a21d67c6921d71a5abb2cfcb197c8256e39bf7ce033d61e5405?show_adv=true\">here</a>.</p>\n<p>In short, every blockchain is different and has unique ways of embedding text or calling functions of smart contracts, making it necessary to know exactly what a journalist needs to get before going about the data extraction and analysis.</p>\n<p><strong>Turning to the fake news use case, how can blockchain be used as a data verification tool? Do you have any examples where it has been used by data journalists in this way?</strong></p>\n<p>Yes, I have completed an extensive study about <a href=\"https://civil.co/\">Civil</a>, one of the most popular blockchain-based journalism projects. Civil aims to prevent disinformation by using cryptoeconomics to incentivise users so that they take action when they discover any form of fake news or other malpractices by a Civil newsroom. The dynamics of how this is possible are described in the <a href=\"https://civil.co/constitution/\">Civil Constitution</a>. In my research, I've taken a critical look at the project to see if the Civil newsroom provides a relative advantage over traditional newsrooms in this way. My conclusion is that it does, but on the condition that Civil users act rationally and predictably and that abuse of the platform cannot corrupt the whole system.</p>\n<p>One of the other promising uses of blockchain to combat fake news and disinformation is in the ability to record original content with an immutable proof of creation. Aside from the usual doctoring of images, one area that is causing headaches nowadays is the ease of manipulating videos <a href=\"http://fortune.com/2018/09/11/deep-fakes-obama-video/\">using 'deepfake' </a>technology. However, blockchain's ability to timestamp original content immutably makes it a viable way to help address this problem as demonstrated by <a href=\"https://truepic.com\">Truepic</a>, which is an app that uses blockchain to notarise images and videos when they are taken. This is done by storing metadata to <a href=\"https://ijnet.org/en/blog/truepic-app-lets-journalists-instantly-verify-images-videos\">certify the authenticity</a> of an original image/video taken using a particular mobile device. I think that such technologies can provide more confidence to readers, assuring them that what they are viewing is not fake. If someone attempts to manipulate the original copy, that could easily be detected as fake since it will either not have an entry on the blockchain or its entry will come at a later time because it is not possible to change the original entry.</p>\n\n                                                                                                                                                                                                   \n                            <p><strong>What level of technical skills do journalists need to start using blockchains in their reporting?</strong></p>\n<p>I believe it all depends on what level of sophistication journalists need to reach their objective. If the objective is to just identify trends in the cryptocurrency and initial coin offering (ICO) space for example, then they can utilise simple web-based API queries to extract the needed data that could thereafter be analysed using MS Excel or any other spreadsheet software. </p>\n<p>Data journalists may also consider enhancing their programming skills in order to tap into blockchains that can give access to their content via APIs. One example of a blockchain that may be of interest in this regard is <a href=\"https://steemit.com/\">Steemit</a>, a blockchain-based platform that allows users to monetise their own content through direct cryptocurrency donations.</p>\n\n                                                                                                  \n                                <p>As illustrated in <a href=\"https://smt.steem.com/smt-whitepaper.pdf\">this Steemit white paper</a>, authors get paid when readers upvote their posts.</p>\n\n                                                                                        <p><a href=\"https://journals.sagepub.com/doi/abs/10.1177/0165551517748290\">A study</a> by Mike Thelwall investigated whether Steemit works as an effective social news platform by rewarding users for social content and curation. This research investigated 925,092 posts to understand how much they earned and what drives members of Steemit to send reward payments to certain posts.</p>\n\n                                                                                                  \n                            <p>Data journalists with the right programming skills might consider undertaking similar research. But if the intention is to analyse millions of connections between nodes in bitcoin and how they formed over time, a higher coding skill level (perhaps in Python or PHP) is needed to communicate with the blockchain and extract the data and store it in a database efficiently. To analyse nodes, journalists also need to have a deeper understanding of network analysis, and the use of <a href=\"https://gephi.org/\">Gephi</a> or other network visualisation/analysis tools may be necessary.</p>\n<p><strong>How can data journalists start using blockchains as a data source?</strong></p>\n<p>Stemming from my belief that it is useful for data journalists to consider using public blockchains as a data source, it is high time to have journalists acquire the basic sets of skills that allow them to access, extract and analyse blockchain data. That's why I have personally started to work on an open source library, called the Data Journalism Github repo, or <a href=\"https://github.com/wsaqaf/ddjblocks\">DDJBlocks</a> (still in development) that could reduce the time and learning curve for journalists starting to look into bitcoin blockchain data for identifying potential stories. </p>\n<p>For demonstration purposes, I have put together a <a href=\"https://www.google.com/maps/d/edit?mid=1xieg8fqlBFqLA1IwTWNICgOsrcuBZoHa&amp;ll=3.954001708663455%2C0&amp;z=2\">Google map</a> showing where donations to Wikileaks have come from over the years. This was done by using DDJBlocks to extract the transactions that include payments to one of the earliest known <a href=\"https://www.blockchain.com/btc/address/1HB5XMLmzFVj8ALj6mfBsbifRoD4miY36v\">Wikileaks wallet addresses</a> on the bitcoin blockchain. Thereafter, those transactions with relay IP addresses had their Geolocation identified in the form of a city. Then, I calculated the sums of all amounts coming from each city and stored these on a <a href=\"https://docs.google.com/spreadsheets/d/1NsrJxsQjvpUHT331f1tJr9LXJEp3O9oBKyXL0X0AxvM/\">spreadsheet</a>, which in turn was directly inputted into the Google Maps web interface to overlay the data on the world map.</p>\n\n                                                                      \n                            <p>Walid’s map of Wikileaks donations, powered by blockchain data. Darker blues represent larger donations.</p>\n\n                                                                      \n                            <p>I have also used the tool to do a <a href=\"https://kumu.io/wsaqaf/pizza#pizza-trail/17skew2md5avvnyygj6rixuqknwkxaxfyq?focus=1\">network visualisation</a> on the address that was used to pay 10,000 bitcoins in 2010 (worth around $41 at the time) for two pizzas from Papa John's. Hanyecz's 10,000 bitcoins have spread over the last eight years, ballooning to a worth of over $65 million today. Marked by <a href=\"https://www.bloomberg.com/graphics/2018-bitcoin-anniversary/\">bloomberg.com</a> as a milestone in the decade-long history of bitcoin, that pizza purchase can arguably be considered as the first ever proof that a cryptocurrency with no central authority behind it could be used as a means of peer-to-peer payment system. It will be quite valuable for data journalists to detect and cover other milestones for bitcoin and the other cryptocurrencies in the years to come.</p>\n<p><strong>Any final thoughts?</strong></p>\n<p>I predict that this form of data journalism will be increasingly relevant as bitcoin and other public blockchains become more accepted in the mainstream and as blockchain adoption reaches new heights. The main challenge is to allocate sufficient time and resources, particularly by journalism educational institutes, so that students and researchers look into this field. It’s important that J-schools get ahead of the curve to equip the next generation of data journalists around the world. During my lectures at Södertörn University, I regularly bring up blockchain as a technology that students need to be aware of alongside traditional centralised database systems.</p>\n\n                                              \n                ",
    "contentSnippet": "When we talk about blockchain and journalism, the focus is often on trust and sustainability. Micro-cryptocurrency payments, powered by blockchain, are being explored as a potential solution to the journalism industry's declining revenue streams. At the same time, the technology's ability to protect and verify information has been touted as an antidote to censorship and fake news -- but is this really all blockchain offers journalists?\nAlthough it hasn't received as much attention, the transaction data that blockchain applications leave behind provides a potential goldmine for investigative journalists. Uses of blockchain range far and wide, including by premium wine makers, the tuna industry, and as a means to combat counterfeit pharmaceuticals, opening up a whole new world of data for journalists to explore.\nIn academia, blockchain has already been used to identify the people behind bitcoin transactions, research illegal activities, and more. Surely, if researchers can use blockchain as an investigative tool, data journalists can too.\nWalid Al-Saqaf agrees. Previously a journalist, turned academic, Walid brings these two worlds together through his work as a Senior Lecturer in Media Technology and Journalism at Södertörn University in Stockholm, Sweden. He is also a Co-Founder and Vice President of the Internet Society Blockchain Special Interest Group. We spoke to him to find out more about how data journalists can use blockchain in their reporting.\nIn your own words, can you provide us with an overview of what blockchain technologies do, as well as some prominent applications of them? \nAs the underlying technology used by the bitcoin cryptocurrency, blockchain technology is a decentralised and distributed database system. I personally think that knowing the internal under-the-hood mechanism of how a blockchain works is not necessary to understand its applications and possible benefits. It may, in fact, be sufficient to say that blockchain is a new type of database that has three characteristics: \nIt is distributed, meaning that a copy of the data contained on the blockchain is cloned on thousands of nodes. \nIt is transparent, allowing you to see all transactions that have occurred since the blockchain was created and making any transaction traceable over time. \nIt is immutable, meaning that it is not possible to change what is written on the blockchain. This is because every transaction is chained together with strong cryptography (hence the name 'blockchain'), creating a permanent archive.\nBlockchains, such as ethereum, allow the creation and execution of advanced smart contracts that automate processes based on predefined rules, effectively ending the need for intermediaries to handle the execution manually. For example, you can use a smart contract to have an automatically executed crowdfunding campaign. In this instance, the blockchain would continuously monitor for incoming donations, automatically forwarding the total to the beneficiary once the goal is met. If the goal is not met, it would allow donors to reclaim their donations by calling the smart contract after the deadline.\nBut this is just one example. The applications of blockchain are many and range from use for exchange of digital assets to crowdfunding, storing and tracking real estate ownership information, electronic voting, securing healthcare records, preserving intellectual property rights, and much more.\nAside from funding, what are the various use cases for blockchain in data journalism?\nBlockchain technology can be used to combat fake news, preserving intellectual rights of content providers, investigating and tracking transactions, limiting bias and external influence, combating censorship, protecting whistleblowers, enhancing citizen journalism. \nTo use blockchain technology effectively, it is necessary to have the technology diffused and adopted widely, which may take years. Nonetheless, I believe it is just a matter of time until we get to the point of having blockchain accepted in the mainstream.\nOn this last point, why are journalistic uses of blockchain contingent on it becoming more widespread?\nIn order to understand and deal with blockchains, journalists have to invest time and energy by getting the training, education and expertise necessary to use and analyse blockchain data. However, since blockchains remain at the very early stage of development, they are not yet scalable and certainly not yet ready for mass adoption. In order to reach critical mass, the technology needs to go through some structural changes and enhancements. Until that happens, journalists can still benefit from exploring some blockchains (like bitcoin and ethereum), but they may have to wait until blockchains become more widely spread to effectively use it for day-to-day activities. \nWhat kind of stories would most benefit from leveraging blockchain technologies?\nData contained in blockchains is well-structured and relatively easy to access through a number of APIs and open-source tools. This means that all kinds of stories could benefit from using blockchains as a data resource. Once journalists have extracted this data, they can use it to undertake investigative journalism in the traditional sense. \nThis means that it is possible for data journalists to extract and analyse blockchain data for many different stories that may be relevant to the public. For example, journalists could look at the rise in the number of cryptocurrency purchases in countries suffering from economic turmoil. Since cryptocurrencies are not linked to any particular government or central bank, they can theoretically survive a global financial crisis. So, an increase in purchases may indicate that people are purchasing bitcoin as a store of value or that people's trust in government is diminishing. \nAnother scenario worth considering is the anticipated wider application of smart contracts to facilitate a plethora of automated services. Such a development can theoretically lead to the replacement of centralised platforms, such as Facebook and Airbnb with alternative blockchain-based systems that use smart contracts instead.\nData journalists can also write stories describing overall activities of certain bitcoin wallet addresses by aggregating the information obtained and analysing thousands of transactions recorded on the public blockchains -- these are open and easily accessible for free via services, such as blockchain.info.\nCan you list some examples of data journalism that have already used blockchain technologies as a reporting tool?\nInstead of giving a list of examples, I'd rather give an example of the two main types of data journalism stories that one can do by using blockchain as a reporting tool: micro and macro.\nIn the micro case, a journalist would zoom in and follow the trail of transactions sent from or received by a particular wallet address. One example of such an approach is manifested in the reporting done for Quartz by Keith Collins on the WannaCry ransomeware attack.\nA screenshot from Keith Collins’ blockchain-powered investigation into ransomware attacks. Live version here.\nCollins was able to know when any bitcoin address sent an amount to the three WannaCry addresses, which received more than $140,000 in bitcoin. He also identified when those hackers started cashing out by forwarding the received funds to other accounts. The fact that public blockchains are trackable makes it possible to trace back any payment to its original source address.\nThis is a feature that makes investigative journalism quite exciting, and also has the advantage of making it possible to prove the findings reached in the investigative story.\nThe other type of reporting is macro, which tries to provide readers with an overview of aggregated information, instead of doing a deep analysis focused on only a few addresses. An example of this type is a report by Camila Russo for Bloomberg, which wrote that the criminal activity on bitcoin in 2018 constituted roughly 10 percent compared to 90 percent in 2013. The report noted that this drop may have been due to the realisation that criminal bitcoin transactions can be tracked (as was the case when US law enforcement units succeeded in tracking and taking down the dark web marketplace called 'Silk Road'). \nGoing to the micro use case, what else can transaction data be used to reveal? It is limited to cryptocurrencies and 'follow the money' investigations, or can journalists use it as a source for a wider array of stories?\nDepending on the blockchain and the type of data stored in the transaction, it can be both financial and non-financial information. Initially, it is logical to use bitcoin for tracking financial transactions and relationships between various nodes in the network since that is the most dominant use case for that blockchain.\nThere is also potential to explore smart contracts created on Ethereum to identify the level of activity and progress achieved for a particular crowdfunding campaign, for example. Looking at the utility of a Ethereum ERC20 token more broadly, it's possible to know how many smart contracts were created and how they were used over time simply by accessing the blockchain data. This can provide journalists with very useful insights on usage patterns regardless of the amounts being sent or received.\nFurthermore, the bitcoin blockchain allows you to embed a small piece of text (maximum 40-bytes) using the OP_RETURN operator. This OP_RETURN operator is used to add text related to a particular transaction that could point to a URL, for example, or provide some clues or meaningful information as a permanent note connected to the transaction. In some research that I am doing to understand the breadth and reach of the WannaCry malware attack, for example, I am investigating all the transactions that involved sending funds to three WannaCry addresses. To this end, I looked for any leads in the OP_RETURN values to identify if there were any unusual messages. Sure enough, I discovered one of the transactions had the OP_RETURN string \"Caution! WannaCry Address!!\", which indicated that this particular sender, who sent a very small amount, wanted to warn other potential victims to not send money to the wallet address and keep a permanent timestamped record on the blockchain with this information. You can find that transaction directly on the blockchain here.\nIn short, every blockchain is different and has unique ways of embedding text or calling functions of smart contracts, making it necessary to know exactly what a journalist needs to get before going about the data extraction and analysis.\nTurning to the fake news use case, how can blockchain be used as a data verification tool? Do you have any examples where it has been used by data journalists in this way?\nYes, I have completed an extensive study about Civil, one of the most popular blockchain-based journalism projects. Civil aims to prevent disinformation by using cryptoeconomics to incentivise users so that they take action when they discover any form of fake news or other malpractices by a Civil newsroom. The dynamics of how this is possible are described in the Civil Constitution. In my research, I've taken a critical look at the project to see if the Civil newsroom provides a relative advantage over traditional newsrooms in this way. My conclusion is that it does, but on the condition that Civil users act rationally and predictably and that abuse of the platform cannot corrupt the whole system.\nOne of the other promising uses of blockchain to combat fake news and disinformation is in the ability to record original content with an immutable proof of creation. Aside from the usual doctoring of images, one area that is causing headaches nowadays is the ease of manipulating videos using 'deepfake' technology. However, blockchain's ability to timestamp original content immutably makes it a viable way to help address this problem as demonstrated by Truepic, which is an app that uses blockchain to notarise images and videos when they are taken. This is done by storing metadata to certify the authenticity of an original image/video taken using a particular mobile device. I think that such technologies can provide more confidence to readers, assuring them that what they are viewing is not fake. If someone attempts to manipulate the original copy, that could easily be detected as fake since it will either not have an entry on the blockchain or its entry will come at a later time because it is not possible to change the original entry.\nWhat level of technical skills do journalists need to start using blockchains in their reporting?\nI believe it all depends on what level of sophistication journalists need to reach their objective. If the objective is to just identify trends in the cryptocurrency and initial coin offering (ICO) space for example, then they can utilise simple web-based API queries to extract the needed data that could thereafter be analysed using MS Excel or any other spreadsheet software. \nData journalists may also consider enhancing their programming skills in order to tap into blockchains that can give access to their content via APIs. One example of a blockchain that may be of interest in this regard is Steemit, a blockchain-based platform that allows users to monetise their own content through direct cryptocurrency donations.\nAs illustrated in this Steemit white paper, authors get paid when readers upvote their posts.\nA study by Mike Thelwall investigated whether Steemit works as an effective social news platform by rewarding users for social content and curation. This research investigated 925,092 posts to understand how much they earned and what drives members of Steemit to send reward payments to certain posts.\nData journalists with the right programming skills might consider undertaking similar research. But if the intention is to analyse millions of connections between nodes in bitcoin and how they formed over time, a higher coding skill level (perhaps in Python or PHP) is needed to communicate with the blockchain and extract the data and store it in a database efficiently. To analyse nodes, journalists also need to have a deeper understanding of network analysis, and the use of Gephi or other network visualisation/analysis tools may be necessary.\nHow can data journalists start using blockchains as a data source?\nStemming from my belief that it is useful for data journalists to consider using public blockchains as a data source, it is high time to have journalists acquire the basic sets of skills that allow them to access, extract and analyse blockchain data. That's why I have personally started to work on an open source library, called the Data Journalism Github repo, or DDJBlocks (still in development) that could reduce the time and learning curve for journalists starting to look into bitcoin blockchain data for identifying potential stories. \nFor demonstration purposes, I have put together a Google map showing where donations to Wikileaks have come from over the years. This was done by using DDJBlocks to extract the transactions that include payments to one of the earliest known Wikileaks wallet addresses on the bitcoin blockchain. Thereafter, those transactions with relay IP addresses had their Geolocation identified in the form of a city. Then, I calculated the sums of all amounts coming from each city and stored these on a spreadsheet, which in turn was directly inputted into the Google Maps web interface to overlay the data on the world map.\nWalid’s map of Wikileaks donations, powered by blockchain data. Darker blues represent larger donations.\nI have also used the tool to do a network visualisation on the address that was used to pay 10,000 bitcoins in 2010 (worth around $41 at the time) for two pizzas from Papa John's. Hanyecz's 10,000 bitcoins have spread over the last eight years, ballooning to a worth of over $65 million today. Marked by bloomberg.com as a milestone in the decade-long history of bitcoin, that pizza purchase can arguably be considered as the first ever proof that a cryptocurrency with no central authority behind it could be used as a means of peer-to-peer payment system. It will be quite valuable for data journalists to detect and cover other milestones for bitcoin and the other cryptocurrencies in the years to come.\nAny final thoughts?\nI predict that this form of data journalism will be increasingly relevant as bitcoin and other public blockchains become more accepted in the mainstream and as blockchain adoption reaches new heights. The main challenge is to allocate sufficient time and resources, particularly by journalism educational institutes, so that students and researchers look into this field. It’s important that J-schools get ahead of the curve to equip the next generation of data journalists around the world. During my lectures at Södertörn University, I regularly bring up blockchain as a technology that students need to be aware of alongside traditional centralised database systems."
  },
  {
    "creator": "Catherine D'Ignazio",
    "title": "Putting data back into context",
    "link": "https://datajournalism.com/read/longreads/putting-data-back-into-context",
    "pubDate": "Thu, 04 Apr 2019 17:02:00 +0200",
    "isoDate": "2019-04-04T15:02:00.000Z",
    "content": "\n                                                                        <p>What happens when an institution collects data about something in the world? The origin of the word <em>data</em> actually means 'that which is given'. And this is typically how newcomers regard data – as a somewhat neutral recording of facts at hand. The information was there, and then an institution collected and stored it. When data journalists investigate an issue, we look for who might have data, how we can acquire those data, and then use them to create new insights into the world.</p>\n<p>But the scholar Johanna Drucker proposes a different word for data: <em>capta</em>. By this, she means, 'that which is taken'. As Johanna states in her paper,<a href=\"http://peterahall.com/mapping/Drucker_graphesis_2011.pdf\"> Graphesis: Visual knowledge production and representation</a>, \"Data are considered objective 'information' while capta is information that is captured because it conforms to the rules and hypothesis set for the experiment\".</p>\n<p>This distinction might seem academic for data journalists, but in fact it's at the root of why <em>context</em> matters so deeply for data journalism. Thinking of data as <em>capta</em> invites us to consider why an institution invested their resources in collecting information, how the institution uses that information, who the information benefits (and who it doesn't), and what the potential limitations of the information are. In short, it points us back to how data are never neutral 'givens', but always situated in a particular context, collected for a particular reason. In Lauren Klein and I's book, called <a href=\"https://bookbook.pubpub.org/data-feminism\">Data Feminism</a>, we devote an entire chapter to the importance of considering context, particularly when the collection environment has any kind of power imbalances.</p>\n\n                                                                                                                                <p>Catherine D'Ignazio is an Assistant Professor of Data Visualization &amp; Civic Media at Emerson College and a research affiliate at the MIT Center for Civic Media and the MIT Media Lab.</p>\n\n                                                                                                                              \n                            <h2>Why context is hard</h2>\n<p>Establishing and understanding the context of your data (capta) is likely one of the single most challenging aspects of doing data journalism. It's like starting out with the leaves of a tree and then trying to connect them back to their branches and roots. But why is context so hard?</p>\n<p>First of all, data are typically collected by institutions for internal purposes and they're not intended to be used by others. As veteran data reporter Tim Henderson, quoting Drew Sullivan, said to the<a href=\"https://www.ire.org/resource-center/listservs/subscribe-nicar-l/\"> NICAR community</a>, \"Data exists to serve the bureaucracy, not the journalist\". The naming, structure and organisation of most datasets are done from the perspective of the institution, not from the perspective of a journalist looking for a story. For example, one semester my students spent several weeks trying to figure out the difference between the columns 'PROD.<em>WASTE</em>(8.1_THRU_8.7)' and '8.8_ONE-TIME_RELEASE' in a dataset tracking the release of toxic chemicals into to the environment by certain corporations. This is not an uncommon occurrence!</p>\n<p>And while the open data movement has led to governments launching more open data portals and APIs, these efforts have prioritised publishing data over publishing the metadata that would actually make the data more useful to outsiders. Part of this is cost-related -- context is expensive. The cities of<a href=\"https://knightfoundation.org/articles/how-city-boston-making-its-data-accessible-everyone\"> Boston</a> and<a href=\"http://www.govtech.com/data/How-Chicagos-Data-Dictionary-is-Enhancing-Open-Government.html\"> Chicago</a> both had to secure external grants from foundations in order to embark on comprehensive metadata projects to annotate the columns of the open datasets on their portals and make their datasets easier to search and find.</p>\n<p>But sometimes, the lack of attention to usability, context and metadata works in favour of the collecting institution, which may have reasons for why it doesn't want certain information to become public. For example, the Boston Police Department (BPD) runs a programme called Field Interrogation and Observation (FIO). For all intents and purposes, this is a stop and frisk programme, in which police log their encounters –--observations, stops, frisks, interrogations -- with private individuals on the streets. In 2014, following a lawsuit won by the American Civil Liberties Union, the BPD was obligated to release their FIO data publicly on Boston's data portal. But when you search for ‘stop frisk’ on the portal, nothing comes up. Journalists and members of the public would need to know the bureaucratic term for the programme (FIO) in order to be able to <a href=\"https://data.boston.gov/dataset/boston-police-department-fio\">locate it on the portal</a>.</p>\n\n                                                                      \n                            <p>Searching for ‘stop frisk’ in the City of Boston's open data portal yields no results. Those searching for data would have to know that the programme is called ‘Field Interrogation and Observation’.</p>\n\n                                                                      \n                            <p>Furthermore, some institutions may publish their data and metadata freely, but be less forthcoming about their data's limitations. This can lead to serious cases of misuse and misrepresentation of data. In one chapter of Data Feminism, The Numbers Don't Speak for Themselves, Lauren Klein and I discuss the case of GDELT: the Global Database for Events, Language and Tone. In a high-profile correction, FiveThirtyEight had to retract a story about the kidnappings of Nigerian girls that used the GDELT database. They had mistakenly used media reports about kidnappings to tell a story about the incidence of kidnappings. While FiveThirtyEight should have verified their data before publishing, we describe how GDELT, because of their pressure to attract big data scientific research funding, failed to describe the limitations of their ‘events’ data (which is not events data at all, but rather ‘media reports about events’ data).</p>\n\n                                                                      \n                            <h2>The Three-Step Context Detective</h2>\n<p>So, what's a data journalist to do? She has to become a ‘context detective’, working with data that have been captured from the world into spreadsheets and databases, and connecting them back into their collection environment. This work is similar to that of a detective -- the journalist has to use incomplete clues that point backwards to the bureaucratic functionings of the collecting institution.</p>\n<p>To understand the data, you must understand the who, what, when, where, why, and how of that bureaucracy. Below I present a process called the Three-Step Context Detective, which I use in the classroom. These steps don't necessarily have to be completed in this order.</p>\n\n                                                                      \n                            <h3>1. Download the data and get orientated</h3>\n<p>Looking at hundreds, thousands or hundreds of thousands of obliquely named rows and columns can be daunting at first. Sometimes newcomers think that the data science ‘wizards’ can just look at a spreadsheet and see a story emerge. Nothing could be further from the truth. Getting oriented in your dataset involves breaking down the basics systematically so that you can ask good questions.</p>\n<p>You can use a programme like Excel or Google Sheets to do basic exploration to answer questions like:</p>\n<ul>\n<li>How many observations (rows of data) do you have?</li>\n<li>How many fields (columns) do you have?</li>\n<li>Is it clear what each row is counting? (Remember those incidences of kidnapping versus media reports about kidnapping – getting crystal clear about what your data is logging is supremely important.)</li>\n<li>What is the time period of the data? Use the ‘Sort’ function on any column with dates or timestamps to see when the data begin and when they end.</li>\n<li>What is the geographic extent of the data?</li>\n<li>Does there appear to be a lot of missing data?</li>\n</ul>\n\n                                                                                                                                 <p>For more on using spreadsheets, check out Brant Houston's article <a href=\"https://datajournalism.com/read/longreads/spreadsheets-for-journalism\">Spreadsheets for journalism</a>, or our video courses <a href=\"https://datajournalism.com/watch/doing-journalism-with-data-first-steps-skills-and-tools\">Doing Journalism with Data: First Steps, Skills and Tools</a> and <a href=\"https://datajournalism.com/watch/cleaning-data-in-excel\">Cleaning Data in Excel</a>. You can also start a conversation in our <a href=\"https://discuss.datajournalism.com/\">forums</a>.</p>\n\n                                                                                                                              \n                            <p>This process of getting oriented with a new dataset is no small task. In fact, Rahul Bhargava and I built a free, online tool called <a href=\"https://databasic.io/en/wtfcsv/\">WTFcsv</a> which captures the emotion that journalists often feel when looking at a new spreadsheet: \"WTF is going on with my csv file?!\" Using WTFcsv, you can continue your orientation process.</p>\n<p>WTFcsv analyses each column from a spreadsheet file and returns a data visualisation that summarises the patterns in each column. For example, the image below depicts data about passengers on the Titanic. The column ‘Sex’ is rendered as a column chart that demonstrates, visually, that there were 314 female and 577 male passengers logged on the Titanic.</p>\n\n                                                                      \n                            \n                                                                      \n                            <p>Rahul and I talk about the importance of asking good questions of your data before you conduct analysis or tell stories. WTFcsv can help you answer all of the basic questions above, as well as start to form your own questions about the dataset in front of you. For example, for the Titanic data, good questions might be about ethics (‘Why is 'Sex' a binary variable?’), data formatting (‘What does the 'Parch' column mean?’), data quality (‘Is this data complete?’), or data analysis (‘Did women survive at a higher rate than men?’).</p>\n<p>It's important to write down all of these questions, because as you go through the next couple steps, you can try to answer them.</p>\n\n                                                                      \n                            <h3>2. Explore all available metadata</h3>\n<p>Metadata is data about data and can be your golden ticket to establishing context for a dataset. In an ideal world, any dataset that you download would have a detailed and up-to-date data dictionary. This is a document that provides a column-by-column description of the dataset, along with guidelines for using it.</p>\n\n                                                                      \n                            \n                                                                      \n                            <p>In the above example, from the <a href=\"https://deepseacoraldata.noaa.gov/internal-documents/program-guidance/science-team-guidance-for-data-management/20170707.xlsx/view\">NOAA National Database of Deep-Sea Corals and Sponges</a>, each field (column) in the dataset is annotated and explained, along with descriptions of data quality, units of measurement, completeness, and usage guidelines.</p>\n<p>Seeking metadata is not always easy or successful. Not all data providers produce data dictionaries. In 2016, journalist J. Albert Bowden sought documentation on the fields in a dataset from the US Department of Agriculture. He was told that explanation of their column headers was a proprietary secret. Moreover, even when there are metadata, providers might fail to call the file ‘data dictionary’. For example, if you use <a href=\"https://data.boston.gov/dataset/311-service-requests\">the City of Boston's 311 data</a>, the data dictionary is called ‘CRM Value Codex’ – not the most attractive and user-friendly name ever.</p>\n<p>And sometimes data dictionaries or other forms of metadata might be outdated because the institution fails to update them when the dataset changes. It's important to have your sceptical, fact-checking, data-verifying journalist hat on at all times.</p>\n\n                                                                      \n                            <h3>3. Background the dataset</h3>\n<p>Journalists often ‘background’ a person or ‘background’ an issue, and likewise the final step of the Three-Step Context Detective is to background your dataset. This may be the most time-consuming aspect of establishing context for your data, but it is well worth the investment in terms of understanding limitations, preventing errors, and discovering newsworthy stories and analysis. In this process, there are at least three separate things to conduct background research on.</p>\n<p><strong>Background the collection process</strong></p>\n<p>Newcomers (as well as old-timers!) can forget that data collection is often a human, material process. Data science consultant Heather Krause advocates for creating <a href=\"https://gijn.org/2017/03/27/data-biographies-getting-to-know-your-data/\">data biographies</a> that describe where the data came from, who collected them, and how they collected them. In the case of the Boston stop and frisk programme discussed above, police officers fill out paper forms after having an encounter with a resident on the street. Then, those forms get turned into the precinct and a staff member logs the values in a database. Before publishing to the website, other staff members remove personally identifying information. It's all very mundane, but absolutely essential to understanding where errors and missing data can be introduced. The meat is in the bureaucratic details like whether data is self-reported, or observed, or measured by a machine. Like what database the organisation uses to store the data. Like whether the way the organisation is counting and measuring has changed recently (making current data and prior data not comparable).</p>\n<p><iframe width=\"894\" height=\"503\" src=\"https://www.youtube-nocookie.com/embed/yCuRQc4xuhA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>\n\n                                                                      \n                            <p>After exploring any available metadata, your best path to backgrounding the collection process is finding a human being to talk to. This might be someone from the collecting organisation, but it's important to think creatively about interviewees when this is not feasible.</p>\n\n                                                                      \n                            <p>After exploring any available metadata, your best path to backgrounding the collection process is finding a human being to talk to. This might be someone from the collecting organisation, but it's important to think creatively about interviewees when this is not feasible. For example, in the case of stop and frisk data, it's hard to get police spokespeople on the phone. Other potential interviewees might include the youth of colour who are disproportionately stopped, the ACLU who sued the police department and did their own background research on the collection process, or criminal justice scholars who have studied the Boston programme. Krause has a helpful<a href=\"https://www.dropbox.com/s/uau1bvjbjvvwgqa/Datassist%20Data%20Biography%20Template.xlsx?dl=0\"> data biography template</a> that you can fill out for this process.</p>\n<p><strong>Background the organisation</strong></p>\n<p>While we have talked a lot about understanding <em>datasets</em>, Yanni Loukissas makes the case in his book,<a href=\"https://mitpress.mit.edu/books/all-data-are-local\"> All Data Are Local</a>, that to use data effectively we also need to understand <em>data settings</em>. Backgrounding the dataset is not just about the data itself – it's also about understanding why an organisation was motivated to collect it in the first place, as well as how they use it.</p>\n<p>In the case of the stop and frisk data, this means doing background research on the Boston Police Department: What is their mission? How long they have existed? What is their budget? How many officers are there? When have they been in the news in the last ten years and why? It also means researching the FIO programme specifically: When and why did the BPD start the programme? Was it part of a national wave of FIO programmes? Is there scholarly and legal debate on whether these programmes are constitutional and effective in reducing crime?</p>\n<p>From this understanding of the underpinning organisational and programmatic goals, it's helpful to try to understand how the organisation uses the data it collects internally. For example, does the BPD use its logs of police-civilian encounters to try to limit racial profiling? Do officers have quotas? Who does the BPD have to report their numbers to?</p>\n<p>Here, again, interviews with real, live human beings are going to be one of the most effective ways of getting information about the organisation's motivations and uses of the data it collects. But when you can't get an inside interview, one of the best ways to find this information is to:</p>\n<p><strong>Background the regulatory environment</strong></p>\n<p>Data is expensive to collect, maintain, and organise. Most organisations don't collect data because they want to but rather because they have to, either to comply with laws or with internal policies. Doing background research on the regulatory environment can often shed light on why the organisation collects data, who it reports that data to, and how it reports the data. For example, all accredited higher education institutions in the US have to collect and report data about sexual assault on college campuses because of the Jeanne Clery Disclosure of Campus Security Policy and the Campus Crime Statistics Act (Clery Act).</p>\n<p>It is typically easier to background federal and state laws, which tend to be available publicly or identified via talking with lawyers and others with legal knowledge. Internal policy documents that guide data collection can be harder, albeit not impossible to access. If you live in a country with public records laws, using those laws to request organisational governance documents and training manuals can be an excellent way to understand the internal regulatory context that guides information collection. As an example, most police departments in the US collect data on the use of force by police officers against civilians. Knowing this, when a white male police officer used excessive force at a pool party in 2015, reporters at MuckRock made a public records request for<a href=\"https://www.muckrock.com/foi/mckinney-2926/mckinney-tx-police-department-use-of-force-policy-and-training-materials-18593/#file-47578\"> the McKinney police use of force policy</a>. On page nine, the policy details when and why officers are required to file a 'Response to Resistance' report (RTR) and who is responsible for maintaining those reports. This policy would be essential background information for any journalist seeking to write a data story about use of force from RTR data.</p>\n\n                                                                      \n                            <h2>Pitfalls</h2>\n<p>So, the Three-Step Context Detective consists of getting oriented, exploring all metadata and backgrounding the dataset (including the collection, the organisation, and the regulatory environment). In the process of building out these connections between the dataset and its broader context, there are two pitfalls to keep your eyes on.</p>\n\n                                                                                                  \n                                <p>There are many ‘Unknowns’ in the <a href=\"https://data.cityofnewyork.us/Health/NYC-Dog-Licensing-Dataset/nu7n-tubp\">2016 dog licensing data from the City of New York</a>. We need to be careful not to make assumptions like ‘Unknown’ means ‘Mixed breed’.</p>\n\n                                                                                        <p>First, beware of your own brain and its penchant for making assumptions to fill in unknowns. It is tempting when looking at columns in a dataset to imagine that you know what they mean, but this can be dangerous. In my data journalism class, we were working with data about<a href=\"https://data.cityofnewyork.us/Health/NYC-Dog-Licensing-Dataset/nu7n-tubp\"> the dogs of New York City</a> -- their breeds, ages, and sex.</p>\n\n                                                                                                  \n                            <p>Of course, some fields had incomplete data and 'breed' was one of those with many 'UNKNOWN' values in the column. One student assumed that breed = UNKNOWN meant that the dog was mixed breed and built their whole story around that incorrect assumption ('UNKNOWN' means the information wasn't filled out by the applicant so we literally do not know the breed). Luckily, the student did end up checking their assumptions and revising the story, and the data itself was fairly low stakes. That said, this illustrates the importance of<a href=\"https://towcenter.org/research/the-curious-journalists-guide-to-data/\"> Jonathan Stray's advice</a> about 'considering multiple explanations for the same data, rather than just accepting the first explanation that makes sense'. The same advice applies when assembling the context around your data just as much as it applies when analysing it.</p>\n<p>Secondly, it's important to remember that power is not equally distributed across the collection process, the organisation, and the regulatory environment. The result of social inequalities in the data setting is that the numbers may appear to tell one story on first exploration, but that story might be completely false because the collection environment has systematically silenced people with less power. What does this mean?</p>\n<p>In <a href=\"https://bookbook.pubpub.org/pub/6ui5n4vo\">The Numbers Don't Speak for Themselves</a>, Lauren Klein and I discuss a story written by three of my students about sexual assault data provided by the Clery Act. What the students found is that campuses with high numbers of sexual assault were not hotbeds of rape culture, instead these campuses were actually providing the most resources and the most supportive environment for survivors to come forward. So, paradoxically, the campuses with the lowest rates of sexual assault were <em>not</em> doing great but rather creating an environment that actively discouraged survivors to report. Meanwhile, those campuses with higher numbers were actually measuring sexual assault closer to what the reality of its incidence is. This kind of pattern will be visible anytime that structural forces like patriarchy, racism, or classism are at work (read: all the time) that lead to the systematic undercounting or overcounting of women and other marginalised groups. The way to address it is through establishing context – the students discovered this through background research, reviewing policy docs and many interviews – rather than accepting the numbers at face value.</p>\n\n                                                                      \n                            <h3>Opportunities</h3>\n<p>Just as there are pitfalls for context, there are also opportunities for journalists and news organisations to create useful resources for readers and other journalists from their work on context. And context is work! Instead of writing a single story from a data exploration, organisations like ProPublica have started to create what Scott Klein calls 'news apps', that is, evergreen resources like<a href=\"https://projects.propublica.org/docdollars/\"> Dollars for Docs</a>. While it is useful for individual readers, Dollars for Docs has also become a data resource for other news organisations to write their own, localised stories on the influence of pharmaceutical companies – for example,<a href=\"https://www.stltoday.com/news/local/metro/database-has-drug-firms-payments-to-doctors/article_0cd2662c-970e-5311-ad0f-61418464cb33.html\"> this story</a> about the effect of pharma money on doctors in St. Louis, Missouri. In this sense, ProPublica has become known as an 'information intermediary', by turning their original investigation's context and data into a resource that is reusable for other organisations.</p>\n\n                                                                      \n                            <p>ProPublica turns the context work that they do compiling and backgrounding datasets into a source of revenue in their <a href=\"https://www.propublica.org/datastore/\">data store</a>.</p>\n\n                                                                      \n                            <p>Verified data and expert contextual information can also become a source of revenue for news organisations. ProPublica maintains <a href=\"https://www.propublica.org/datastore/\">a data store</a> where you can purchase datasets on a variety of topics. Many of the datasets available come with excellent ‘data user guides’ – a term coined by Bob Gradeck, manager of the Western Pennsylvania Regional Data Center. In his work promoting open data, he saw the need for metadata that goes beyond the data dictionary to provide a narrative account of a dataset - where it comes from, how it is used by the organisation, and what its limitations are. Examples of Bob’s work can be seen in the data user guides for <a href=\"https://docs.google.com/document/d/1S4uouiP1X06BhJTf-9YPXSAuCDlhousYOn6EpMe-9RM/edit\">311 data in Pittsburgh</a>.</p>\n<p>The Associated Press (AP) is also getting into the <em>data + context = revenue</em> game. They spent extensive time compiling<a href=\"https://www.propublica.org/datastore/dataset/school-segregation-charter-district-data\"> a national database on school segregation in the US</a>, which comes with a 20-page data user guide including where the data is collected from and what kinds of questions it can be used to answer. It's available for purchase, and the AP is starting to develop a subscription model where organisations can pay for access to other datasets, context, and discussions with reporters who worked on those issues.</p>\n\n                                                                      \n                            <h2>Conclusion</h2>\n<p>The bottom line is that putting data back into context is laborious but absolutely necessary work for telling data stories. Context is the way to get the story right, even in the face of meager metadata, bureaucratic obstacles, and power imbalances in the data setting. Do you have stories about your work with context and data? Share them in our forums.</p>\n\n                                              \n                ",
    "contentSnippet": "What happens when an institution collects data about something in the world? The origin of the word data actually means 'that which is given'. And this is typically how newcomers regard data – as a somewhat neutral recording of facts at hand. The information was there, and then an institution collected and stored it. When data journalists investigate an issue, we look for who might have data, how we can acquire those data, and then use them to create new insights into the world.\nBut the scholar Johanna Drucker proposes a different word for data: capta. By this, she means, 'that which is taken'. As Johanna states in her paper, Graphesis: Visual knowledge production and representation, \"Data are considered objective 'information' while capta is information that is captured because it conforms to the rules and hypothesis set for the experiment\".\nThis distinction might seem academic for data journalists, but in fact it's at the root of why context matters so deeply for data journalism. Thinking of data as capta invites us to consider why an institution invested their resources in collecting information, how the institution uses that information, who the information benefits (and who it doesn't), and what the potential limitations of the information are. In short, it points us back to how data are never neutral 'givens', but always situated in a particular context, collected for a particular reason. In Lauren Klein and I's book, called Data Feminism, we devote an entire chapter to the importance of considering context, particularly when the collection environment has any kind of power imbalances.\nCatherine D'Ignazio is an Assistant Professor of Data Visualization & Civic Media at Emerson College and a research affiliate at the MIT Center for Civic Media and the MIT Media Lab.\nWhy context is hard\nEstablishing and understanding the context of your data (capta) is likely one of the single most challenging aspects of doing data journalism. It's like starting out with the leaves of a tree and then trying to connect them back to their branches and roots. But why is context so hard?\nFirst of all, data are typically collected by institutions for internal purposes and they're not intended to be used by others. As veteran data reporter Tim Henderson, quoting Drew Sullivan, said to the NICAR community, \"Data exists to serve the bureaucracy, not the journalist\". The naming, structure and organisation of most datasets are done from the perspective of the institution, not from the perspective of a journalist looking for a story. For example, one semester my students spent several weeks trying to figure out the difference between the columns 'PROD.WASTE(8.1_THRU_8.7)' and '8.8_ONE-TIME_RELEASE' in a dataset tracking the release of toxic chemicals into to the environment by certain corporations. This is not an uncommon occurrence!\nAnd while the open data movement has led to governments launching more open data portals and APIs, these efforts have prioritised publishing data over publishing the metadata that would actually make the data more useful to outsiders. Part of this is cost-related -- context is expensive. The cities of Boston and Chicago both had to secure external grants from foundations in order to embark on comprehensive metadata projects to annotate the columns of the open datasets on their portals and make their datasets easier to search and find.\nBut sometimes, the lack of attention to usability, context and metadata works in favour of the collecting institution, which may have reasons for why it doesn't want certain information to become public. For example, the Boston Police Department (BPD) runs a programme called Field Interrogation and Observation (FIO). For all intents and purposes, this is a stop and frisk programme, in which police log their encounters –--observations, stops, frisks, interrogations -- with private individuals on the streets. In 2014, following a lawsuit won by the American Civil Liberties Union, the BPD was obligated to release their FIO data publicly on Boston's data portal. But when you search for ‘stop frisk’ on the portal, nothing comes up. Journalists and members of the public would need to know the bureaucratic term for the programme (FIO) in order to be able to locate it on the portal.\nSearching for ‘stop frisk’ in the City of Boston's open data portal yields no results. Those searching for data would have to know that the programme is called ‘Field Interrogation and Observation’.\nFurthermore, some institutions may publish their data and metadata freely, but be less forthcoming about their data's limitations. This can lead to serious cases of misuse and misrepresentation of data. In one chapter of Data Feminism, The Numbers Don't Speak for Themselves, Lauren Klein and I discuss the case of GDELT: the Global Database for Events, Language and Tone. In a high-profile correction, FiveThirtyEight had to retract a story about the kidnappings of Nigerian girls that used the GDELT database. They had mistakenly used media reports about kidnappings to tell a story about the incidence of kidnappings. While FiveThirtyEight should have verified their data before publishing, we describe how GDELT, because of their pressure to attract big data scientific research funding, failed to describe the limitations of their ‘events’ data (which is not events data at all, but rather ‘media reports about events’ data).\nThe Three-Step Context Detective\nSo, what's a data journalist to do? She has to become a ‘context detective’, working with data that have been captured from the world into spreadsheets and databases, and connecting them back into their collection environment. This work is similar to that of a detective -- the journalist has to use incomplete clues that point backwards to the bureaucratic functionings of the collecting institution.\nTo understand the data, you must understand the who, what, when, where, why, and how of that bureaucracy. Below I present a process called the Three-Step Context Detective, which I use in the classroom. These steps don't necessarily have to be completed in this order.\n1. Download the data and get orientated\nLooking at hundreds, thousands or hundreds of thousands of obliquely named rows and columns can be daunting at first. Sometimes newcomers think that the data science ‘wizards’ can just look at a spreadsheet and see a story emerge. Nothing could be further from the truth. Getting oriented in your dataset involves breaking down the basics systematically so that you can ask good questions.\nYou can use a programme like Excel or Google Sheets to do basic exploration to answer questions like:\nHow many observations (rows of data) do you have?\nHow many fields (columns) do you have?\nIs it clear what each row is counting? (Remember those incidences of kidnapping versus media reports about kidnapping – getting crystal clear about what your data is logging is supremely important.)\nWhat is the time period of the data? Use the ‘Sort’ function on any column with dates or timestamps to see when the data begin and when they end.\nWhat is the geographic extent of the data?\nDoes there appear to be a lot of missing data?\nFor more on using spreadsheets, check out Brant Houston's article Spreadsheets for journalism, or our video courses Doing Journalism with Data: First Steps, Skills and Tools and Cleaning Data in Excel. You can also start a conversation in our forums.\nThis process of getting oriented with a new dataset is no small task. In fact, Rahul Bhargava and I built a free, online tool called WTFcsv which captures the emotion that journalists often feel when looking at a new spreadsheet: \"WTF is going on with my csv file?!\" Using WTFcsv, you can continue your orientation process.\nWTFcsv analyses each column from a spreadsheet file and returns a data visualisation that summarises the patterns in each column. For example, the image below depicts data about passengers on the Titanic. The column ‘Sex’ is rendered as a column chart that demonstrates, visually, that there were 314 female and 577 male passengers logged on the Titanic.\nRahul and I talk about the importance of asking good questions of your data before you conduct analysis or tell stories. WTFcsv can help you answer all of the basic questions above, as well as start to form your own questions about the dataset in front of you. For example, for the Titanic data, good questions might be about ethics (‘Why is 'Sex' a binary variable?’), data formatting (‘What does the 'Parch' column mean?’), data quality (‘Is this data complete?’), or data analysis (‘Did women survive at a higher rate than men?’).\nIt's important to write down all of these questions, because as you go through the next couple steps, you can try to answer them.\n2. Explore all available metadata\nMetadata is data about data and can be your golden ticket to establishing context for a dataset. In an ideal world, any dataset that you download would have a detailed and up-to-date data dictionary. This is a document that provides a column-by-column description of the dataset, along with guidelines for using it.\nIn the above example, from the NOAA National Database of Deep-Sea Corals and Sponges, each field (column) in the dataset is annotated and explained, along with descriptions of data quality, units of measurement, completeness, and usage guidelines.\nSeeking metadata is not always easy or successful. Not all data providers produce data dictionaries. In 2016, journalist J. Albert Bowden sought documentation on the fields in a dataset from the US Department of Agriculture. He was told that explanation of their column headers was a proprietary secret. Moreover, even when there are metadata, providers might fail to call the file ‘data dictionary’. For example, if you use the City of Boston's 311 data, the data dictionary is called ‘CRM Value Codex’ – not the most attractive and user-friendly name ever.\nAnd sometimes data dictionaries or other forms of metadata might be outdated because the institution fails to update them when the dataset changes. It's important to have your sceptical, fact-checking, data-verifying journalist hat on at all times.\n3. Background the dataset\nJournalists often ‘background’ a person or ‘background’ an issue, and likewise the final step of the Three-Step Context Detective is to background your dataset. This may be the most time-consuming aspect of establishing context for your data, but it is well worth the investment in terms of understanding limitations, preventing errors, and discovering newsworthy stories and analysis. In this process, there are at least three separate things to conduct background research on.\nBackground the collection process\nNewcomers (as well as old-timers!) can forget that data collection is often a human, material process. Data science consultant Heather Krause advocates for creating data biographies that describe where the data came from, who collected them, and how they collected them. In the case of the Boston stop and frisk programme discussed above, police officers fill out paper forms after having an encounter with a resident on the street. Then, those forms get turned into the precinct and a staff member logs the values in a database. Before publishing to the website, other staff members remove personally identifying information. It's all very mundane, but absolutely essential to understanding where errors and missing data can be introduced. The meat is in the bureaucratic details like whether data is self-reported, or observed, or measured by a machine. Like what database the organisation uses to store the data. Like whether the way the organisation is counting and measuring has changed recently (making current data and prior data not comparable).\n\nAfter exploring any available metadata, your best path to backgrounding the collection process is finding a human being to talk to. This might be someone from the collecting organisation, but it's important to think creatively about interviewees when this is not feasible.\nAfter exploring any available metadata, your best path to backgrounding the collection process is finding a human being to talk to. This might be someone from the collecting organisation, but it's important to think creatively about interviewees when this is not feasible. For example, in the case of stop and frisk data, it's hard to get police spokespeople on the phone. Other potential interviewees might include the youth of colour who are disproportionately stopped, the ACLU who sued the police department and did their own background research on the collection process, or criminal justice scholars who have studied the Boston programme. Krause has a helpful data biography template that you can fill out for this process.\nBackground the organisation\nWhile we have talked a lot about understanding datasets, Yanni Loukissas makes the case in his book, All Data Are Local, that to use data effectively we also need to understand data settings. Backgrounding the dataset is not just about the data itself – it's also about understanding why an organisation was motivated to collect it in the first place, as well as how they use it.\nIn the case of the stop and frisk data, this means doing background research on the Boston Police Department: What is their mission? How long they have existed? What is their budget? How many officers are there? When have they been in the news in the last ten years and why? It also means researching the FIO programme specifically: When and why did the BPD start the programme? Was it part of a national wave of FIO programmes? Is there scholarly and legal debate on whether these programmes are constitutional and effective in reducing crime?\nFrom this understanding of the underpinning organisational and programmatic goals, it's helpful to try to understand how the organisation uses the data it collects internally. For example, does the BPD use its logs of police-civilian encounters to try to limit racial profiling? Do officers have quotas? Who does the BPD have to report their numbers to?\nHere, again, interviews with real, live human beings are going to be one of the most effective ways of getting information about the organisation's motivations and uses of the data it collects. But when you can't get an inside interview, one of the best ways to find this information is to:\nBackground the regulatory environment\nData is expensive to collect, maintain, and organise. Most organisations don't collect data because they want to but rather because they have to, either to comply with laws or with internal policies. Doing background research on the regulatory environment can often shed light on why the organisation collects data, who it reports that data to, and how it reports the data. For example, all accredited higher education institutions in the US have to collect and report data about sexual assault on college campuses because of the Jeanne Clery Disclosure of Campus Security Policy and the Campus Crime Statistics Act (Clery Act).\nIt is typically easier to background federal and state laws, which tend to be available publicly or identified via talking with lawyers and others with legal knowledge. Internal policy documents that guide data collection can be harder, albeit not impossible to access. If you live in a country with public records laws, using those laws to request organisational governance documents and training manuals can be an excellent way to understand the internal regulatory context that guides information collection. As an example, most police departments in the US collect data on the use of force by police officers against civilians. Knowing this, when a white male police officer used excessive force at a pool party in 2015, reporters at MuckRock made a public records request for the McKinney police use of force policy. On page nine, the policy details when and why officers are required to file a 'Response to Resistance' report (RTR) and who is responsible for maintaining those reports. This policy would be essential background information for any journalist seeking to write a data story about use of force from RTR data.\nPitfalls\nSo, the Three-Step Context Detective consists of getting oriented, exploring all metadata and backgrounding the dataset (including the collection, the organisation, and the regulatory environment). In the process of building out these connections between the dataset and its broader context, there are two pitfalls to keep your eyes on.\nThere are many ‘Unknowns’ in the 2016 dog licensing data from the City of New York. We need to be careful not to make assumptions like ‘Unknown’ means ‘Mixed breed’.\nFirst, beware of your own brain and its penchant for making assumptions to fill in unknowns. It is tempting when looking at columns in a dataset to imagine that you know what they mean, but this can be dangerous. In my data journalism class, we were working with data about the dogs of New York City -- their breeds, ages, and sex.\nOf course, some fields had incomplete data and 'breed' was one of those with many 'UNKNOWN' values in the column. One student assumed that breed = UNKNOWN meant that the dog was mixed breed and built their whole story around that incorrect assumption ('UNKNOWN' means the information wasn't filled out by the applicant so we literally do not know the breed). Luckily, the student did end up checking their assumptions and revising the story, and the data itself was fairly low stakes. That said, this illustrates the importance of Jonathan Stray's advice about 'considering multiple explanations for the same data, rather than just accepting the first explanation that makes sense'. The same advice applies when assembling the context around your data just as much as it applies when analysing it.\nSecondly, it's important to remember that power is not equally distributed across the collection process, the organisation, and the regulatory environment. The result of social inequalities in the data setting is that the numbers may appear to tell one story on first exploration, but that story might be completely false because the collection environment has systematically silenced people with less power. What does this mean?\nIn The Numbers Don't Speak for Themselves, Lauren Klein and I discuss a story written by three of my students about sexual assault data provided by the Clery Act. What the students found is that campuses with high numbers of sexual assault were not hotbeds of rape culture, instead these campuses were actually providing the most resources and the most supportive environment for survivors to come forward. So, paradoxically, the campuses with the lowest rates of sexual assault were not doing great but rather creating an environment that actively discouraged survivors to report. Meanwhile, those campuses with higher numbers were actually measuring sexual assault closer to what the reality of its incidence is. This kind of pattern will be visible anytime that structural forces like patriarchy, racism, or classism are at work (read: all the time) that lead to the systematic undercounting or overcounting of women and other marginalised groups. The way to address it is through establishing context – the students discovered this through background research, reviewing policy docs and many interviews – rather than accepting the numbers at face value.\nOpportunities\nJust as there are pitfalls for context, there are also opportunities for journalists and news organisations to create useful resources for readers and other journalists from their work on context. And context is work! Instead of writing a single story from a data exploration, organisations like ProPublica have started to create what Scott Klein calls 'news apps', that is, evergreen resources like Dollars for Docs. While it is useful for individual readers, Dollars for Docs has also become a data resource for other news organisations to write their own, localised stories on the influence of pharmaceutical companies – for example, this story about the effect of pharma money on doctors in St. Louis, Missouri. In this sense, ProPublica has become known as an 'information intermediary', by turning their original investigation's context and data into a resource that is reusable for other organisations.\nProPublica turns the context work that they do compiling and backgrounding datasets into a source of revenue in their data store.\nVerified data and expert contextual information can also become a source of revenue for news organisations. ProPublica maintains a data store where you can purchase datasets on a variety of topics. Many of the datasets available come with excellent ‘data user guides’ – a term coined by Bob Gradeck, manager of the Western Pennsylvania Regional Data Center. In his work promoting open data, he saw the need for metadata that goes beyond the data dictionary to provide a narrative account of a dataset - where it comes from, how it is used by the organisation, and what its limitations are. Examples of Bob’s work can be seen in the data user guides for 311 data in Pittsburgh.\nThe Associated Press (AP) is also getting into the data + context = revenue game. They spent extensive time compiling a national database on school segregation in the US, which comes with a 20-page data user guide including where the data is collected from and what kinds of questions it can be used to answer. It's available for purchase, and the AP is starting to develop a subscription model where organisations can pay for access to other datasets, context, and discussions with reporters who worked on those issues.\nConclusion\nThe bottom line is that putting data back into context is laborious but absolutely necessary work for telling data stories. Context is the way to get the story right, even in the face of meager metadata, bureaucratic obstacles, and power imbalances in the data setting. Do you have stories about your work with context and data? Share them in our forums."
  },
  {
    "creator": "Brant Houston",
    "title": "Spreadsheets for journalism",
    "link": "https://datajournalism.com/read/longreads/spreadsheets-for-journalism",
    "pubDate": "Thu, 04 Apr 2019 17:29:00 +0200",
    "isoDate": "2019-04-04T15:29:00.000Z",
    "content": "\n                                                                        <p>It is still the easiest laugh to get from a group of journalists -- professionals or students -- throughout the world.\nAll that needs to be said is: “We all know you got into journalism to do math”.</p>\n<p>The laughs come because most journalists have seen themselves primarily as storytellers and word artists. For them, numbers are worrisome, boring, or interfere with the flow of an article.</p>\n<p>Furthermore, the perception of journalists’ inadequacy and inability to deal with numbers has been increased by mathematicians and statisticians over the decades. They’ve pored through newspapers and websites, listened to radio, and watched broadcast news with the purpose of finding errors and ignorance whenever journalists have reported on numbers.</p>\n<p>The book, <a href=\"https://www.amazon.com/Mathematician-Reads-Newspaper-Allen-Paulos/dp/0465089992\">A Mathematician Reads the Newspaper</a>, by John Allen Paulos was relentless in its pursuit of journalists’ mathematical misfortunes. That book followed his previous book, <a href=\"https://www.amazon.com/Innumeracy-Mathematical-Illiteracy-Its-Consequences/dp/0809058405\">Innumeracy</a>, which was broader in its criticism of math impairment across many professions, but included journalists among those wrongheaded.</p>\n<p>Paulos’ books and other statisticians’ criticism implied that journalists hate numbers and can’t do math and, perhaps, never will. But that has become untrue over the past two decades. This change can be linked to the surge of journalists using data and to the self-realisation that they often do some kind of math every day. Whether it is deciphering budgets, examining salaries, or looking at accident or murder rates, most journalists these days are constantly counting and comparing numbers.</p>\n<p>Certainly, up until the 1980s, it was the rare journalist who understood the difference between mean and median, could calculate a percentage difference, or do a simple rate. At the Kansas City Star, where I worked in the 1980s, there was a copy editor who knew how to do percentage difference by paper and pencil and he sometimes had a small line of reporters at his desk waiting for him to do that calculation for each of their stories.</p>\n\n                                                                                                                                <p>Brant Houston is an award-winning journalist, journalism professor and author. He was an investigative reporter in US daily newsrooms for 17 years and was executive director of Investigative Reporters and Editors for more than a decade.</p>\n\n                                                                                                                              \n                            <p>A major example of innumeracy over the years was that news stories would favour sports team owners -- without realising it -- during labour negotiations between owners and players. These stories would cite the average salary of players rather than the median, thus letting the huge salaries of a few star players inflate the average. If the reporters had used median, they would have seen how few players made the average and the perception that all players were millionaires was false.</p>\n<p>In other instances, journalists would report that it was fair for workers to get the same percentage increase in wages, without realising that a 3% increase for someone making $150,000 (it’s $4,500) is much greater than a 3% increase for someone making $30,000 (it’s $900).\nJournalists would also fail to use rates for putting raw numbers in perspective. One city would be called the murder capital of a country based on the total number of murders, despite having a much lower murder rate than other cities. A road intersection would be deemed the most perilous based on total number of collisions, rather than the rate of collisions compared to traffic.</p>\n\n                                                                                                  \n                                <p>Always check the mathematics behind ‘murder capital’ claims -- sometimes it isn’t as robust as it seems.</p>\n\n                                                                                        <p>An intersection that has a hundred collisions a year, when the traffic through it is 100,000 cars a year, is less riskier than an intersection that that has a hundred collisions a year with only 10,000 cars passing through it in the same year.</p>\n\n                                                                                                  \n                            <p>But the public shaming of journalists who made mathematical errors left reporters, as the long-time journalist and top data journalism instructor <a href=\"https://twitter.com/sarahcnyt?lang=en\">Sarah Cohen</a> wrote in her book,<a href=\"https://www.amazon.com/Numbers-newsroom-Using-math-statistics/dp/B0006E8VEC\"> Numbers in the Newsroom</a>, with \"the impression we can't use any numbers without fearing retribution\". (Cohen's book is an invaluable guide on journalism and math.)</p>\n<p>Yet it was in the late 1980s that a small band of journalists began to embrace the power of accurate numbers and calculations as they began to work with data. They also discovered the spreadsheet. And, inspired by Philip Meyer's book,<a href=\"https://www.amazon.com/Precision-Journalism-Reporters-Introduction-Science/dp/0742510883\"> Precision Journalism</a>, they came to see the power of math and numbers, rather than scorning or avoiding them.</p>\n<p>Data journalism workshops given by Meyer at the University of North Carolina and by Investigative Reporters and Editors with its companion organisation, <a href=\"https://www.ire.org/nicar\">NICAR</a>, drew hundreds of journalists eager to learn data analysis. At those workshops and then at NICAR conferences, they received training that included math -– training that was seldom, if ever, offered in classrooms for journalists or newsrooms. In fact, journalism professors wanting to keep up with the profession attended those workshops and became the few including math and numbers in their classes.</p>\n<p>In those workshops, I and my colleagues found the previous teaching of math had lacked the appropriate approach and perspective. The best approach demystifies ’math’ and focuses on the basics that allow journalists apply math in a practical way –- that is, to summarise numbers, put them in context, and determine if the numbers are misleading or lies.</p>\n<p>The result of the workshops -– which spread globally -- was an increased understanding of numbers and thus the ability to write more lucidly about those numbers. Numbers were not boring if they revealed shocking ethnic disparities, large numbers of failing bridges, or alarming rates of murder.</p>\n<p>It was clear it was much easier to deal with numbers if the teaching led to that immediate illumination about a topic.</p>\n\n                                                                                                  \n                                <p>A screenshot of Visicalc -- the first spreadsheet that combined all essential features of modern spreadsheet applications. Credit: <a href=\"https://en.wikipedia.org/wiki/File:Visicalc.png\">Wikimedia</a>.</p>\n\n                                                                                        <p>In addition, the use of spreadsheets, be it Microsoft Excel or Google Sheets, made the math easier because journalists could rely on automatic calculations once the numbers were entered in. That also increased journalists’ confidence in interpreting statistics and surveys, as well as encouraging them to employ more advanced statistical methods.</p>\n\n                                                                                                  \n                            <p>One manifestation of this change in journalism can be seen in the number of websites and news columns devoted to the interpretation of numbers. Among the places devoted to numbers:<a href=\"http://www.wsj.com/news/author/8046\"> a regular Saturday column</a> by Jo Craven McGinty on numbers in the Wall Street Journal, the<a href=\"https://www.nytimes.com/section/upshot\"> Upshot column</a> in the New York Times, and the<a href=\"https://fivethirtyeight.com/\"> FiveThirtyEight</a> website by Nate Silver.</p>\n<p>Another manifestation is the<a href=\"https://ire.org/awards/philip-meyer-awards/\"> Philip Meyer Awards</a>, international awards given by Investigative Reporters and Editors, that recognise the best uses of social science in journalism. Year after year, since 2005, these awards show the progress that has been made in the field's numeracy. For example, an investigation by Bayerischer Rundfunk and Der Spiegel,<a href=\"https://www.hanna-und-ismail.de/english/index.html\"> No Place for Foreigners. Why Hanna is invited to view the apartment and Ismail is not</a>, revealed discrimination against foreigners in the German housing market through a large-scale survey of landlords. They found that potential renters with Arab and Turkish names were frequently ignored.</p>\n\n                                                                      \n                            <p>This investigation drew heavily on number-based experiments, like the difference in chances pictured above. Read the full piece <a href=\"https://www.hanna-und-ismail.de/english/index.html\">here</a>.</p>\n\n                                                                      \n                            <p>In another award winner, Buzzfeed and BBC<a href=\"https://www.buzzfeednews.com/article/heidiblake/the-tennis-racket\"> used a million simulations of tennis matches</a> to discover suspicious patterns in the shifting of betting odds and players who lost matches they statistically should not have lost.</p>\n<p>In the US, journalists at<a href=\"https://www.chalkbeat.org/posts/co/2011/03/06/when-scores-seem-too-good-to-believe/\"> several newsrooms</a> have shown<a href=\"http://www.philly.com/philly/hp/news_update/20110501_City_school_s_fast-rising_test_scores.html\"> widespread cheating on standardised tests</a> by showing that test scores were way too high based on analysis of previous years' scores. In a similarly math-based investigation, ProPublica<a href=\"https://www.propublica.org/article/us-lags-behind-world-in-temp-worker-protections\"> uncovered</a> a disturbing trend: temporary workers are hurt up to six times the rate of permanent employees, and their injuries are more severe.</p>\n<p>And there are many examples of smaller but effective stories using numbers. Years ago, a reporter, who had just received training in spreadsheets, found the city she covered had uniformly miscalculated percentage changes in its annual budget. Some reporters found political associates in governments receiving much larger salaries than regular employees. Others calculated serious cost overruns in government programmes.</p>\n\n                                                                      \n                            <h1>The spreadsheet as the basic, starter tool</h1>\n\n                                                                      \n                            <p>With just a spreadsheet, a journalist can let the software do the counting and calculating, allowing them to concentrate on the purpose and result of their inquiry. It also opens the door to understanding more advanced statistics, and the use or misuse of statistics by governments and businesses.</p>\n<p>The mathematical tools in a spreadsheet can be divided into two groups: data management and calculations.</p>\n<p>Data management, in which the counting is automatically completed within the spreadsheet, includes:</p>\n<ul>\n<li>Filtering data based on a criteria</li>\n<li>Sorting to bring meaning to numbers by looking at them from high to low or low to high</li>\n<li>Summarising by grouping topics into categories, and summing or counting the numbers associated with each category</li>\n</ul>\n<p>Important basic calculations, some of which can be automatically executed and some which must be performed by the journalist, include:</p>\n<ul>\n<li>Summing up a column or row of numbers</li>\n<li>Determining the mean or median of a column</li>\n<li>Calculating percentage difference</li>\n<li>Calculating a rate</li>\n<li>Calculating a ratio</li>\n</ul>\n<h2>Data management</h2>\n\n                                                                      \n                            <p>Let’s begin with filtering. There’s a recreational boating accident database in the US that has details of accidents that led to deaths. Here is a sample of that data, which is probably collected in many other countries.</p>\n\n                                                                                                  \n                                <p>An abbreviated version of the recreational boating accident database kept by the US Coast Guard.</p>\n\n                                                                                        <p>By using the filter function in a spreadsheet, a journalist can quickly answer the following question: How many persons died of drowning who were not wearing a life jacket (PFD – Personal Flotation Device) and could not swim? It turns out that nearly two-thirds of the drowning deaths include people who could not swim and did not wear a life jacket. A slice of the data appears below.</p>\n\n                                                                                                                              \n                                <p>This shows only the accidental boating deaths caused by drowning and in which the victim was not wearing a life jacket and could not swim.</p>\n\n                                                                                        <p>All the journalist has to do is click the filter icon, picking one of the scroll arrows in a column, and choose the criteria. The journalistic value of using this tool is immediately clear because the reporter now has a story showing that some of the deaths could have been preventable.</p>\n\n                                                                                                  \n                            <p>Simply sorting numbers can bring meaning to them, or it can take the political spin off them.</p>\n<p>For example, the World Health Organisation issues <a href=\"https://www.who.int/gho/mortality_burden_disease/life_tables/hale/en/\">an annual report</a> on the healthy life expectancy of males and females in each country. The annual report is issued with the countries listed alphabetically. (Below is a simplified version of the data created by eliminating some of the columns of information.)</p>\n\n                                                                                                  \n                                <p>An abbreviated version of the Healthy Life Expectancy database from the World Health Organization.</p>\n\n                                                                                        <p>Sort the countries by the highest life expectancy to the lowest, and you can see the biggest differences –- potentially the start of a story on why some countries are higher and some are lower. This is done with a simple calculation of subtracting the life expectancy of males from females, and sorting by that difference.</p>\n\n                                                                                                                              \n                                <p>The Health Life Expectancy data with the calculated difference between male and female ages sorted by the largest difference to the smallest.</p>\n\n                                                                                        <p>As you can see, many of the largest differences are in countries that were a part of the former Soviet Union. Again, this could be the start for an illuminating story on why that is.</p>\n\n                                                                                                  \n                            <p>Grouping numbers in categories and counting or summing them (or both) can give a valuable overview of a dataset.  A spreadsheet has an excellent tool for summarising, called a Pivot Table. Let’s have a look at how this tool can help discover which retailer sells the most guns in Missouri.</p>\n<p>By clicking on the Insert tab and then on the icon for the Pivot Table, journalists can choose to count by the numbers of licenses a business holds.</p>\n\n                                                                      \n                            <p>The Pivot Table icon has been selected in the left hand corner.</p>\n\n                                                                      \n                            <p>The Pivot Table allows you to count the number of each business with licenses by choosing from a list in a selection screen.</p>\n\n                                                                      \n                            <p>This pivot table shows the number of dealerships licensed under a unique business name.</p>\n\n                                                                      \n                            <p>Sorting from high to low based on number of licenses, it’s possible to see that the corporation Walmart has the most licenses to sell guns in Missouri.</p>\n\n                                                                                                  \n                                <p>This data shows the number of licensed dealerships by unique business name sorted by largest number to smallest.</p>\n\n                                                                                        <p>In these examples of data management, the journalist only has to do one calculation: subtraction (in the healthy life expectancy dataset, where male ages are minused from female ages). The software does all the other counting and arranging.</p>\n\n                                                                                                  \n                            <h2>Calculations</h2>\n\n                                                                      \n                            <p>Journalists can rapidly total columns of numbers by using the formula or icon for summing a column.</p>\n<p>The icon in a spreadsheet is one way to do a sum, but if there are blank rows it is better to put in the specific range of numbers. Here is a list of salaries at an imaginary government agency.</p>\n\n                                                                      \n                            <p>The icon can be used for one group, but because of the blank row it will stop at that group unless the range is dragged upwards. It is easier to do this calculation =sum(b2:b9) than worry about missing a row when specifying the range.</p>\n\n                                                                                                  \n                                <p>This is a fictional dataset on municipal salaries earned by political appointees with the total salaries added for the previous year.</p>\n\n                                                                                        <p>The brilliance of a spreadsheet is that it maps the data, which allows formulas to calculated and copied easily. Instead of doing calculations with numbers, journalists can use the ‘addresses’ of the numbers.</p>\n\n                                                                                                  \n                            <p>The mean is often known as the average and, in fact, spreadsheets use the word average for the calculation. But be wary: means can obscure the effect of a large number on the average – such as a CEO or a team’s superstar – or of a small number – such as a group of lowly paid workers. A median, in which half the numbers are higher and half are lower, can serve as lie detector and can correct for those ’outliers’.</p>\n\n                                                                      \n                            <p>For example, a team of five athletes has one star and four regular players. </p>\n<p>If the average is calculated with the formula  =AVERAGE(b3:b7), then the average salary is $158,000, thus making it appear that most players are making $158,000.</p>\n\n                                                                                                  \n                                <p>A fictional dataset of the game salaries for professional basketball players.</p>\n\n                                                                                        <p>However, the median with the formula  =MEDIAN(b3:b7) shows that the median salary is $50,000, which is a much more accurate indication of what most of the players are making. By reporting only the average, a journalist would mislead the audience into thinking players are making much than they are.</p>\n\n                                                                                                  \n                            <p>Calculating a percentage difference is one of the most powerful tools a journalist can use. It puts numbers in proportion. For example, a journalist might want to look at the impact of salary raises on individuals at an agency. In the two columns in the agency worksheet, last year’s wages and this year’s wages are listed. As seen below, the calculation of percentage difference is not $7,000 (the difference) divided by the previous salary ($45,000), but rather the formula =D2/B2. (The = sign is needed for any formula.)</p>\n<p>So, to calculate a percentage difference, last year’s wage is subtracted from this year’s wage. Then the difference is divided by last year’s wage. With this calculation, the actual impact on each worker is seen. These are not the usual raises, of course, but fictional ones given to a politician’s associates.</p>\n\n                                                                                                  \n                                <p>This dataset shows the percentage difference calculated between last year salaries and this year’s salaries.</p>\n\n                                                                                        <p>Percentage difference is used in many reports -- budgets and trade, for example -- to show both raw numbers and how they compare to each other.</p>\n\n                                                                                                  \n                            <p>Rates are used throughout the world, whether they are for traffic accidents, mortality, crime, or many other issues. Rates are used so that more fair comparisons can be made between categories, often addressing risk. For example, one city could have 600 murders a year and another could have 400 murders a year. But if the population of the city with 600 murders a year is much larger, then the murder rate is much lower and, thus, the risk of being murdered is much lower. (Crime rates can be more complex than this, but this is a frequent use of rates.)</p>\n\n                                                                                                  \n                                <p>Actual data from the Federal Bureau of Investigation with the murder rate calculated for the larger cities in the US.</p>\n\n                                                                                        <p>A rate is calculated by thinking of the number of incidents per population (which could mean people, number of vehicles if its traffic, and so on). In the case of murder rates, it would be number of murders divided by the city population. In this example, the US city with the most murders (Chicago) does not have the highest murder rate.</p>\n\n                                                                                                  \n                            <p>Ratios are numbers that are extremely useful when writing about numbers. It can be much more concise to write that one number is double that of another, rather than it is 100% higher. It can be quite startling to find one group of people is jailed twice as often as another, or a pharmaceutical drug has a success rate three times higher than another.</p>\n<p>For example, one ethnic group has 8000 persons jailed each year. Another group has 4000 persons jailed each year. By using the formula =8000/4000 the ration is determined to be 2 to 1, or double that. If the first ethnic group makes up only 10% of the total population, then a journalist has the beginning of an inquiry to answer why.</p>\n\n                                                                      \n                            <p>For more on using spreadsheets, check out our video courses <a href=\"https://datajournalism.com/watch/doing-journalism-with-data-first-steps-skills-and-tools\">Doing Journalism with Data: First Steps, Skills and Tools</a> and <a href=\"https://datajournalism.com/watch/cleaning-data-in-excel\">Cleaning Data in Excel</a>. You can also start a conversation in our <a href=\"https://discuss.datajournalism.com/\">forums</a>.</p>\n\n                                                                      \n                            <h1>Conclusion</h1>\n<p>These basic functions and calculations allow journalists to overcome a fear of numbers and to leap into using math for stories. If the growth of data journalism is anything to go by, the adoption of these new skills benefits both the field and newsrooms, with inquiries that are more accurate, use better comparisons, and give greater context. It all adds up to what every reporter strives for: meaningful and insightful journalism. And that is no laughing matter.</p>\n\n                                              \n                ",
    "contentSnippet": "It is still the easiest laugh to get from a group of journalists -- professionals or students -- throughout the world.\nAll that needs to be said is: “We all know you got into journalism to do math”.\nThe laughs come because most journalists have seen themselves primarily as storytellers and word artists. For them, numbers are worrisome, boring, or interfere with the flow of an article.\nFurthermore, the perception of journalists’ inadequacy and inability to deal with numbers has been increased by mathematicians and statisticians over the decades. They’ve pored through newspapers and websites, listened to radio, and watched broadcast news with the purpose of finding errors and ignorance whenever journalists have reported on numbers.\nThe book, A Mathematician Reads the Newspaper, by John Allen Paulos was relentless in its pursuit of journalists’ mathematical misfortunes. That book followed his previous book, Innumeracy, which was broader in its criticism of math impairment across many professions, but included journalists among those wrongheaded.\nPaulos’ books and other statisticians’ criticism implied that journalists hate numbers and can’t do math and, perhaps, never will. But that has become untrue over the past two decades. This change can be linked to the surge of journalists using data and to the self-realisation that they often do some kind of math every day. Whether it is deciphering budgets, examining salaries, or looking at accident or murder rates, most journalists these days are constantly counting and comparing numbers.\nCertainly, up until the 1980s, it was the rare journalist who understood the difference between mean and median, could calculate a percentage difference, or do a simple rate. At the Kansas City Star, where I worked in the 1980s, there was a copy editor who knew how to do percentage difference by paper and pencil and he sometimes had a small line of reporters at his desk waiting for him to do that calculation for each of their stories.\nBrant Houston is an award-winning journalist, journalism professor and author. He was an investigative reporter in US daily newsrooms for 17 years and was executive director of Investigative Reporters and Editors for more than a decade.\nA major example of innumeracy over the years was that news stories would favour sports team owners -- without realising it -- during labour negotiations between owners and players. These stories would cite the average salary of players rather than the median, thus letting the huge salaries of a few star players inflate the average. If the reporters had used median, they would have seen how few players made the average and the perception that all players were millionaires was false.\nIn other instances, journalists would report that it was fair for workers to get the same percentage increase in wages, without realising that a 3% increase for someone making $150,000 (it’s $4,500) is much greater than a 3% increase for someone making $30,000 (it’s $900).\nJournalists would also fail to use rates for putting raw numbers in perspective. One city would be called the murder capital of a country based on the total number of murders, despite having a much lower murder rate than other cities. A road intersection would be deemed the most perilous based on total number of collisions, rather than the rate of collisions compared to traffic.\nAlways check the mathematics behind ‘murder capital’ claims -- sometimes it isn’t as robust as it seems.\nAn intersection that has a hundred collisions a year, when the traffic through it is 100,000 cars a year, is less riskier than an intersection that that has a hundred collisions a year with only 10,000 cars passing through it in the same year.\nBut the public shaming of journalists who made mathematical errors left reporters, as the long-time journalist and top data journalism instructor Sarah Cohen wrote in her book, Numbers in the Newsroom, with \"the impression we can't use any numbers without fearing retribution\". (Cohen's book is an invaluable guide on journalism and math.)\nYet it was in the late 1980s that a small band of journalists began to embrace the power of accurate numbers and calculations as they began to work with data. They also discovered the spreadsheet. And, inspired by Philip Meyer's book, Precision Journalism, they came to see the power of math and numbers, rather than scorning or avoiding them.\nData journalism workshops given by Meyer at the University of North Carolina and by Investigative Reporters and Editors with its companion organisation, NICAR, drew hundreds of journalists eager to learn data analysis. At those workshops and then at NICAR conferences, they received training that included math -– training that was seldom, if ever, offered in classrooms for journalists or newsrooms. In fact, journalism professors wanting to keep up with the profession attended those workshops and became the few including math and numbers in their classes.\nIn those workshops, I and my colleagues found the previous teaching of math had lacked the appropriate approach and perspective. The best approach demystifies ’math’ and focuses on the basics that allow journalists apply math in a practical way –- that is, to summarise numbers, put them in context, and determine if the numbers are misleading or lies.\nThe result of the workshops -– which spread globally -- was an increased understanding of numbers and thus the ability to write more lucidly about those numbers. Numbers were not boring if they revealed shocking ethnic disparities, large numbers of failing bridges, or alarming rates of murder.\nIt was clear it was much easier to deal with numbers if the teaching led to that immediate illumination about a topic.\nA screenshot of Visicalc -- the first spreadsheet that combined all essential features of modern spreadsheet applications. Credit: Wikimedia.\nIn addition, the use of spreadsheets, be it Microsoft Excel or Google Sheets, made the math easier because journalists could rely on automatic calculations once the numbers were entered in. That also increased journalists’ confidence in interpreting statistics and surveys, as well as encouraging them to employ more advanced statistical methods.\nOne manifestation of this change in journalism can be seen in the number of websites and news columns devoted to the interpretation of numbers. Among the places devoted to numbers: a regular Saturday column by Jo Craven McGinty on numbers in the Wall Street Journal, the Upshot column in the New York Times, and the FiveThirtyEight website by Nate Silver.\nAnother manifestation is the Philip Meyer Awards, international awards given by Investigative Reporters and Editors, that recognise the best uses of social science in journalism. Year after year, since 2005, these awards show the progress that has been made in the field's numeracy. For example, an investigation by Bayerischer Rundfunk and Der Spiegel, No Place for Foreigners. Why Hanna is invited to view the apartment and Ismail is not, revealed discrimination against foreigners in the German housing market through a large-scale survey of landlords. They found that potential renters with Arab and Turkish names were frequently ignored.\nThis investigation drew heavily on number-based experiments, like the difference in chances pictured above. Read the full piece here.\nIn another award winner, Buzzfeed and BBC used a million simulations of tennis matches to discover suspicious patterns in the shifting of betting odds and players who lost matches they statistically should not have lost.\nIn the US, journalists at several newsrooms have shown widespread cheating on standardised tests by showing that test scores were way too high based on analysis of previous years' scores. In a similarly math-based investigation, ProPublica uncovered a disturbing trend: temporary workers are hurt up to six times the rate of permanent employees, and their injuries are more severe.\nAnd there are many examples of smaller but effective stories using numbers. Years ago, a reporter, who had just received training in spreadsheets, found the city she covered had uniformly miscalculated percentage changes in its annual budget. Some reporters found political associates in governments receiving much larger salaries than regular employees. Others calculated serious cost overruns in government programmes.\nThe spreadsheet as the basic, starter tool\nWith just a spreadsheet, a journalist can let the software do the counting and calculating, allowing them to concentrate on the purpose and result of their inquiry. It also opens the door to understanding more advanced statistics, and the use or misuse of statistics by governments and businesses.\nThe mathematical tools in a spreadsheet can be divided into two groups: data management and calculations.\nData management, in which the counting is automatically completed within the spreadsheet, includes:\nFiltering data based on a criteria\nSorting to bring meaning to numbers by looking at them from high to low or low to high\nSummarising by grouping topics into categories, and summing or counting the numbers associated with each category\nImportant basic calculations, some of which can be automatically executed and some which must be performed by the journalist, include:\nSumming up a column or row of numbers\nDetermining the mean or median of a column\nCalculating percentage difference\nCalculating a rate\nCalculating a ratio\nData management\nLet’s begin with filtering. There’s a recreational boating accident database in the US that has details of accidents that led to deaths. Here is a sample of that data, which is probably collected in many other countries.\nAn abbreviated version of the recreational boating accident database kept by the US Coast Guard.\nBy using the filter function in a spreadsheet, a journalist can quickly answer the following question: How many persons died of drowning who were not wearing a life jacket (PFD – Personal Flotation Device) and could not swim? It turns out that nearly two-thirds of the drowning deaths include people who could not swim and did not wear a life jacket. A slice of the data appears below.\nThis shows only the accidental boating deaths caused by drowning and in which the victim was not wearing a life jacket and could not swim.\nAll the journalist has to do is click the filter icon, picking one of the scroll arrows in a column, and choose the criteria. The journalistic value of using this tool is immediately clear because the reporter now has a story showing that some of the deaths could have been preventable.\nSimply sorting numbers can bring meaning to them, or it can take the political spin off them.\nFor example, the World Health Organisation issues an annual report on the healthy life expectancy of males and females in each country. The annual report is issued with the countries listed alphabetically. (Below is a simplified version of the data created by eliminating some of the columns of information.)\nAn abbreviated version of the Healthy Life Expectancy database from the World Health Organization.\nSort the countries by the highest life expectancy to the lowest, and you can see the biggest differences –- potentially the start of a story on why some countries are higher and some are lower. This is done with a simple calculation of subtracting the life expectancy of males from females, and sorting by that difference.\nThe Health Life Expectancy data with the calculated difference between male and female ages sorted by the largest difference to the smallest.\nAs you can see, many of the largest differences are in countries that were a part of the former Soviet Union. Again, this could be the start for an illuminating story on why that is.\nGrouping numbers in categories and counting or summing them (or both) can give a valuable overview of a dataset.  A spreadsheet has an excellent tool for summarising, called a Pivot Table. Let’s have a look at how this tool can help discover which retailer sells the most guns in Missouri.\nBy clicking on the Insert tab and then on the icon for the Pivot Table, journalists can choose to count by the numbers of licenses a business holds.\nThe Pivot Table icon has been selected in the left hand corner.\nThe Pivot Table allows you to count the number of each business with licenses by choosing from a list in a selection screen.\nThis pivot table shows the number of dealerships licensed under a unique business name.\nSorting from high to low based on number of licenses, it’s possible to see that the corporation Walmart has the most licenses to sell guns in Missouri.\nThis data shows the number of licensed dealerships by unique business name sorted by largest number to smallest.\nIn these examples of data management, the journalist only has to do one calculation: subtraction (in the healthy life expectancy dataset, where male ages are minused from female ages). The software does all the other counting and arranging.\nCalculations\nJournalists can rapidly total columns of numbers by using the formula or icon for summing a column.\nThe icon in a spreadsheet is one way to do a sum, but if there are blank rows it is better to put in the specific range of numbers. Here is a list of salaries at an imaginary government agency.\nThe icon can be used for one group, but because of the blank row it will stop at that group unless the range is dragged upwards. It is easier to do this calculation =sum(b2:b9) than worry about missing a row when specifying the range.\nThis is a fictional dataset on municipal salaries earned by political appointees with the total salaries added for the previous year.\nThe brilliance of a spreadsheet is that it maps the data, which allows formulas to calculated and copied easily. Instead of doing calculations with numbers, journalists can use the ‘addresses’ of the numbers.\nThe mean is often known as the average and, in fact, spreadsheets use the word average for the calculation. But be wary: means can obscure the effect of a large number on the average – such as a CEO or a team’s superstar – or of a small number – such as a group of lowly paid workers. A median, in which half the numbers are higher and half are lower, can serve as lie detector and can correct for those ’outliers’.\nFor example, a team of five athletes has one star and four regular players. \nIf the average is calculated with the formula  =AVERAGE(b3:b7), then the average salary is $158,000, thus making it appear that most players are making $158,000.\nA fictional dataset of the game salaries for professional basketball players.\nHowever, the median with the formula  =MEDIAN(b3:b7) shows that the median salary is $50,000, which is a much more accurate indication of what most of the players are making. By reporting only the average, a journalist would mislead the audience into thinking players are making much than they are.\nCalculating a percentage difference is one of the most powerful tools a journalist can use. It puts numbers in proportion. For example, a journalist might want to look at the impact of salary raises on individuals at an agency. In the two columns in the agency worksheet, last year’s wages and this year’s wages are listed. As seen below, the calculation of percentage difference is not $7,000 (the difference) divided by the previous salary ($45,000), but rather the formula =D2/B2. (The = sign is needed for any formula.)\nSo, to calculate a percentage difference, last year’s wage is subtracted from this year’s wage. Then the difference is divided by last year’s wage. With this calculation, the actual impact on each worker is seen. These are not the usual raises, of course, but fictional ones given to a politician’s associates.\nThis dataset shows the percentage difference calculated between last year salaries and this year’s salaries.\nPercentage difference is used in many reports -- budgets and trade, for example -- to show both raw numbers and how they compare to each other.\nRates are used throughout the world, whether they are for traffic accidents, mortality, crime, or many other issues. Rates are used so that more fair comparisons can be made between categories, often addressing risk. For example, one city could have 600 murders a year and another could have 400 murders a year. But if the population of the city with 600 murders a year is much larger, then the murder rate is much lower and, thus, the risk of being murdered is much lower. (Crime rates can be more complex than this, but this is a frequent use of rates.)\nActual data from the Federal Bureau of Investigation with the murder rate calculated for the larger cities in the US.\nA rate is calculated by thinking of the number of incidents per population (which could mean people, number of vehicles if its traffic, and so on). In the case of murder rates, it would be number of murders divided by the city population. In this example, the US city with the most murders (Chicago) does not have the highest murder rate.\nRatios are numbers that are extremely useful when writing about numbers. It can be much more concise to write that one number is double that of another, rather than it is 100% higher. It can be quite startling to find one group of people is jailed twice as often as another, or a pharmaceutical drug has a success rate three times higher than another.\nFor example, one ethnic group has 8000 persons jailed each year. Another group has 4000 persons jailed each year. By using the formula =8000/4000 the ration is determined to be 2 to 1, or double that. If the first ethnic group makes up only 10% of the total population, then a journalist has the beginning of an inquiry to answer why.\nFor more on using spreadsheets, check out our video courses Doing Journalism with Data: First Steps, Skills and Tools and Cleaning Data in Excel. You can also start a conversation in our forums.\nConclusion\nThese basic functions and calculations allow journalists to overcome a fear of numbers and to leap into using math for stories. If the growth of data journalism is anything to go by, the adoption of these new skills benefits both the field and newsrooms, with inquiries that are more accurate, use better comparisons, and give greater context. It all adds up to what every reporter strives for: meaningful and insightful journalism. And that is no laughing matter."
  },
  {
    "creator": "Susan McGregor",
    "title": "Privacy and data leaks",
    "link": "https://datajournalism.com/read/longreads/privacy-and-data-leaks",
    "pubDate": "Thu, 02 May 2019 00:00:00 +0200",
    "isoDate": "2019-05-01T22:00:00.000Z",
    "content": "\n                                                                        <p>At its most core, the essential work of journalism is to gather and verify non-public information, evaluate its potential value to the public, and then -- if that value is substantial enough -- organise and publish it in such a way that it helps people make informed decisions about their lives. In a very important sense, this means that reporting and publishing around leaked data is no different than any other reporting: Once verified, the question of what to publish, and how, is driven principally by how it can best serve the public good. </p>\n<p>Yet both the scale and detail of today's information leaks, especially when combined with a highly networked -- and therefore global -- publishing environment, means that modern leaks present substantive practical and the ethical considerations for journalists. In addition to protecting the source of the leaked information, journalists -- like everyone else -- must work in a digital environment where their activities are almost constantly and ubiquitously tracked, making it all too easy to inadvertently reveal the direction of an ongoing investigation. Moreover, because leaks are now often larger than any one journalist -- or journalistic organisation -- can typically handle, they present unique collaboration and publication challenges, all of which must be carefully engineered to balance efficacy, transparency, and privacy. </p>\n<p>Despite these complexities, there are a range of useful methods and heuristics that can help journalists make ethical decisions about using secret, sensitive, or personal data. In fact, the most important -- and sometimes the most significant -- challenge is for journalists to accurately recognise when an ethical situation exists, especially in the fast-paced world of online publishing. For example, both our <a href=\"https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/mcgregor\">research</a> and our professional experience indicates that when journalists accurately perceive the sensitivity of the information they have obtained, they handle it with more caution and care. Yet our work also indicates that journalists typically <a href=\"https://www.usenix.org/node/190977\">rely on their sources' judgment</a> when assessing the sensitivity of the information they are being given. This suggests that while sources can, in general, trust journalists to handle leaked data carefully, it may also lead to problematic oversights when journalists report on data that has been simply ‘dumped’ online, especially since the sensitivity and/or significance of that data may not be immediately obvious.</p>\n\n                                                                     \n                            <p>Wikileaks has faced criticism for its data dumps, which have at times left sensitive and personal data exposed.</p>\n\n                                                                      \n                            <p>In our view, the mechanism through which data is obtained does not change the ethical standards with which it should be treated. If anything, sensitive information made accessible by a hack or a leak deserves more careful handling, since the agendas of those that have made the information public are uncertain at best. Moreover, the smash-and-grab data collection methods that are typical of hacks and breaches virtually ensures that the information of many private individuals -- whose only failing, often, is having been in a database along with someone or something of interest -- will be swept up along with anything potentially newsworthy. As such, journalists need to take particular care that their work does not implicate or injure those who are only 'guilty by association’. It is because of this, in part, that ‘leaked’ data may in fact demand more thoughtful handling than material provided by a confidential source.</p>\n\n                                                                      \n                            <h2>Hacked, breached, or leaked: key considerations when reporting with private or personal data</h2>\n<p>As we noted above, journalists can typically be relied upon to think carefully about how they handle sensitive data provided by a trusted or confidential source, in part because a human source will often impress upon the journalist the risks of mishandling the information. The journalist's desire to preserve valuable source relationships (and her own reputation) may also help counterbalance the impulse to publish material that may be more salacious than newsworthy. When data is simply 'dumped' online with little or no context, however, this source-oriented conscientiousness may go out the window. Combine this with the pressure journalists may feel from editors or competitors to get a story out quickly, and it can be hard to justify the delay required to weigh how significant the story in leaked data set actually is.</p>\n\n                                                                      \n                            \n                                                                                                                  \n                            <p>To help illustrate how the lack of a ‘human’ source -- especially when coupled with the exciting and even illicit nature of leaked data -- can raise crucial ethical challenges, we'll look at the issues presented by the data made public through two prominent hacks: the <a href=\"https://en.wikipedia.org/wiki/Sony_Pictures_hack\">Sony email hack</a> and the <a href=\"https://en.wikipedia.org/wiki/Ashley_Madison_data_breach\">Ashley Madison hack</a>.</p>\n\n                                                                      \n                            <p>In late November 2014, a massive dump of internal data from Sony Pictures was posted on the data-sharing site Pastebin, following a months-long hack of the entertainment company's systems. The hacked data contained everything from email exchanges to personnel files, and the question of what motivated the attack -- as well as who had perpetrated it -- quickly dominated headlines across the news spectrum. Yet many of the articles examining the contents of the leaked data also leaned toward the tabloid, focusing on <a href=\"https://www.theguardian.com/film/filmblog/2014/dec/10/sony-email-hack-jolie-hollywood\">executives' nasty exchanges</a> and <a href=\"https://www.theguardian.com/film/2014/dec/10/sony-hack-eamils-angelina-jolie-scott-rudin-amy-pascal-david-fincher\">celebrity name-calling</a>.</p>\n\n                                                                      \n                            <p>A snapshot of the Guardian’s <a href=\"https://www.theguardian.com/film/filmblog/2014/dec/10/sony-email-hack-jolie-hollywood\">reporting</a> on the Sony email hack.</p>\n\n                                                                      \n                            <p>Although many may find it hard to muster sympathy for these powerful and high-profile individuals, the coverage's focus on the machinations of Hollywood dealmaking and franchise evolutions also threatened to overshadow many of the substantive issues revealed by the documents, such as <a href=\"https://www.theverge.com/2014/12/12/7382287/project-goliath\">industry-wide coordination on lobbying efforts</a> designed to reshape the way that online content is served, or the fact that the data <a href=\"https://www.wsj.com/articles/sony-pictures-hack-reveals-more-data-than-previously-believed-1417734425\">revealed</a> the social security numbers, home addresses, and salaries of tens of thousands employees. </p>\n<p>The emphasis in the total coverage of the Sony hack, moreover, stands in stark contrast to coverage of similarly controversial leaks like the Snowden documents, where the focus has remained on the actions of powerful companies and nation-states, rather than on the foibles and gaffes of the individuals involved.</p>\n\n                                                                      \n                            <p>Less than a year after the Sony Pictures hack, another high-profile hack and data dump made particularly intimate details of a large number of people's lives essentially public online. In this case, however, the individuals whose information was posted were not celebrities or Hollywood dealmakers, but people from all walks of life who had joined an online dating site purportedly designed to facilitate extramarital affairs.</p>\n\n                                                                      \n                            <p>The Ashley Madison website offers a dating service for married individuals.</p>\n\n                                                                      \n                            <p>Although the Ashley Madison hack generally contained less <a href=\"https://en.wikipedia.org/wiki/Personally_identifiable_information\">personally identifiable information</a> than the Sony Pictures hack, the ramifications of the breach for those affected were sometimes devastating: <a href=\"https://splinternews.com/two-suicides-may-have-resulted-from-ashley-madison-leak-1793850197\">multiple suicides</a> <a href=\"https://splinternews.com/scared-dead-relieved-how-the-ashley-madison-hack-cha-1793853468\">were attributed to the hack</a>, with <a href=\"https://www.troyhunt.com/the-opportunistic-and-empty-threat-that/\">widespread blackmailing campaigns</a>, lost political careers, marriages, and community relationships resulting from the fallout. While there was some substantive reporting to be done on aspects of the leaked data -- for example, on the <a href=\"https://www.businessinsider.com/ashley-madison-users-in-us-government-2015-8\">potentially inappropriate use of the service from government offices</a>, and assessment <a href=\"https://gizmodo.com/how-ashley-madison-hid-its-fembot-con-from-users-and-in-1728410265\">of the fraud claims that had been made by former users</a> -- reporting that simply plucked individuals out of the Ashley Madison databases and treated them as 'sources' for additional comment may well have done more to traumatise people who had already been victimised by the breach in the first place. And while there is no doubt that accountability reporting can sometimes have negative consequences for those whom it covers, as journalists we must make every reasonable effort to ensure that those consequences are reserved for those legitimately suspected of actual wrongdoing, and not simply on people whose choices may differ from our own.</p>\n\n                                                                      \n                            <h2>Inverting implicit biases</h2>\n<p>A key component of the ethical challenge presented by cases like those above stems from our need, as journalists, to confront our own implicit biases. Whether or not we find the subjects of leaked data likable or even sympathetic, we must carefully weigh the news value of reporting with leaked data and the privacy interests of the people we are reporting on. As we’ve discussed, this can be particularly difficult to do when the data in question is simply posted online, since these datasets lack a human source to remind us of the potential ramifications of publishing. Importantly, this loss of context often also obscures the motives of the people who obtained and/or posted the hacked data in the first place -- something that should <a href=\"https://www.cjr.org/criticism/ashley_madison_hack_reporting.php\">arguably be a core focus</a> of the reporting on it.</p>\n<p>By their very nature, of course, our own biases are difficult to counter. Consulting with colleagues and editors is always a good place to start sanity-checking our first judgements. Another strategy is to use a simple thought experiment: If the contents of the leak were something that we personally believe should be private’ -- perhaps individuals' <a href=\"https://www.buzzfeednews.com/article/azeenghorayshi/grindr-hiv-status-privacy#.hfMZPpRAND\">HIV status, for example</a> -- would we still report on it, and how? Especially when we consider that most of us are <a href=\"https://www.cjr.org/criticism/ashley_madison_hack_reporting.php\">unwilling subjects of data collection</a> in the first place, it's important that journalists consider how to minimise additional harm to any individuals whose personal information they are working with -- no matter how it came into their hands.</p>\n<p>At times, however, reporting with sensitive and/or personal information is unavoidable, and may be essential to an important piece of accountability journalism. Where that is the case, there are still a number of ways that journalists can do the reporting and publishing that they need to while minimising potential harm as much as possible. Though today's networked data environment means there are few guarantees to be had, a clearly defined and thoroughly considered process -- especially one conducted in consultation with experienced colleagues -- can help journalists be confident in the appropriateness of their reporting and publication choices when dealing with hacked, leaked, and otherwise sensitive data.</p>\n\n                                                                      \n                            <h2>Considerations for reporting</h2>\n<h3>Good security helps ensure privacy protections</h3>\n<p>While we are <a href=\"https://academiccommons.columbia.edu/doi/10.7916/D89P3D4M\">unabashed advocates</a> for strong information security practices, a key reason for this is the protections that they allow journalists to provide for their human sources as well as any potentially sensitive data resources that they may have. Although many companies affected by data leaks and breaches have far more resources to dedicate to security, journalists have <a href=\"http://interactive.fusion.net/dirty-little-secrets/\">demonstrated</a> a unique <a href=\"https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/mcgregor\">ability</a> to protect information. The first step in treating personal data ethically is to take reasonable precautions to ensure that it does not slip out of your control. A great resource for beginning to enhance your own security know-how is the Electronic Frontier Foundation's <a href=\"https://ssd.eff.org/en\">Surveillance Self-Defense</a> website, which has everything from a security ‘starter pack’ to guides on particular tools.</p>\n\n                                                                      \n                            <p>There are many resources available online for journalists to start building up their security capability.</p>\n\n                                                                      \n                            <h3>Take care when verifying</h3>\n<p>While many appeared to relish The Intercept's <a href=\"https://www.nytimes.com/2017/06/06/business/media/intercept-reality-winner-russia-trump-leak.html\">apparent missteps</a> when <a href=\"https://www.washingtonpost.com/blogs/erik-wemple/wp/2017/06/06/did-the-intercept-bungle-nsa-leak/?utm_term=.051550dd6e5a\">verifying documents</a> allegedly provided to them by NSA contractor Reality Leigh Winner, it can be hard to know what information may tip off interested parties as you verify elements of a story. </p>\n<p>If you are relying on web searches, consider using a location-masking browser like <a href=\"https://www.torproject.org/projects/torbrowser.html\">Tor</a> to make it more difficult for online service providers to infer what you are researching. If you are relying on human sources, consider what you know about the provenance of the data you are dealing with. If it is genuine, ask yourself: What is the likely position of someone who would have access to it, and what might lead another person to guess where it came from? Use your answers to guide what information you share with whom when verifying.</p>\n<p>No matter the circumstance, however, it is always wiser to avoid sharing original documents. Instead, retype segments of content (correcting obvious spelling and punctuation errors) that you need to reveal -- many organisations will distribute sensitive documents with unique typographical or formatting features, so that they can pinpoint the source of an internal leak if originals show up online.</p>\n\n                                                                      \n                            <p>To protect their source, <a href=\"https://www.axios.com/read-trumps-private-leaked-executive-time-schedules-00e9313a-3066-4988-a6dc-711a47de661e.html\">Axios retyped the content</a> of leaked White House schedules.</p>\n\n                                                                      \n                            <p>Finally, it's always a good idea review documents (especially PDFs, .doc/x files and spreadsheets) on an old computer that will never be connected to the internet (use a thumb drive to move them there). While this may seem cumbersome at first, it also helps protect your other information (and your organisation) from the malware and viruses that leaked data may contain. This is especially true if you plan to print hard copies of documents -- the ‘enable editing’ permission required for printing can also put your own computer's data at greater risk.</p>\n\n                                                                      \n                            \n                                                                      \n                            <h2>Considerations for publishing</h2>\n<h3>How much is enough?</h3>\n<p>Providing reasonable privacy protections can sometimes seem at odds with imperatives around both accountability and transparency. When it comes to publishing data, however, the choice is not ‘all or nothing’. In fact, digital publishing gives journalists a range of ways to strike a balance between protecting private data and being transparent about their work. </p>\n<p>Naturally, any data to be published must first be verified; this alone will limit the incidental exposure of personal information, since verification -- in addition to being a hallmark of responsible journalism -- is incredibly time-consuming.</p>\n<p>Once verified, there is a question of relevance: Is the personal information you plan to publish essential to the story, or not? This can be a difficult question to answer. But just as we should reflect on why we might include information in a story about someone's age, race, immigration status, or other demographic attributes, we should relevance-test personal information we intend to publish. In short: Does the story really need it? This is especially important to consider given the ripple-effect of revealing personal information about someone. Family members, work colleagues, and -- in this age of social media -- even casual acquaintances may be affected by the revelation of your subject's personal details. Moreover, since most journalism today is inevitably published into a global context, the norms to consider when publishing personal details are not confined to a single region or culture. In general, you should be as conservative as possible without sacrificing the integrity of the story. Only after doing a thorough risk-benefit analysis, which keeps the costs to <em>the individual</em> at the center of your reasoning, should you make a determination about what to publish.</p>\n\n                                                                      \n                            <h3>Balancing privacy and transparency</h3>\n<p>Just as journalism seeks to hold power to account, journalists should make themselves accountable to the public as well. Where possible, this means <a href=\"https://twitter.com/jayrosen_nyu/status/1081393397355737088\">sharing data, sharing code</a>, and <a href=\"https://www.propublica.org/nerds/how-we-calculated-injury-rates-for-temp-and-non-temp-workers\">providing detailed methodologies</a> for <a href=\"https://www.propublica.org/article/temporary-work-lasting-harm\">the stories</a> that you produce. </p>\n<p>Yet, in many cases, wholesale publication of data or documents may violate the privacy of innocent people, or even put them at risk. In these cases, there are a number of <a href=\"https://datajournalism.com/read/longreads/de-identification-for-data-journalists\">methods</a> that journalists can use to make key information public.</p>\n<p><strong>Redaction</strong></p>\n<p>Tools like <a href=\"https://www.documentcloud.org/\">DocumentCloud</a> allow journalists to both publish documents and retain fine-grained control over them, offering both <a href=\"https://www.documentcloud.org/help/modification\">redaction</a> and annotation tools. Journalists hoping to redact information from documents are advised to do so with tools designed for the purpose, as some methods (for example, drawing black boxes in Adobe, or ‘hiding’ Excel columns or rows) <a href=\"https://www.cjr.org/analysis/manafort-mueller-redacted-document-ukraine.php\">are easily undone</a>. It's also important to keep in mind that simple methods of de-identification (such as removing names and addresses) <a href=\"https://www.cs.utexas.edu/~shmat/shmat_cacm10.pdf\">are often insufficient to protect people's identities</a>, given how much information is available to cross-reference online.</p>\n\n                                                                                                  \n                                <p>An example of a redacted CIA document. Source: <a href=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Redacted_CIA_document.jpg/512px-Redacted_CIA_document.jpg\">Wikimedia</a>.</p>\n\n                                                                                        <blockquote><p>Is the personal information you plan to publish essential to the story, or not?</p>\n</blockquote>\n\n                                                                                                  \n                            <p><strong>Samples and summaries</strong></p>\n<p>Another way that journalists can help protect individuals' privacy when publishing private data is to publish only a curated sample of data, or to publish data that has been summarised to the point that re-identification will be difficult, if not impossible. As above, however, the vast quantity of information available online means that both samples and summaries must be carefully designed to avoid revealing more than intended. We suggest consulting with a statistician if possible to help ensure that the measures taken are sufficient. A good place to start is to remove any obviously identifying information combinations, and then using the fully reported story to help guide your thinking about what samples or summaries will further clarify that story without inadvertently exposing individuals' information.</p>\n<p><strong>Visualisation</strong></p>\n<p>Visualisation is itself often a way of aggregating data in such a way that meaningful patterns are revealed in otherwise heterogeneous datasets. An advantage of visualisation for privacy protection is that a very small subset of data features are typically needed to create a visualisation, and, if well done, they add real value to the story being told. Static graphics, in addition to being more platform-friendly, also naturally limit the amount of inference and post-publication manipulation that is possible.</p>\n\n                                                                                                                                                             <p>When the Journal News published an interactive map of gun permit holders following a school shooting in 2012, <a href=\"https://datajournalism.com/read/longreads/ethical-questions-in-data-journalism-and-the-power-of-online-discussion\">many were outraged</a> by the apparent privacy invasion.</p>\n\n                                                                                                                                                                                         <p>In 2011, the Guardian was careful to publish maps about the UK Riots such that the underlying information was not accessible, out of concern for the type of backlash that the Journal News ultimately suffered. Both of these maps are now currently offline.</p>\n\n                                                                                                                                                                                         <p>Maps published with the Fusion series <a href=\"https://fusion.tv/story/5568/florida-citys-stop-frisk-nabs-thousands-of-kids-finds-5-year-olds-suspicious/\">Suspect City</a> illustrate where and how different age groups were stopped by police in Miami Gardens, Florida. Because many of those stopped were vulnerable or underage, these visualisations show the patterns of stops without revealing other personal details.</p>\n\n                                                                                                  \n                            <p><strong>Selective release</strong></p>\n<p>Sometimes a journalist may find themselves in possession of a unique trove of data that they simply do not have the resources to investigate as thoroughly as they would like. Happily, however, digital technologies remove the requirement to choose between a ‘data dump’ and keeping information entirely to yourself. For example, academic researchers are increasingly using simple contracts to support useful data sharing while also helping protect their data from misuse. Similarly, during the initial phase of the <a href=\"https://www.icij.org/investigations/panama-papers/\">Panama Papers</a> reporting, journalists working on the project were required to sign contracts about how they would handle both the data and their reporting processes -- an approach that succeeded in keeping the work safeguarded until the stories were ready to publish.</p>\n<p>If there are reasons that data cannot simply be published, journalists can still indicate that they are open to data-sharing requests. While this barrier alone may be sufficient to deter many bad actors, requiring anyone wishing to use the data to sign a simple contract agreeing not to use or disclose it in certain ways offers an additional layer of protection, as they could be held legally liable if they fail to protect the data as agreed. Although this approach obviously involves some risk, it often provides a good balance between protecting sensitive or personal information and allowing responsible parties to hold journalists to account.</p>\n\n                                                                      \n                            <h2>Conclusion</h2>\n<p>As the breadth and complexity of the broader data environment continues to grow, so, too, do the ethical challenges around reporting and publishing with such data. While the core considerations of news value and the public interest help to answer questions about what journalists should cover and how, the nature and scale of digital leaks and digital publishing have introduced new ethical issues that often need to be examined. However, just as with many other processes in digital journalism -- like verification -- creating a thoughtful, well-defined process for evaluating leaked data and deciding how it will be handled goes a long way to ensuring that your reporting efforts are not only efficient, but ethically sound.</p>\n\n                                                                      \n                            <p>For more on privacy and data journalism:</p>\n<ul>\n<li><a href=\"https://datajournalism.com/read/longreads/de-identification-for-data-journalists\">De-identification for data journalists</a></li>\n<li><a href=\"https://datajournalism.com/read/longreads/data-journalism-and-the-ethics-of-publishing-twitter-data\">Data journalism and the ethics of publishing Twitter data</a></li>\n<li><a href=\"https://datajournalism.com/read/longreads/ethical-questions-in-data-journalism-and-the-power-of-online-discussion\">Ethical questions in data journalism and the power of online discussion</a></li>\n</ul>\n\n                                              \n                ",
    "contentSnippet": "At its most core, the essential work of journalism is to gather and verify non-public information, evaluate its potential value to the public, and then -- if that value is substantial enough -- organise and publish it in such a way that it helps people make informed decisions about their lives. In a very important sense, this means that reporting and publishing around leaked data is no different than any other reporting: Once verified, the question of what to publish, and how, is driven principally by how it can best serve the public good. \nYet both the scale and detail of today's information leaks, especially when combined with a highly networked -- and therefore global -- publishing environment, means that modern leaks present substantive practical and the ethical considerations for journalists. In addition to protecting the source of the leaked information, journalists -- like everyone else -- must work in a digital environment where their activities are almost constantly and ubiquitously tracked, making it all too easy to inadvertently reveal the direction of an ongoing investigation. Moreover, because leaks are now often larger than any one journalist -- or journalistic organisation -- can typically handle, they present unique collaboration and publication challenges, all of which must be carefully engineered to balance efficacy, transparency, and privacy. \nDespite these complexities, there are a range of useful methods and heuristics that can help journalists make ethical decisions about using secret, sensitive, or personal data. In fact, the most important -- and sometimes the most significant -- challenge is for journalists to accurately recognise when an ethical situation exists, especially in the fast-paced world of online publishing. For example, both our research and our professional experience indicates that when journalists accurately perceive the sensitivity of the information they have obtained, they handle it with more caution and care. Yet our work also indicates that journalists typically rely on their sources' judgment when assessing the sensitivity of the information they are being given. This suggests that while sources can, in general, trust journalists to handle leaked data carefully, it may also lead to problematic oversights when journalists report on data that has been simply ‘dumped’ online, especially since the sensitivity and/or significance of that data may not be immediately obvious.\nWikileaks has faced criticism for its data dumps, which have at times left sensitive and personal data exposed.\nIn our view, the mechanism through which data is obtained does not change the ethical standards with which it should be treated. If anything, sensitive information made accessible by a hack or a leak deserves more careful handling, since the agendas of those that have made the information public are uncertain at best. Moreover, the smash-and-grab data collection methods that are typical of hacks and breaches virtually ensures that the information of many private individuals -- whose only failing, often, is having been in a database along with someone or something of interest -- will be swept up along with anything potentially newsworthy. As such, journalists need to take particular care that their work does not implicate or injure those who are only 'guilty by association’. It is because of this, in part, that ‘leaked’ data may in fact demand more thoughtful handling than material provided by a confidential source.\nHacked, breached, or leaked: key considerations when reporting with private or personal data\nAs we noted above, journalists can typically be relied upon to think carefully about how they handle sensitive data provided by a trusted or confidential source, in part because a human source will often impress upon the journalist the risks of mishandling the information. The journalist's desire to preserve valuable source relationships (and her own reputation) may also help counterbalance the impulse to publish material that may be more salacious than newsworthy. When data is simply 'dumped' online with little or no context, however, this source-oriented conscientiousness may go out the window. Combine this with the pressure journalists may feel from editors or competitors to get a story out quickly, and it can be hard to justify the delay required to weigh how significant the story in leaked data set actually is.\nTo help illustrate how the lack of a ‘human’ source -- especially when coupled with the exciting and even illicit nature of leaked data -- can raise crucial ethical challenges, we'll look at the issues presented by the data made public through two prominent hacks: the Sony email hack and the Ashley Madison hack.\nIn late November 2014, a massive dump of internal data from Sony Pictures was posted on the data-sharing site Pastebin, following a months-long hack of the entertainment company's systems. The hacked data contained everything from email exchanges to personnel files, and the question of what motivated the attack -- as well as who had perpetrated it -- quickly dominated headlines across the news spectrum. Yet many of the articles examining the contents of the leaked data also leaned toward the tabloid, focusing on executives' nasty exchanges and celebrity name-calling.\nA snapshot of the Guardian’s reporting on the Sony email hack.\nAlthough many may find it hard to muster sympathy for these powerful and high-profile individuals, the coverage's focus on the machinations of Hollywood dealmaking and franchise evolutions also threatened to overshadow many of the substantive issues revealed by the documents, such as industry-wide coordination on lobbying efforts designed to reshape the way that online content is served, or the fact that the data revealed the social security numbers, home addresses, and salaries of tens of thousands employees. \nThe emphasis in the total coverage of the Sony hack, moreover, stands in stark contrast to coverage of similarly controversial leaks like the Snowden documents, where the focus has remained on the actions of powerful companies and nation-states, rather than on the foibles and gaffes of the individuals involved.\nLess than a year after the Sony Pictures hack, another high-profile hack and data dump made particularly intimate details of a large number of people's lives essentially public online. In this case, however, the individuals whose information was posted were not celebrities or Hollywood dealmakers, but people from all walks of life who had joined an online dating site purportedly designed to facilitate extramarital affairs.\nThe Ashley Madison website offers a dating service for married individuals.\nAlthough the Ashley Madison hack generally contained less personally identifiable information than the Sony Pictures hack, the ramifications of the breach for those affected were sometimes devastating: multiple suicides were attributed to the hack, with widespread blackmailing campaigns, lost political careers, marriages, and community relationships resulting from the fallout. While there was some substantive reporting to be done on aspects of the leaked data -- for example, on the potentially inappropriate use of the service from government offices, and assessment of the fraud claims that had been made by former users -- reporting that simply plucked individuals out of the Ashley Madison databases and treated them as 'sources' for additional comment may well have done more to traumatise people who had already been victimised by the breach in the first place. And while there is no doubt that accountability reporting can sometimes have negative consequences for those whom it covers, as journalists we must make every reasonable effort to ensure that those consequences are reserved for those legitimately suspected of actual wrongdoing, and not simply on people whose choices may differ from our own.\nInverting implicit biases\nA key component of the ethical challenge presented by cases like those above stems from our need, as journalists, to confront our own implicit biases. Whether or not we find the subjects of leaked data likable or even sympathetic, we must carefully weigh the news value of reporting with leaked data and the privacy interests of the people we are reporting on. As we’ve discussed, this can be particularly difficult to do when the data in question is simply posted online, since these datasets lack a human source to remind us of the potential ramifications of publishing. Importantly, this loss of context often also obscures the motives of the people who obtained and/or posted the hacked data in the first place -- something that should arguably be a core focus of the reporting on it.\nBy their very nature, of course, our own biases are difficult to counter. Consulting with colleagues and editors is always a good place to start sanity-checking our first judgements. Another strategy is to use a simple thought experiment: If the contents of the leak were something that we personally believe should be private’ -- perhaps individuals' HIV status, for example -- would we still report on it, and how? Especially when we consider that most of us are unwilling subjects of data collection in the first place, it's important that journalists consider how to minimise additional harm to any individuals whose personal information they are working with -- no matter how it came into their hands.\nAt times, however, reporting with sensitive and/or personal information is unavoidable, and may be essential to an important piece of accountability journalism. Where that is the case, there are still a number of ways that journalists can do the reporting and publishing that they need to while minimising potential harm as much as possible. Though today's networked data environment means there are few guarantees to be had, a clearly defined and thoroughly considered process -- especially one conducted in consultation with experienced colleagues -- can help journalists be confident in the appropriateness of their reporting and publication choices when dealing with hacked, leaked, and otherwise sensitive data.\nConsiderations for reporting\nGood security helps ensure privacy protections\nWhile we are unabashed advocates for strong information security practices, a key reason for this is the protections that they allow journalists to provide for their human sources as well as any potentially sensitive data resources that they may have. Although many companies affected by data leaks and breaches have far more resources to dedicate to security, journalists have demonstrated a unique ability to protect information. The first step in treating personal data ethically is to take reasonable precautions to ensure that it does not slip out of your control. A great resource for beginning to enhance your own security know-how is the Electronic Frontier Foundation's Surveillance Self-Defense website, which has everything from a security ‘starter pack’ to guides on particular tools.\nThere are many resources available online for journalists to start building up their security capability.\nTake care when verifying\nWhile many appeared to relish The Intercept's apparent missteps when verifying documents allegedly provided to them by NSA contractor Reality Leigh Winner, it can be hard to know what information may tip off interested parties as you verify elements of a story. \nIf you are relying on web searches, consider using a location-masking browser like Tor to make it more difficult for online service providers to infer what you are researching. If you are relying on human sources, consider what you know about the provenance of the data you are dealing with. If it is genuine, ask yourself: What is the likely position of someone who would have access to it, and what might lead another person to guess where it came from? Use your answers to guide what information you share with whom when verifying.\nNo matter the circumstance, however, it is always wiser to avoid sharing original documents. Instead, retype segments of content (correcting obvious spelling and punctuation errors) that you need to reveal -- many organisations will distribute sensitive documents with unique typographical or formatting features, so that they can pinpoint the source of an internal leak if originals show up online.\nTo protect their source, Axios retyped the content of leaked White House schedules.\nFinally, it's always a good idea review documents (especially PDFs, .doc/x files and spreadsheets) on an old computer that will never be connected to the internet (use a thumb drive to move them there). While this may seem cumbersome at first, it also helps protect your other information (and your organisation) from the malware and viruses that leaked data may contain. This is especially true if you plan to print hard copies of documents -- the ‘enable editing’ permission required for printing can also put your own computer's data at greater risk.\nConsiderations for publishing\nHow much is enough?\nProviding reasonable privacy protections can sometimes seem at odds with imperatives around both accountability and transparency. When it comes to publishing data, however, the choice is not ‘all or nothing’. In fact, digital publishing gives journalists a range of ways to strike a balance between protecting private data and being transparent about their work. \nNaturally, any data to be published must first be verified; this alone will limit the incidental exposure of personal information, since verification -- in addition to being a hallmark of responsible journalism -- is incredibly time-consuming.\nOnce verified, there is a question of relevance: Is the personal information you plan to publish essential to the story, or not? This can be a difficult question to answer. But just as we should reflect on why we might include information in a story about someone's age, race, immigration status, or other demographic attributes, we should relevance-test personal information we intend to publish. In short: Does the story really need it? This is especially important to consider given the ripple-effect of revealing personal information about someone. Family members, work colleagues, and -- in this age of social media -- even casual acquaintances may be affected by the revelation of your subject's personal details. Moreover, since most journalism today is inevitably published into a global context, the norms to consider when publishing personal details are not confined to a single region or culture. In general, you should be as conservative as possible without sacrificing the integrity of the story. Only after doing a thorough risk-benefit analysis, which keeps the costs to the individual at the center of your reasoning, should you make a determination about what to publish.\nBalancing privacy and transparency\nJust as journalism seeks to hold power to account, journalists should make themselves accountable to the public as well. Where possible, this means sharing data, sharing code, and providing detailed methodologies for the stories that you produce. \nYet, in many cases, wholesale publication of data or documents may violate the privacy of innocent people, or even put them at risk. In these cases, there are a number of methods that journalists can use to make key information public.\nRedaction\nTools like DocumentCloud allow journalists to both publish documents and retain fine-grained control over them, offering both redaction and annotation tools. Journalists hoping to redact information from documents are advised to do so with tools designed for the purpose, as some methods (for example, drawing black boxes in Adobe, or ‘hiding’ Excel columns or rows) are easily undone. It's also important to keep in mind that simple methods of de-identification (such as removing names and addresses) are often insufficient to protect people's identities, given how much information is available to cross-reference online.\nAn example of a redacted CIA document. Source: Wikimedia.\nIs the personal information you plan to publish essential to the story, or not?\nSamples and summaries\nAnother way that journalists can help protect individuals' privacy when publishing private data is to publish only a curated sample of data, or to publish data that has been summarised to the point that re-identification will be difficult, if not impossible. As above, however, the vast quantity of information available online means that both samples and summaries must be carefully designed to avoid revealing more than intended. We suggest consulting with a statistician if possible to help ensure that the measures taken are sufficient. A good place to start is to remove any obviously identifying information combinations, and then using the fully reported story to help guide your thinking about what samples or summaries will further clarify that story without inadvertently exposing individuals' information.\nVisualisation\nVisualisation is itself often a way of aggregating data in such a way that meaningful patterns are revealed in otherwise heterogeneous datasets. An advantage of visualisation for privacy protection is that a very small subset of data features are typically needed to create a visualisation, and, if well done, they add real value to the story being told. Static graphics, in addition to being more platform-friendly, also naturally limit the amount of inference and post-publication manipulation that is possible.\nWhen the Journal News published an interactive map of gun permit holders following a school shooting in 2012, many were outraged by the apparent privacy invasion.\nIn 2011, the Guardian was careful to publish maps about the UK Riots such that the underlying information was not accessible, out of concern for the type of backlash that the Journal News ultimately suffered. Both of these maps are now currently offline.\nMaps published with the Fusion series Suspect City illustrate where and how different age groups were stopped by police in Miami Gardens, Florida. Because many of those stopped were vulnerable or underage, these visualisations show the patterns of stops without revealing other personal details.\nSelective release\nSometimes a journalist may find themselves in possession of a unique trove of data that they simply do not have the resources to investigate as thoroughly as they would like. Happily, however, digital technologies remove the requirement to choose between a ‘data dump’ and keeping information entirely to yourself. For example, academic researchers are increasingly using simple contracts to support useful data sharing while also helping protect their data from misuse. Similarly, during the initial phase of the Panama Papers reporting, journalists working on the project were required to sign contracts about how they would handle both the data and their reporting processes -- an approach that succeeded in keeping the work safeguarded until the stories were ready to publish.\nIf there are reasons that data cannot simply be published, journalists can still indicate that they are open to data-sharing requests. While this barrier alone may be sufficient to deter many bad actors, requiring anyone wishing to use the data to sign a simple contract agreeing not to use or disclose it in certain ways offers an additional layer of protection, as they could be held legally liable if they fail to protect the data as agreed. Although this approach obviously involves some risk, it often provides a good balance between protecting sensitive or personal information and allowing responsible parties to hold journalists to account.\nConclusion\nAs the breadth and complexity of the broader data environment continues to grow, so, too, do the ethical challenges around reporting and publishing with such data. While the core considerations of news value and the public interest help to answer questions about what journalists should cover and how, the nature and scale of digital leaks and digital publishing have introduced new ethical issues that often need to be examined. However, just as with many other processes in digital journalism -- like verification -- creating a thoughtful, well-defined process for evaluating leaked data and deciding how it will be handled goes a long way to ensuring that your reporting efforts are not only efficient, but ethically sound.\nFor more on privacy and data journalism:\nDe-identification for data journalists\nData journalism and the ethics of publishing Twitter data\nEthical questions in data journalism and the power of online discussion"
  }
]
