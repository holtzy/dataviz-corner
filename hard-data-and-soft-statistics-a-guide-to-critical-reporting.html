<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/dd8d3b5be9662933.css" as="style"/><link rel="stylesheet" href="/_next/static/css/dd8d3b5be9662933.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-59c5c889f52620d6.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-f11614d8aa7ee555.js" defer=""></script><script src="/_next/static/chunks/pages/_app-891652dd44e1e4e1.js" defer=""></script><script src="/_next/static/chunks/996-eeb5175dbd5dba8f.js" defer=""></script><script src="/_next/static/chunks/36-94b5e24e03efc6db.js" defer=""></script><script src="/_next/static/chunks/pages/%5B...slug%5D-af748dcc13a25fcb.js" defer=""></script><script src="/_next/static/sFP8FhENNxqFJqEgxMEWp/_buildManifest.js" defer=""></script><script src="/_next/static/sFP8FhENNxqFJqEgxMEWp/_ssgManifest.js" defer=""></script></head><body><div id="__next"></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"creator":"Morten Jerven","title":"Hard data and soft statistics: a guide to critical reporting","link":"https://datajournalism.com/read/longreads/hard-data-and-soft-statistics-a-guide-to-critical-reporting","pubDate":"Sat, 31 Aug 2019 06:56:00 +0200","author":"Morten Jerven","content":"\n                                                                        \u003cp\u003eIt is generally believed by the layman, the expert, and the journalist that numbers are hard and judgements are soft. It means that when we see a number, or a statistic, we think of it as objective, accurate, and incontestable. However, when we hear that someone consider, believes, or has an opinion our sceptical mind awakens. But often numbers are far softer than we commonly assume. Basic metrics such as inflation, or debt as a share of GDP, have been shown to change radically after revisions and, at times, they have been revealed to be fraudulent. It turns out that just as we would take one person’s view as an anecdotal observation that needs to be questioned, numbers and statistics should also be subject to serious cross examination.\u003c/p\u003e\n\n                                                                                                                                \u003cp\u003eMorten Jerven is a Professor at the Norwegian University of Life Sciences and a Visiting Professor in Economic History at Lund University. He's published widely on economic development statistics and authored three books, including \u003ca href=\"https://www.amazon.com/Poor-Numbers-Development-Statistics-Political/dp/080147860X\"\u003ePoor Numbers: How We Are Misled by African Development Statistics and What to Do about It\u003c/a\u003e.\u003c/p\u003e\n\n                                                                                                                              \n                            \u003ch2\u003eWhy and how are statistics used?\u003c/h2\u003e\n\u003cp\u003eThere are many reasons why numbers and statistics are so popular in usage, but perhaps the most important one is because they allow us to say something and make decisions about things we know nothing about. An important example for finance journalism comes from Bond Credit Ratings -- where countries are \u003ca href=\"https://en.wikipedia.org/wiki/Bond_credit_rating\"\u003erated ‘Triple A’\u003c/a\u003e or not, which in turn allows investors to decide whether they should put their millions in, say, a Cypriote bond or not. \u003c/p\u003e\n\u003cp\u003eWithout the Triple A rating, the investor would have no signal, no information, and no basis whatsoever to make that investment decision. The Triple A rating allows the investor to sort countries, which they may have no knowledge of, and invest based on this signal of the economy’s credit worthiness. Should Cyprus boast a good rating, for example, investors may buy bonds without knowing whether it is a sovereign state, or whether it is a dependency of Italy, Greece, Turkey, or Great Britain, let alone what its capital and main export is, or the name of the currency. It further allows the investor to change their mind about the investment when the rating changes -- even if this change only reflects the mood of investors, rather than the economy’s stability.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eFinancial ratings of European states by Standard \u0026amp; Poor's rating on 18 February 2019. Credit: \u003ca href=\"https://en.wikipedia.org/wiki/File:Europe_Standard_and_Poor%27s.jpg\"\u003eWikimedia\u003c/a\u003e (\u003ca href=\"https://creativecommons.org/licenses/by-sa/4.0/deed.en\"\u003eCC BY-SA 4.0\u003c/a\u003e).\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eThe fact that numbers are not always objective reflections, based on accurate readings and observations, is easy to forget. After all, some of the key metaphors we use to report financial numbers are often taken from meteorology, where we do indeed have ways of measuring rainfall, temperatures, and wind. These phenomena do exist, regardless of whether we measure them or not. But that is not true of all things we measure and, for a lot of the numbers that are regularly reported, it is important to be wary that the process of measuring does not in itself constitute the existence of a phenomenon.\u003c/p\u003e\n\n                                                                      \n                            \u003ch2\u003eMeasuring the immeasurable\u003c/h2\u003e\n\u003cp\u003eThere is no way of objectively inserting a measurement stick to determine a Triple A rating, and much less is there any way of reading other subjective phenomena. Take corruption, for example. Corruption is one of those things that is undeniably important and on the forefront of what journalists should report on, but there is no way the level of corruption can be objectively gauged. By its nature, corruption takes place in secret and concealed forms, and it’s possible to argue that the extent to which it is acceptable or not depends a lot on the specific situation, the cultural setting, and variations in law.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003e\u003ca href=\"https://www.flickr.com/photos/87913776@N00/4363265760\"\u003efutureatlas.com\u003c/a\u003e on Flickr (\u003ca href=\"https://creativecommons.org/licenses/by/2.0/\"\u003eCC BY 2.0\u003c/a\u003e).\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eYet, we want to measure it, and there are many rankings that purport a definitive picture of corruption across countries. To understand the limitations of these indices, let’s take a look at a hypothetical: If you stopped one random European person in the street and asked them “how corrupt is Nigeria?” and the person responded “oh very corrupt, I don’t trust African people at all, they don’t follow rules like we do in Europe”, you should, and I think most journalists would, dismiss this as unreportable prejudice. However, if an organisation, like Transparency International (TI), calls up 100 people in a survey and asks them to rate Nigeria and Sweden on a corruption scale from 1 to 10, this number and the resultant \u003ca href=\"http://www.transparency.org/research/cpi/overview\"\u003eCorruption Perception Index\u003c/a\u003e becomes headline news.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003e\u003ciframe width=\"800\" height=\"400\" src=\"https://www.youtube-nocookie.com/embed/OXApeTYRYNQ\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eThese indices are created to be influential and the channel by which they gain most of their influence is through being re-reported by journalists as an accurate reflection of a country’s socio-political circumstances. The truth is: TI’s Corruption Perception Index is just an average subjective perception index, which reflects the prejudices people have about governance in poor countries. Corruption is not observable, and nor are many other social-political phenomena, so reporting on them without reporting on how the data is generated should not pass a journalist’s basic fact-checking tests.\u003c/p\u003e\n\n                                                                      \n                            \u003ch2\u003eFact-checking a statistic\u003c/h2\u003e\n\u003cp\u003eFact-checking a statistic requires more than checking a claim by turning to statistics. In 2014, The Economist \u003ca href=\"https://www.economist.com/leaders/2014/11/08/how-to-lie-with-indices\"\u003ewrote a guide on lying with indices\u003c/a\u003e, which laid out the dirty tricks that are in use by the compilers of rankings, indices, and other numbers that summarise the world. Journalists can seek help from researchers and institutions that use these numbers in their work, as they normally know their weaknesses, and there is an increasing area of critical scholarship that questions the effects of quantification. Simple first questions that journalists might ask themselves:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIs this phenomena objectively observable? There is, for example, a difference between recording rainfall and happiness.\u003c/li\u003e\n\u003cli\u003eUnder what conditions was the issue observed? Numbers on rape victims might be very difficult to observe, compared to cars crossing a toll bridge.\u003c/li\u003e\n\u003cli\u003eWho made the observation? Consider whether they would have any reason to present a biased measure.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eJust as we would subject a witness statement to cross examination, we should also tear apart a statistic when it is presented to us.\u003c/p\u003e\n\n                                                                                                 \u003cp\u003e\u003cstrong\u003eCase study: Fact-checking subjective indices\u003c/strong\u003e \u003c/p\u003e\n\u003cp\u003eBy \u003ca href=\"https://datajournalism.com/contributors/KateWilkinson\"\u003eKate Wilkinson\u003c/a\u003e, Africa Check \u003c/p\u003e\n\u003cp\u003eIn 2014, Africa Check \u003ca href=\"https://africacheck.org/reports/is-sa-bottom-of-the-class-in-maths-and-science-why-ranking-is-meaningless/\"\u003einvestigated\u003c/a\u003e reports that South Africa’s maths and science education was the ‘worst in the world’.\u003c/p\u003e\n\u003cp\u003eThese regular headlines were based on the findings of the World Economic Forum’s (WEF) Global Information Technology Reports, which rank countries on the quality of their math and science education. In the \u003ca href=\"http://www3.weforum.org/docs/GITR2016/WEF_GITR_South_Africa_2016.pdf\"\u003elatest report\u003c/a\u003e from 2016, South Africa was again ranked last out of 139 countries.  \u003c/p\u003e\n\u003cp\u003eBut the WEF does not conduct standardised tests to assess the quality of maths and science education in the countries surveyed. Rather, the rankings are the result of an ‘executive opinion survey’, where unidentified ‘business leaders’ are asked to rate the quality of maths and science education in their country on a scale from 1 (worst) to 7 (best). \u003c/p\u003e\n\u003cp\u003eThe resulting education rankings are not, in fact, an assessment of the quality of education in South Africa, or any of the other countries. Instead, this subjective index reflects the personal opinions of a small group of unidentified people about a topic in which they are not an expert.  \u003c/p\u003e\n\u003cp\u003eIn fact-checking this statistic, we spoke to leading education experts in South Africa. They were able to critique the ranking and point us to the most recent and reliable education rankings. \u003c/p\u003e\n\u003cp\u003eFor instance, Martin Gustafsson, an economics researcher at the University of Stellenbosch, told us: “There is valuable data in the [WEF] report. For things like business confidence it is useful. But you can’t apply opinions to things like education. It is like asking business experts what they think the HIV rate is”.  \u003c/p\u003e\n\u003cp\u003eFor journalists reporting on \u003ca href=\"https://africacheck.org/factsheets/guide-assessing-south-africas-schooling-system/\"\u003eeducation levels\u003c/a\u003e, looking into other sources provides a more comprehensive view of the country’s performance. For example, standardised, cross national testing \u003ca href=\"http://www.sacmeq.org/reports.htm\"\u003ereveals\u003c/a\u003e that South Africa does have problems with its maths education, but it performs better than a number of countries. In 2007, the Southern and Eastern Africa Consortium for Monitoring Educational Quality \u003ca href=\"http://resep.sun.ac.za/wp-content/uploads/2012/07/Spaull-2012-SACMEQ-at-a-Glance-10-countries.pdf\"\u003eranked\u003c/a\u003e South Africa eighth out of fifteen countries for it’s math performance. Mozambique, Uganda, Lesotho, Namibia, Malawi and Zambia all performed worse. \u003c/p\u003e\n\u003cp\u003eRemember: Make sure to check whether a ranking is measuring the \u003cem\u003eactual\u003c/em\u003e phenomena or simply what people \u003cem\u003ethink\u003c/em\u003e about it. Always look for additional sources and expert help to corroborate or contextualise a subjective index.\u003c/p\u003e\n\n                                                                     \n                            \u003ch2\u003eThe impact of statistical reporting\u003c/h2\u003e\n\u003cp\u003eUncritical reporting on subjective indices does, of course, have real consequences. The more frequently soft statistics are re-reported, the more likely we’ll bring them into the fold of our arsenal of hard statistics, forgetting that ‘corruption’ is not a quantity that can be easily gauged. There are consequences for pretending that you can measure something that you cannot -- as it may seriously mislead us to think we know something we do not -- and, in the end, these can translate into very bad decisions.\u003c/p\u003e\n\u003cp\u003eOne of these bad decisions was made in 2015, when the UK’s Financial Conduct Authority \u003ca href=\"http://www.fca.org.uk/static/documents/final-notices/bank-of-beirut.pdf\"\u003epenalised\u003c/a\u003e the Bank of Beirut for failing to establish sufficient controls against money laundering and other financial crimes. As part of this decision, the Authority banned the Bank from taking on customers in ‘high-risk’ corruption jurisdictions. But there is no list of countries that are corrupt or not, or an objective measurement, so they reached for the second best thing: TI’s Corruption Perception Index.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.cgdev.org/blog/why-arbitrary-measures-money-laundering-risk-are-nonsensical-and-unfair\"\u003eAs commentators\u003c/a\u003e pointed out at the time, there is a close to perfect correlation in a country’s GDP per capita and its ranking on the Corruption Perception Index. So, in effect, low income countries were not getting the banking services of high income countries, simply because the people surveyed by TI suspected that these lower income countries are more corrupt.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eThe Center for Global Development \u003ca href=\"https://www.cgdev.org/blog/why-arbitrary-measures-money-laundering-risk-are-nonsensical-and-unfair\"\u003emapped\u003c/a\u003e the countries which the FCA considered to be ‘high risk’, that is, any ranking 60 or below on the Index.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eCorruption reportage is something that we need to our focus attention on, along with other similar ‘unknowns’ in the list of global problems, like trafficking, drugs, and illicit finance. Increasingly, organisations that are involved in promoting political traction on these issues find themselves involved in a numbers game. There is always going to be an incentive to highlight the bigness of a problem to help further the cause. And there is no reason to doubt that TI, or other organisations such as Walk Free, which publishes a\u003ca href=\"https://www.theguardian.com/global-development/poverty-matters/2014/nov/28/global-slavery-index-walk-free-human-trafficking-anne-gallagher\"\u003e Global Slavery Index\u003c/a\u003e, are not aware of the flawed nature of the data. But they make a calculated risk where they hope that the upside, in terms of more influence, is bigger than the downsides of mismeasuring.\u003c/p\u003e\n\u003cp\u003eTo combat superficial reporting, and avoid exacerbating its impacts, journalists need to dig deeper and question clickbait indicators. Strategies include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eContacting researchers and experts in the field -- they’ll be able to comment on the usefulness of subjective indicators, as well as any limitations\u003c/li\u003e\n\u003cli\u003eLooking beyond numbers, by producing investigative stories that look for information which isn’t considered or communicated by an indicator\u003c/li\u003e\n\u003cli\u003eAsking yourself how the story would hold if you used a different measure or a different data source to frame the narrative.\u003c/li\u003e\n\u003c/ul\u003e\n\n                                                                                                 \u003cp\u003e\u003cstrong\u003eCase study: Reporting on corruption\u003c/strong\u003e  \u003cp\u003e  By \u003ca href=\"https://datajournalism.com/contributors/KateWilkinson\"\u003eKate Wilkinson\u003c/a\u003e, Africa Check  \u003c/p\u003e\n\u003cp\u003eHow much money has South Africa lost due to corruption since democracy started in 1994? A popular and widely shared estimate is R700 billion. This figure has been published by newspapers and tweeted by a prominent trade union leader -- but, as Africa Check’s \u003ca href=\"https://africacheck.org/reports/has-sa-lost-r700-billion-to-corruption-since-1994-why-the-calculation-is-wrong/\"\u003e2015 fact-check\u003c/a\u003e revealed, it’s a thumbsuck. \u003c/p\u003e\n\u003cp\u003eUnderpinning these reports, a civil society handbook claimed that ‘damages from corruption’ is usually estimated at ‘between 10% and 25%, and in some cases as high as 40 to 50%’ of the country’s public procurement contracts. But no source was provided to substantiate these estimates.  \u003c/p\u003e\n\u003cp\u003eAs years passed, the claim changed. It was then reported that around 20% of the country’s \u003cem\u003egross domestic product\u003c/em\u003e -- not procurement contracts -- was lost every year to corruption.  \u003c/p\u003e\n\u003cp\u003eSo, how much has corruption cost South Africa? The frustrating -– and logical –- answer is that we just can’t say for sure.  \u003c/p\u003e\n\u003cp\u003eThe country’s treasury has not attempted to calculate an estimate. And while governance experts agree that a large amount of money has been lost, they won't be drawn on an exact number.  \u003c/p\u003e\n\u003cp\u003eWith little information available, journalists should be wary of definitive reports on national corruption levels. Instead, it may be possible to piece together a picture of corruption in a country by bringing together different sources.  \u003c/p\u003e\n\u003cp\u003eNational surveys are one resource that can be used to shed light on people's experience of bribery and corruption. For example, in 2016 nearly a third (32.3%) of adults in Nigeria \u003ca href=\"https://africacheck.org/factsheets/factsheet-everyday-corruption-in-nigeria-who-is-on-the-take/\"\u003ereported paying bribes\u003c/a\u003e to a public official or said that they were asked to in the year before. But, as always, all surveys should be interrogated and corroborated with country-level experts.\u003c/p\u003e\n\n                                                                     \n                            \u003ch2\u003eTrust and mistrust in official numbers\u003c/h2\u003e\n\u003cp\u003eSo far, we’ve looked at how numbers tend to be reported as hard facts, and now we’ll move onto its exception: when statistical reporting is linked to foul play. Perhaps a country is accused of skewing incomes to receive aid, or another country is seen to downplay the social impacts of a political crisis; on the other end of the spectrum, overly critical reporting of statistic can also be problematic. The fact that the social world is complex and difficult to understand means that even quantitative phenomena don’t lend themselves to cheap and fast summaries that are easily reportable. To illustrate this point, let’s look at something that should be relatively easy to count – namely money. But turns out that numbers are soft here too.\u003c/p\u003e\n\n                                                                                                  \n                                \u003cp\u003eMeasuring economic activity is hard at the best of times, let alone when faced with the challenges presented in low income countries. Credit: \u003ca href=\"https://en.wikipedia.org/wiki/File:Ghana_Cedi_banknotes.jpg\"\u003eWikimedia\u003c/a\u003e.\u003c/p\u003e\n\n                                                                                        \u003cp\u003eOn 5 November 2010, Ghana’s Statistical Services\u003ca href=\"https://www.theguardian.com/business/2012/nov/20/economics-ghana\"\u003e announced new and revised GDP estimates\u003c/a\u003e. Overnight, the size of the economy was adjusted upward by over 60%, which meant that previous GDP estimates had missed about US$13 billion worth of economic activity. While this change in GDP was exceptionally large, it turned out to be far from an isolated case.\u003c/p\u003e\n\n                                                                                                  \n                            \u003cp\u003eIn 2012, I wrote\u003ca href=\"https://africanarguments.org/?attachment_id=12884\"\u003e a summary of this\u003c/a\u003e situation, explaining in layman terms how one country, like Ghana, could go from being so poor one day to an aspiring middle-income one the next. My intent with the piece was to demystify the process and to lay bare the basic discrepancies between global standards of measurement and local challenges of data availability and resources. The simple fact is: It is very demanding and costly to measure a country’s whole economy, particularly in low income countries where very few businesses and individuals are reporting taxes, and only a minority of economic transactions are recorded, with most taking place in the informal, unrecorded economy. Yet, despite these nuances, when the Guardian reprinted the story, they smacked the headline:\u003ca href=\"http://www.guardian.co.uk/business/2012/nov/20/economics-ghana\"\u003e Lies, damn lies and GDP\u003c/a\u003e on it. As anyone who has read that piece, or\u003ca href=\"https://www.amazon.com/Poor-Numbers-Development-Statistics-Political/dp/080147860X\"\u003e my book\u003c/a\u003e will know, I go to some lengths to dispel the beliefs that there was a hidden political agenda behind this revision.\u003c/p\u003e\n\u003cp\u003eWhile critical skills often fail when it comes to thinking about converting complex realities into simple numbers, data users also manage to maintain an inherent criticism of any ‘official number’, based on simply distrusting some states and trusting others. A similarly misguided gut reaction exists among scholars, who may never trust a number from Sudan, Ghana or South Africa, but would not hesitate to use the same number if the World Bank recycled it.\u003c/p\u003e\n\n                                                                                                 \u003cp\u003e\u003cstrong\u003eCase study: Promoting trust through fact-checking national statistics \u003c/strong\u003e  \u003cp\u003e  By \u003ca href=\"https://datajournalism.com/contributors/KateWilkinson\"\u003eKate Wilkinson\u003c/a\u003e, Africa Check  \u003c/p\u003e  \u003cp\u003e  In 2018, Africa Check \u003ca href=\"https://africacheck.org/reports/no-china-does-not-own-more-than-70-of-kenyas-external-debt/\"\u003einvestigated\u003c/a\u003e claims by US-based news website Quartz that much of Kenya’s borrowing in recent years has originated from China. Further, they \u003ca href=\"https://qz.com/1324618/china-is-kenyas-largest-creditor-with-72-of-total-bilateral-debt/amp/\"\u003ereported\u003c/a\u003e that the country’s obligations to Beijing run ‘much deeper than many ordinary Kenyans realise’, under the headline: ‘China now owns more than 70% of Kenya’s external debt’.  \u003c/p\u003e  \u003cp\u003e  The size of Kenya’s public debt was a campaign issue in the country’s 2017 elections and this debt is sometimes conflated with China’s role in Kenya. The Asian country financed the \u003ca href=\"http://krc.co.ke/standard-gauge-railway/\"\u003estandard gauge railway project\u003c/a\u003e, the most visible of the government’s economic growth projects. Yet, its full cost remains unclear because the agreement is closed.   \u003c/p\u003e  \u003cp\u003e  Inaccurate reporting on the issue breeds mistrust in the Kenyan government and its official treasury data.  \u003c/p\u003e  \u003cp\u003e  The first step in fact-checking this claim was to find out what information it was based on. Quartz said that the source of the information was an article in the Nairobi-based Business Daily newspaper. That article had relied on information from the 2018/19 Kenyan budget statement which showed that Kenya owed China KSh534.1 billion. This, they said, was 72% of the country’s total bilateral debt of KSh741 billion.  \u003c/p\u003e  \u003cp\u003e  But while it may be true for bilateral debt, it’s not true for all of Kenya’s foreign debt.   \u003c/p\u003e  \u003cp\u003e  Next, Africa Check sought out experts to explain and unpack the numbers.  \u003c/p\u003e  \u003cp\u003e   “All bilateral debt [is] external debt but not all external debt [is] bilateral debt,” said Odongo Kodongo, a financial economist and associate professor at Wits University’s business school.  \u003c/p\u003e  \u003cp\u003e  External debt is the total public and private debt that a country owes foreign creditors. It includes multilateral debt, commercial debt, bilateral debt, and guaranteed debt.  \u003c/p\u003e  \u003cp\u003e  Experts used Kenyan treasury documents to estimate that China’s share of Kenya’s external debt was KSh534.07 billion of KSh2.51 trillion as of 31 March 2018. This was equal to 21.3% -- not 70%.   \u003c/p\u003e  \u003cp\u003e  China’s role in Kenya is a controversial issue. This fact-check showed the public that official government data could be relied upon to determine if claims about the countries’ relations were true.  \u003c/p\u003e\u003c/p\u003e\n\n                                                                     \n                            \u003cp\u003eThe failure to reflect on the softness of statistics in general, and rather look to a higher authority, was also reflected in an episode a few years later. On 7 April 2014, the Nigerian Bureau of Statistics (NBS) \u003ca href=\"https://africanarguments.org/2014/04/08/what-does-nigerias-new-gdp-number-actually-mean-by-morten-jerven/\"\u003edeclared\u003c/a\u003e that their GDP estimates were also being revised upward to $510 billion, an 89% increase from the old estimate. It was controversial: The GDP revision was bigger than expected, and, the IMF’s Statistics Department had provided technical assistance for the revision, which they viewed as incomplete. But the IMF has no authority to endorse or not endorse statistics, so they couldn’t stand in the way when Nigeria wanted to release the new numbers.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eGDP information is released by the NBS in \u003ca href=\"https://nigerianstat.gov.ng/elibrary?queries[search]=GDP\"\u003equarterly reports\u003c/a\u003e. This infographic from the Q4 and 2018 full year report shows a substantial increase in the 2014 growth rate.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eWhen Yemi Kale, the director of NBS, announced the new numbers, they were keen to make sure that the credibility of the statistics was not undermined, well aware that the international media’s trust was low. This meant that the IMF Mission Chief to Nigeria was well prepared for the inevitable question as to whether they ‘endorsed’ the new numbers. When asked, he made a long \u003ca href=\"https://www.imf.org/ieo/files/completedevaluations/BP6_-_Data_and_Statistics_at_the_IMF%E2%80%94Quality_Assurances_for_Low-Income_Countries.PDF\"\u003estatement\u003c/a\u003e that ended by summarising how the IMF supported “the efforts being made to improve the statistics of the Federation, as a basis of sound decision making. Let me state that we endorse this wholeheartedly and will support Nigeria in this regard”. In the end, this statement became reported as ‘the IMF endorsed the numbers’, despite the broader messages of statistical capacity building that it contained.\u003c/p\u003e\n\u003cp\u003eSo, what would be a more conscientious way to report on this number? Rather than focusing on narratives of trust or distrust around the revision itself, possible stories could’ve looked at:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe challenges of measuring economic activity in low income countries\u003c/li\u003e\n\u003cli\u003eResourcing limitations at the NBS, which meant benchmark data for the GDP hadn’t been updated properly for more than a quarter of a century\u003c/li\u003e\n\u003cli\u003eThe role of the IMF, and its inability to endorse a country’s data.\u003c/li\u003e\n\u003c/ul\u003e\n\n                                                                      \n                            \u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eBy digging deeper, and looking beyond a simplistic numbers, journalists are able to provide more accurate and comprehensive reports on social issues. When confronted with a number on corruption, unemployment, or the size of the illegal economy, journalists should always think critically about where that number came from. Numbers are much softer than we would like to think, and we trust and mistrust them far more than justified by their accuracy.\u003c/p\u003e\n\n                                              \n                ","contentSnippet":"It is generally believed by the layman, the expert, and the journalist that numbers are hard and judgements are soft. It means that when we see a number, or a statistic, we think of it as objective, accurate, and incontestable. However, when we hear that someone consider, believes, or has an opinion our sceptical mind awakens. But often numbers are far softer than we commonly assume. Basic metrics such as inflation, or debt as a share of GDP, have been shown to change radically after revisions and, at times, they have been revealed to be fraudulent. It turns out that just as we would take one person’s view as an anecdotal observation that needs to be questioned, numbers and statistics should also be subject to serious cross examination.\nMorten Jerven is a Professor at the Norwegian University of Life Sciences and a Visiting Professor in Economic History at Lund University. He's published widely on economic development statistics and authored three books, including Poor Numbers: How We Are Misled by African Development Statistics and What to Do about It.\nWhy and how are statistics used?\nThere are many reasons why numbers and statistics are so popular in usage, but perhaps the most important one is because they allow us to say something and make decisions about things we know nothing about. An important example for finance journalism comes from Bond Credit Ratings -- where countries are rated ‘Triple A’ or not, which in turn allows investors to decide whether they should put their millions in, say, a Cypriote bond or not. \nWithout the Triple A rating, the investor would have no signal, no information, and no basis whatsoever to make that investment decision. The Triple A rating allows the investor to sort countries, which they may have no knowledge of, and invest based on this signal of the economy’s credit worthiness. Should Cyprus boast a good rating, for example, investors may buy bonds without knowing whether it is a sovereign state, or whether it is a dependency of Italy, Greece, Turkey, or Great Britain, let alone what its capital and main export is, or the name of the currency. It further allows the investor to change their mind about the investment when the rating changes -- even if this change only reflects the mood of investors, rather than the economy’s stability.\nFinancial ratings of European states by Standard \u0026 Poor's rating on 18 February 2019. Credit: Wikimedia (CC BY-SA 4.0).\nThe fact that numbers are not always objective reflections, based on accurate readings and observations, is easy to forget. After all, some of the key metaphors we use to report financial numbers are often taken from meteorology, where we do indeed have ways of measuring rainfall, temperatures, and wind. These phenomena do exist, regardless of whether we measure them or not. But that is not true of all things we measure and, for a lot of the numbers that are regularly reported, it is important to be wary that the process of measuring does not in itself constitute the existence of a phenomenon.\nMeasuring the immeasurable\nThere is no way of objectively inserting a measurement stick to determine a Triple A rating, and much less is there any way of reading other subjective phenomena. Take corruption, for example. Corruption is one of those things that is undeniably important and on the forefront of what journalists should report on, but there is no way the level of corruption can be objectively gauged. By its nature, corruption takes place in secret and concealed forms, and it’s possible to argue that the extent to which it is acceptable or not depends a lot on the specific situation, the cultural setting, and variations in law.\nfutureatlas.com on Flickr (CC BY 2.0).\nYet, we want to measure it, and there are many rankings that purport a definitive picture of corruption across countries. To understand the limitations of these indices, let’s take a look at a hypothetical: If you stopped one random European person in the street and asked them “how corrupt is Nigeria?” and the person responded “oh very corrupt, I don’t trust African people at all, they don’t follow rules like we do in Europe”, you should, and I think most journalists would, dismiss this as unreportable prejudice. However, if an organisation, like Transparency International (TI), calls up 100 people in a survey and asks them to rate Nigeria and Sweden on a corruption scale from 1 to 10, this number and the resultant Corruption Perception Index becomes headline news.\n\nThese indices are created to be influential and the channel by which they gain most of their influence is through being re-reported by journalists as an accurate reflection of a country’s socio-political circumstances. The truth is: TI’s Corruption Perception Index is just an average subjective perception index, which reflects the prejudices people have about governance in poor countries. Corruption is not observable, and nor are many other social-political phenomena, so reporting on them without reporting on how the data is generated should not pass a journalist’s basic fact-checking tests.\nFact-checking a statistic\nFact-checking a statistic requires more than checking a claim by turning to statistics. In 2014, The Economist wrote a guide on lying with indices, which laid out the dirty tricks that are in use by the compilers of rankings, indices, and other numbers that summarise the world. Journalists can seek help from researchers and institutions that use these numbers in their work, as they normally know their weaknesses, and there is an increasing area of critical scholarship that questions the effects of quantification. Simple first questions that journalists might ask themselves:\nIs this phenomena objectively observable? There is, for example, a difference between recording rainfall and happiness.\nUnder what conditions was the issue observed? Numbers on rape victims might be very difficult to observe, compared to cars crossing a toll bridge.\nWho made the observation? Consider whether they would have any reason to present a biased measure.\nJust as we would subject a witness statement to cross examination, we should also tear apart a statistic when it is presented to us.\nCase study: Fact-checking subjective indices \nBy Kate Wilkinson, Africa Check \nIn 2014, Africa Check investigated reports that South Africa’s maths and science education was the ‘worst in the world’.\nThese regular headlines were based on the findings of the World Economic Forum’s (WEF) Global Information Technology Reports, which rank countries on the quality of their math and science education. In the latest report from 2016, South Africa was again ranked last out of 139 countries.  \nBut the WEF does not conduct standardised tests to assess the quality of maths and science education in the countries surveyed. Rather, the rankings are the result of an ‘executive opinion survey’, where unidentified ‘business leaders’ are asked to rate the quality of maths and science education in their country on a scale from 1 (worst) to 7 (best). \nThe resulting education rankings are not, in fact, an assessment of the quality of education in South Africa, or any of the other countries. Instead, this subjective index reflects the personal opinions of a small group of unidentified people about a topic in which they are not an expert.  \nIn fact-checking this statistic, we spoke to leading education experts in South Africa. They were able to critique the ranking and point us to the most recent and reliable education rankings. \nFor instance, Martin Gustafsson, an economics researcher at the University of Stellenbosch, told us: “There is valuable data in the [WEF] report. For things like business confidence it is useful. But you can’t apply opinions to things like education. It is like asking business experts what they think the HIV rate is”.  \nFor journalists reporting on education levels, looking into other sources provides a more comprehensive view of the country’s performance. For example, standardised, cross national testing reveals that South Africa does have problems with its maths education, but it performs better than a number of countries. In 2007, the Southern and Eastern Africa Consortium for Monitoring Educational Quality ranked South Africa eighth out of fifteen countries for it’s math performance. Mozambique, Uganda, Lesotho, Namibia, Malawi and Zambia all performed worse. \nRemember: Make sure to check whether a ranking is measuring the actual phenomena or simply what people think about it. Always look for additional sources and expert help to corroborate or contextualise a subjective index.\nThe impact of statistical reporting\nUncritical reporting on subjective indices does, of course, have real consequences. The more frequently soft statistics are re-reported, the more likely we’ll bring them into the fold of our arsenal of hard statistics, forgetting that ‘corruption’ is not a quantity that can be easily gauged. There are consequences for pretending that you can measure something that you cannot -- as it may seriously mislead us to think we know something we do not -- and, in the end, these can translate into very bad decisions.\nOne of these bad decisions was made in 2015, when the UK’s Financial Conduct Authority penalised the Bank of Beirut for failing to establish sufficient controls against money laundering and other financial crimes. As part of this decision, the Authority banned the Bank from taking on customers in ‘high-risk’ corruption jurisdictions. But there is no list of countries that are corrupt or not, or an objective measurement, so they reached for the second best thing: TI’s Corruption Perception Index.\nAs commentators pointed out at the time, there is a close to perfect correlation in a country’s GDP per capita and its ranking on the Corruption Perception Index. So, in effect, low income countries were not getting the banking services of high income countries, simply because the people surveyed by TI suspected that these lower income countries are more corrupt.\nThe Center for Global Development mapped the countries which the FCA considered to be ‘high risk’, that is, any ranking 60 or below on the Index.\nCorruption reportage is something that we need to our focus attention on, along with other similar ‘unknowns’ in the list of global problems, like trafficking, drugs, and illicit finance. Increasingly, organisations that are involved in promoting political traction on these issues find themselves involved in a numbers game. There is always going to be an incentive to highlight the bigness of a problem to help further the cause. And there is no reason to doubt that TI, or other organisations such as Walk Free, which publishes a Global Slavery Index, are not aware of the flawed nature of the data. But they make a calculated risk where they hope that the upside, in terms of more influence, is bigger than the downsides of mismeasuring.\nTo combat superficial reporting, and avoid exacerbating its impacts, journalists need to dig deeper and question clickbait indicators. Strategies include:\nContacting researchers and experts in the field -- they’ll be able to comment on the usefulness of subjective indicators, as well as any limitations\nLooking beyond numbers, by producing investigative stories that look for information which isn’t considered or communicated by an indicator\nAsking yourself how the story would hold if you used a different measure or a different data source to frame the narrative.\nCase study: Reporting on corruption  \n  By Kate Wilkinson, Africa Check  \nHow much money has South Africa lost due to corruption since democracy started in 1994? A popular and widely shared estimate is R700 billion. This figure has been published by newspapers and tweeted by a prominent trade union leader -- but, as Africa Check’s 2015 fact-check revealed, it’s a thumbsuck. \nUnderpinning these reports, a civil society handbook claimed that ‘damages from corruption’ is usually estimated at ‘between 10% and 25%, and in some cases as high as 40 to 50%’ of the country’s public procurement contracts. But no source was provided to substantiate these estimates.  \nAs years passed, the claim changed. It was then reported that around 20% of the country’s gross domestic product -- not procurement contracts -- was lost every year to corruption.  \nSo, how much has corruption cost South Africa? The frustrating -– and logical –- answer is that we just can’t say for sure.  \nThe country’s treasury has not attempted to calculate an estimate. And while governance experts agree that a large amount of money has been lost, they won't be drawn on an exact number.  \nWith little information available, journalists should be wary of definitive reports on national corruption levels. Instead, it may be possible to piece together a picture of corruption in a country by bringing together different sources.  \nNational surveys are one resource that can be used to shed light on people's experience of bribery and corruption. For example, in 2016 nearly a third (32.3%) of adults in Nigeria reported paying bribes to a public official or said that they were asked to in the year before. But, as always, all surveys should be interrogated and corroborated with country-level experts.\nTrust and mistrust in official numbers\nSo far, we’ve looked at how numbers tend to be reported as hard facts, and now we’ll move onto its exception: when statistical reporting is linked to foul play. Perhaps a country is accused of skewing incomes to receive aid, or another country is seen to downplay the social impacts of a political crisis; on the other end of the spectrum, overly critical reporting of statistic can also be problematic. The fact that the social world is complex and difficult to understand means that even quantitative phenomena don’t lend themselves to cheap and fast summaries that are easily reportable. To illustrate this point, let’s look at something that should be relatively easy to count – namely money. But turns out that numbers are soft here too.\nMeasuring economic activity is hard at the best of times, let alone when faced with the challenges presented in low income countries. Credit: Wikimedia.\nOn 5 November 2010, Ghana’s Statistical Services announced new and revised GDP estimates. Overnight, the size of the economy was adjusted upward by over 60%, which meant that previous GDP estimates had missed about US$13 billion worth of economic activity. While this change in GDP was exceptionally large, it turned out to be far from an isolated case.\nIn 2012, I wrote a summary of this situation, explaining in layman terms how one country, like Ghana, could go from being so poor one day to an aspiring middle-income one the next. My intent with the piece was to demystify the process and to lay bare the basic discrepancies between global standards of measurement and local challenges of data availability and resources. The simple fact is: It is very demanding and costly to measure a country’s whole economy, particularly in low income countries where very few businesses and individuals are reporting taxes, and only a minority of economic transactions are recorded, with most taking place in the informal, unrecorded economy. Yet, despite these nuances, when the Guardian reprinted the story, they smacked the headline: Lies, damn lies and GDP on it. As anyone who has read that piece, or my book will know, I go to some lengths to dispel the beliefs that there was a hidden political agenda behind this revision.\nWhile critical skills often fail when it comes to thinking about converting complex realities into simple numbers, data users also manage to maintain an inherent criticism of any ‘official number’, based on simply distrusting some states and trusting others. A similarly misguided gut reaction exists among scholars, who may never trust a number from Sudan, Ghana or South Africa, but would not hesitate to use the same number if the World Bank recycled it.\nCase study: Promoting trust through fact-checking national statistics   \n  By Kate Wilkinson, Africa Check  \n  \n  In 2018, Africa Check investigated claims by US-based news website Quartz that much of Kenya’s borrowing in recent years has originated from China. Further, they reported that the country’s obligations to Beijing run ‘much deeper than many ordinary Kenyans realise’, under the headline: ‘China now owns more than 70% of Kenya’s external debt’.  \n  \n  The size of Kenya’s public debt was a campaign issue in the country’s 2017 elections and this debt is sometimes conflated with China’s role in Kenya. The Asian country financed the standard gauge railway project, the most visible of the government’s economic growth projects. Yet, its full cost remains unclear because the agreement is closed.   \n  \n  Inaccurate reporting on the issue breeds mistrust in the Kenyan government and its official treasury data.  \n  \n  The first step in fact-checking this claim was to find out what information it was based on. Quartz said that the source of the information was an article in the Nairobi-based Business Daily newspaper. That article had relied on information from the 2018/19 Kenyan budget statement which showed that Kenya owed China KSh534.1 billion. This, they said, was 72% of the country’s total bilateral debt of KSh741 billion.  \n  \n  But while it may be true for bilateral debt, it’s not true for all of Kenya’s foreign debt.   \n  \n  Next, Africa Check sought out experts to explain and unpack the numbers.  \n  \n   “All bilateral debt [is] external debt but not all external debt [is] bilateral debt,” said Odongo Kodongo, a financial economist and associate professor at Wits University’s business school.  \n  \n  External debt is the total public and private debt that a country owes foreign creditors. It includes multilateral debt, commercial debt, bilateral debt, and guaranteed debt.  \n  \n  Experts used Kenyan treasury documents to estimate that China’s share of Kenya’s external debt was KSh534.07 billion of KSh2.51 trillion as of 31 March 2018. This was equal to 21.3% -- not 70%.   \n  \n  China’s role in Kenya is a controversial issue. This fact-check showed the public that official government data could be relied upon to determine if claims about the countries’ relations were true.  \n\n\n                                                                     \n                            \nThe failure to reflect on the softness of statistics in general, and rather look to a higher authority, was also reflected in an episode a few years later. On 7 April 2014, the Nigerian Bureau of Statistics (NBS) declared that their GDP estimates were also being revised upward to $510 billion, an 89% increase from the old estimate. It was controversial: The GDP revision was bigger than expected, and, the IMF’s Statistics Department had provided technical assistance for the revision, which they viewed as incomplete. But the IMF has no authority to endorse or not endorse statistics, so they couldn’t stand in the way when Nigeria wanted to release the new numbers.\nGDP information is released by the NBS in quarterly reports. This infographic from the Q4 and 2018 full year report shows a substantial increase in the 2014 growth rate.\nWhen Yemi Kale, the director of NBS, announced the new numbers, they were keen to make sure that the credibility of the statistics was not undermined, well aware that the international media’s trust was low. This meant that the IMF Mission Chief to Nigeria was well prepared for the inevitable question as to whether they ‘endorsed’ the new numbers. When asked, he made a long statement that ended by summarising how the IMF supported “the efforts being made to improve the statistics of the Federation, as a basis of sound decision making. Let me state that we endorse this wholeheartedly and will support Nigeria in this regard”. In the end, this statement became reported as ‘the IMF endorsed the numbers’, despite the broader messages of statistical capacity building that it contained.\nSo, what would be a more conscientious way to report on this number? Rather than focusing on narratives of trust or distrust around the revision itself, possible stories could’ve looked at:\nThe challenges of measuring economic activity in low income countries\nResourcing limitations at the NBS, which meant benchmark data for the GDP hadn’t been updated properly for more than a quarter of a century\nThe role of the IMF, and its inability to endorse a country’s data.\nConclusion\nBy digging deeper, and looking beyond a simplistic numbers, journalists are able to provide more accurate and comprehensive reports on social issues. When confronted with a number on corruption, unemployment, or the size of the illegal economy, journalists should always think critically about where that number came from. Numbers are much softer than we would like to think, and we trust and mistrust them far more than justified by their accuracy.","guid":"https://datajournalism.com/read/longreads/hard-data-and-soft-statistics-a-guide-to-critical-reporting","isoDate":"2019-08-31T04:56:00.000Z","blogTitle":"DataJournalism.com"}},"__N_SSG":true},"page":"/[...slug]","query":{"slug":["hard-data-and-soft-statistics-a-guide-to-critical-reporting"]},"buildId":"sFP8FhENNxqFJqEgxMEWp","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>