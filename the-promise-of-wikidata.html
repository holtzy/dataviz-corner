<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/dd8d3b5be9662933.css" as="style"/><link rel="stylesheet" href="/_next/static/css/dd8d3b5be9662933.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-59c5c889f52620d6.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-f11614d8aa7ee555.js" defer=""></script><script src="/_next/static/chunks/pages/_app-694ee0f2821639fc.js" defer=""></script><script src="/_next/static/chunks/996-eeb5175dbd5dba8f.js" defer=""></script><script src="/_next/static/chunks/36-94b5e24e03efc6db.js" defer=""></script><script src="/_next/static/chunks/pages/%5B...slug%5D-af748dcc13a25fcb.js" defer=""></script><script src="/_next/static/0XYe7aucOTe3b0iK50JEi/_buildManifest.js" defer=""></script><script src="/_next/static/0XYe7aucOTe3b0iK50JEi/_ssgManifest.js" defer=""></script></head><body><div id="__next"></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"creator":"Monika Sengul-Jones","title":"The promise of Wikidata","link":"https://datajournalism.com/read/longreads/the-promise-of-wikidata","pubDate":"Wed, 10 Feb 2021 09:05:00 +0100","author":"Monika Sengul-Jones","content":"\n                                                                        \u003cp\u003eA decade ago, let’s say you wanted to know the population of the metropolitan area of Accra, Ghana using the open web. For a quick answer, you might look at a Wikipedia article’s infobox on Accra. \u003c/p\u003e\n\u003cp\u003eThere would be a number. Let’s say you used another language version, French Wikipedia. You might get a second number. With a search engine, a third number. \u003c/p\u003e\n\u003cp\u003eEach might be correct, contextually. Of course, people are born, die, move, and boundaries are being negotiated. Any population statistic is bound to be out-of-date from the moment it’s collected. \u003c/p\u003e\n\u003cp\u003eBut the variation—and lag—in updates on data points like population across Wikipedias, not to mention elsewhere on the web, has frustrated open access semantic web advocates, working for more machine-readable linked data online. Because the inconsistency isn't an issue of controversial records or out-of-date datasets. Rather, it's a problem of unlinked data. \u003c/p\u003e\n\u003cp\u003eEnter \u003ca href=\"https://www.wikidata.org/wiki/Wikidata:Main_Page\"\u003eWikidata\u003c/a\u003e, in 2012. A machine and human-readable linked knowledge base that straddles the best of both. Humans can edit. Machines can read. \u003c/p\u003e\n\u003cp\u003eUpdate the population of a city in Wikidata; insert the linked identifier into article pages—and bada-bing, bada-boom–when the linked database is updated, all the identifiers running the information also update. The population of Accra, Ghana can be consistent no matter where you look. \u003c/p\u003e\n\u003cp\u003eWikidata is a sister project to the better-known crowdsourced encyclopedia, \u003ca href=\"www.wikipedia.org\"\u003eWikipedia\u003c/a\u003e—which has \u003ca href=\"https://datajournalism.com/read/longreads/harnessing-wikipedias-superpowers-for-journalism\"\u003ebenefits for data journalists\u003c/a\u003e. \u003c/p\u003e\n\u003cp\u003eAnd both are part of the \u003ca href=\"https://www.wikimedia.org/\"\u003eWikimedia\u003c/a\u003e movement, whose mission is to bring \"free educational content to the world.\" \u003c/p\u003e\n\u003cp\u003eBut unlike Wikipedia, which at 20 years old is recognised for being surprisingly reliable despite \u003ca href=\"https://en.wikipedia.org/wiki/Predictions_of_the_end_of_Wikipedia\"\u003epredictions that the end of Wikipedia is near\u003c/a\u003e, Wikidata is best known for having a promising future—that hasn't quite arrived. Illustrated, for example, by the fact that Accra's population is still unlinked on Wikipedia \u003ca href=\"https://en.wikipedia.org/wiki/Accra#cite_note-2010_Census-2\"\u003e(the English Wikipedia article's infobox references the census, not Wikidata)\u003c/a\u003e.\u003c/p\u003e\n\n                                                                                                \u003cblockquote\u003e\u003cp\u003eComputers are often very fast, but generally very dumb, so in a system of explicit representation of information, you have to tell them everything.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n                                                                     \n                            \u003ch3\u003eHow can data journalists use the Wikimedia movement's linked knowledge base data?\u003c/h3\u003e\n\u003cp\u003eThe first step is discerning the difference between the promise of the project and how it works today. \u003ca href=\"https://twitter.com/el_giesemann?lang=en\"\u003eElisabeth Giesemann\u003c/a\u003e, from \u003ca href=\"https://www.wikimedia.de/\"\u003eWikimedia Deutschland\u003c/a\u003e, recently gave \u003ca href=\"https://www.youtube.com/watch?v=oiVOG3FuUFQ\"\u003ea talk for journalists\u003c/a\u003e and explained that Wikidata is actualising the vision of a semantic web touted by Sir Tim Berners-Lee, creator of the world wide web. \u003c/p\u003e\n\u003cp\u003eBerners-Lee similarly champions Wikidata. He co-founded the \u003ca href=\"https://theodi.org/\"\u003eOpen Data Institute\u003c/a\u003e, which recently honoured \u003ca href=\"https://www.dataversity.net/wikidata-winner-first-odi-open-data-awards/\"\u003eWikiData with a special award\u003c/a\u003e. \u003c/p\u003e\n\u003cp\u003eThough there’s evidence that Wikidata is already ushering in a new era of linked data—with the dataset being incorporated into commercial technologies such as \u003ca href=\"https://www.wired.com/story/inside-the-alexa-friendly-world-of-wikidata/\"\u003eAmazon's Alexa\u003c/a\u003e and \u003ca href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44818.pdf\"\u003eGoogle's knowledge graph\u003c/a\u003e—there are limitations, including whether or not web pages link to Wikidata. \u003c/p\u003e\n\u003cp\u003eThe project suffers from the biases and vandalism that plague other Wikimedia projects. Including gender gaps in the contributor base—the majority of the volunteer editors are male. And the majority of the data is from—and about—the Northern hemisphere. The project is young, Giesemann emphasises. \u003c/p\u003e\n\u003cp\u003eAs a concept, one might compare Wikidata to a busy train station. There are millions of links between data points and interlinks to other open datasets. \u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.linkedin.com/in/vrandecic/\"\u003eDenny Vrandečić\u003c/a\u003e, who designed Wikidata and worked for Google for six years as an ontologist before joining the Wikimedia Foundation last summer, said \u003ca href=\"https://www.youtube.com/watch?v=Oips1aW738Q\"\u003eWikidata connects to 40,000 other databases\u003c/a\u003e, including Wikipedia, \u003ca href=\"https://wiki.dbpedia.org/\"\u003eDBpedia\u003c/a\u003e, Library of Congress, German Bibliotech, and VIAF.\u003c/p\u003e\n\u003cp\u003e“[It’s] a backbone of the web of data,” said Kat Thornton, a researcher at Yale University Library with expertise in linked data. “If you are interested in data, it is better than search. It would be oversimplifying Wikidata to call it search. [There] you are matching the string, Wikidata’s web of knowledge is far more powerful than string matching.”\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eA cropped visualisation of Wikidata's position in the linked open data, which according to Kat Thornton, a researcher at Yale University Library, is the backbone of the web.\u003c/p\u003e\n\n                                                                      \n                            \u003ch3\u003eHow Wikidata is designed to work\u003c/h3\u003e\n\u003cp\u003eLike other linked data projects, Wikidata models information using the Resource Description Framework (RDF). This model expresses data in \u003ca href=\"https://en.wikipedia.org/wiki/Semantic_triple\"\u003esemantic triples\u003c/a\u003e. Subject --\u0026gt; predicate --\u0026gt; object. For example, Accra is an instance of a city. \u003c/p\u003e\n\u003cp\u003eAn item can be a definite object—a specific book, person, event, or place. Or a concept—\u003ca href=\"https://www.wikidata.org/wiki/Q189125\"\u003etransgender\u003c/a\u003e, \u003ca href=\"https://www.wikidata.org/wiki/Q485446\"\u003eoptimism\u003c/a\u003e, or \u003ca href=\"https://www.wikidata.org/wiki/Q1425577\"\u003epromise\u003c/a\u003e. Items are identified with a “Q” and a unique number. \u003c/p\u003e\n\u003cp\u003eAccra is Q3761. City is Q515. Predicates are labelled with a “P” and a number. Instance of is P31. Relationships between items are stored as statements. Accra is an instance of a city. Q3761--\u0026gt;P31--\u0026gt;Q515. \u003c/p\u003e\n\u003cp\u003e“Computers are often very fast, but generally very dumb, so in a system of explicit representation of information, you have to tell them everything. Tokyo is a city. The sky is up. Water is wet,” \u003ca href=\"https://techblog.wikimedia.org/2020/03/24/computational-knowledge-wikidata-wikidata-query-service-and-women-who-are-mayors/\"\u003ewrote Trey Jones\u003c/a\u003e, a software engineer for the Wikimedia Foundation, in \u003ca href=\"https://techblog.wikimedia.org/2020/03/24/computational-knowledge-wikidata-wikidata-query-service-and-women-who-are-mayors/\"\u003ea recent article on linked data\u003c/a\u003e.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eDiagram visualising linked data in triples, using the item for Accra, Ghana as an example.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eHowever, many data are factually contentious. When is someone dead? Most births and deaths are cut and dry—unless you are \u003ca href=\"https://en.wikipedia.org/wiki/Terri_Schiavo_case\"\u003eTerri Schiavo\u003c/a\u003e. \u003c/p\u003e\n\u003cp\u003eHow about Taiwan? The instance of a sovereign country or the territory of another country like Sudan, Palestine, Crimea, Northern Ireland. The list goes on (\u003ca href=\"https://en.wikipedia.org/wiki/List_of_territorial_disputes\"\u003eand it does\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eHelpfully, Wikidata allows for ambiguity. Taiwan is an instance of a country. Taiwan is an instance of a territory. Both statements can exist in \u003ca href=\"https://www.wikidata.org/wiki/Q865\"\u003ethe item\u003c/a\u003e. \u003c/p\u003e\n\u003cp\u003eThere’s also room for references, which enable the reuse of linked data by improving error detection. But contributors can make statements without them. \u003c/p\u003e\n\u003cp\u003eThis lowers barriers to entry for data donations and new contributions, but can mean that \"inaccurate data, or messy database imports, such as \u003ca href=\"https://www.wikidata.org/wiki/Wikidata:WikiProject_Authority_control/The_Peerage_errors\"\u003epeerage\u003c/a\u003e or vandalism, are a challenge,\" said Jim Hayes, a volunteer Wikidata contributor and \u003ca href=\"https://wikimediadc.org/wiki/Home\"\u003eWikimedia D.C.\u003c/a\u003e member in an email interview.\u003c/p\u003e\n\n                                                                                                 \u003cblockquote\u003e\u003cp\u003eAs a concept, one might compare Wikidata to a busy train station. There are millions of links between data points and interlinks to other open datasets.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n                                                                     \n                            \u003cp\u003eAs a result, there are partialities in the available linked data. Some organisations have already \u003ca href=\"https://www.wikidata.org/wiki/Wikidata:Data_donation#Organisations_who_have_worked_with_Wikidata\"\u003edonated data\u003c/a\u003e. \u003c/p\u003e\n\u003cp\u003eThere are \u003ca href=\"https://www.wikidata.org/wiki/Wikidata:Flemish_art_collections,_Wikidata_and_Linked_Open_Data\"\u003e25,000 items for Flemish paintings in Wikidata\u003c/a\u003e, thanks to 2015 data donation from a collective of Flemish museums and galleries. But other topics—such as the cultural heritage of nations in the Global South—are left wanting. \u003c/p\u003e\n\u003cp\u003eThis is a topic \u003ca href=\"https://www.linkedin.com/in/masssly/?originalSubdomain=gh\"\u003eMohammed Sadat Abdulai\u003c/a\u003e, a co-lead of the non-profit organisation \u003ca href=\"https://artandfeminism.org/\"\u003eArt+Feminism\u003c/a\u003e and community communication manager with \u003ca href=\"https://www.wikimedia.de/\"\u003eWikimedia Deutschland\u003c/a\u003e, deals with daily. \u003c/p\u003e\n\u003cp\u003eHe said in a recent phone call that Wikidata’s eurocentrism can materialise not only through the presence or absence of data, but also in the subtle way that data is modelled.\u003c/p\u003e\n\u003cp\u003e“If you come from a different way of thinking, you will find it is difficult to model your way of thinking with Wikidata,” he said. He gave the example of name etymologies. “There are Ghanian names in \u003ca href=\"https://www.wikidata.org/wiki/Q32238\"\u003eDagbani\u003c/a\u003e that are meaningful through their connection to days of the week,” said Abdulai. “Atani means a female born on a Monday. But this meaning is not easy to model using subclasses in Wikidata.”\u003c/p\u003e\n\u003cp\u003eAbdulai strives to expand representation of Ghana with Wikidata, but his experience suggests there can be linguistic ghettos. “It is a good thing Wikidata is flexible,” he said. “You can find your own way of modelling. But since it is not conventional, you end up working in your own little space.”\u003c/p\u003e\n\n                                                                                                 \u003ch3\u003eQuick links\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://www.wikidata.org/wiki/Wikidata:Introduction\"\u003eAn introduction to Wikidata\u003c/a\u003e\u003c/strong\u003e \u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://www.wikidata.org/wiki/Wikidata:Linked_open_data_workflow\"\u003eDonate data to Wikidata\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\n                                                                                                                 \n                            \u003cp\u003eProbe differences in titles and coverage to understand possible regional differences and sources on the topic. This screenshare video uses the article on the 2020 women’s strike against Polish abortion law as an example. It demonstrates how to find the Wikidata link and access multiple language versions of a Wikipedia article.\u003c/p\u003e\n\n                                                                      \n                            \u003ch3\u003eThree ways data journalists can bring Wikidata into their data storytelling\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e1. As a shortcut between Wikipedia versions\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOne easy way to use Wikidata is as a node of connection between Wikipedias, which \u003ca href=\"https://datajournalism.com/read/longreads/harnessing-wikipedias-superpowers-for-journalism\"\u003eyou can also use for data journalism\u003c/a\u003e. The Wikidata item is the node of connection between articles in different language Wikipedias. This shortcut can help you check for variation in existing coverage, and pry for new angles. \u003c/p\u003e\n\u003cp\u003eFor instance, the 2020 women's strike in Poland against the abortion law has articles in 17 Wikipedias in different languages, each a slightly different version providing coverage of the strike. To quickly dig into details of variation on Wikipedias, and the narratives they index, use Wikidata to toggle between article versions.\u003c/p\u003e\n\n                                                                                                 \u003cblockquote\u003e\u003cp\u003eIf we’re going to call other sites untrustworthy, we can’t just say “trust us” as the reason why.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n                                                                     \n                            \u003cp\u003e\u003cstrong\u003e2. Use Wikidata at scale. The API is available and the interface is multilingual.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis requires caution, however. \u003ca href=\"https://www.linkedin.com/in/barrett-golding/\"\u003eBarrett Golding\u003c/a\u003e can attest. When the pandemic hit last year, Golding—a former NPR producer and freelance data journalist—launched \u003ca href=\"https://iffy.news/\"\u003eIffy.news\u003c/a\u003e. Designed for researchers and journalists, the site contains indexes and lists on sources of mis/disinformation. \u003c/p\u003e\n\u003cp\u003eGolding uses large-scale data harvesting to showcase whether or not a website has a reputation for fact-checking, based on credibility rankings from databases such as Media Bias/Fact Check. “If we’re going to call other sites untrustworthy, we can’t just say “trust us” as the reason why. So each Iffy site links to the failed fact-checks that make that site unreliable,” Golding explained. \u003c/p\u003e\n\u003cp\u003eMore recently, he began accessing information from Wikidata and Wikipedia to cross-check the reliability of websites and online sources, thanks to a grant from \u003ca href=\"https://www.wikicred.org/\"\u003eWikiCred\u003c/a\u003e. (For disclosure, I am also working \u003ca href=\"https://misinfocon.com/reading-together-reliability-and-multilingual-global-communities-3c7e9bc4af03\u0026amp;sa=D\u0026amp;source=editors\u0026amp;ust=1612904501353000\u0026amp;usg=AOvVaw0z0443lJWhBdH49s3jr0-1\"\u003eon a project on reliable sources and Wikipedia funded by Wikicred)\u003c/a\u003e. \u003c/p\u003e\n\u003cp\u003eThat’s where things fell apart. The data were too piecemeal. \u003c/p\u003e\n\u003cp\u003eInfowars, a well-known instance of “fake news,” is described as such in its English Wikipedia article. Wikipedia editors have also blacklisted the website from being used as a reliable source in citations, according to the hand-updated list of \u003ca href=\"https://en.wikipedia.org/wiki/Wikipedia:Reliable_sources/Perennial_sources\"\u003ePerennial sources\u003c/a\u003e. \u003c/p\u003e\n\u003cp\u003eBut these classifications didn’t make it to Wikidata. Infowars was just an instance of news satire and a website. (That is, until two weeks ago, when the \u003ca href=\"https://www.wikidata.org/w/index.php?title=Q46997916\u0026amp;type=revision\u0026amp;diff=1339704897\u0026amp;oldid=1339685446\"\u003eitem was edited\u003c/a\u003e to include as an instance of “fake news”). \u003c/p\u003e\n\u003cp\u003eThe takeaway for data journalists? Be aware that large-scale data harvesting from Wikidata’s API can scrape out nuance at scale, rather the other way around.\u003c/p\u003e\n\n                                                                                                 \u003ch3\u003eQuick links\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://www.wikidata.org/wiki/Wikidata:Contribute\"\u003e\u003cstrong\u003eHow to contribute to Wikidata\u003c/strong\u003e\u003c/a\u003e \u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://pypi.org/project/Wikidata/\"\u003e\u003cstrong\u003eWikidata API for Python\u003c/strong\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://iffy.news/\"\u003e\u003cstrong\u003eIffy.news\u003c/strong\u003e\u003c/a\u003e \u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Wikipedia:Reliable_sources/Perennial_sources\"\u003e\u003cstrong\u003eWikipedia Perennial Sources\u003c/strong\u003e\u003c/a\u003e\u003c/p\u003e\n\n                                                                     \n                            \u003cp\u003e\u003cstrong\u003eLeverage Wikidata's strengths\u003c/strong\u003e \u003c/p\u003e\n\u003cp\u003eThere are Wikidata storytelling success stories, which often include using the dataset in conjunction with other data. \u003ca href=\"https://www.kcl.ac.uk/people/laura-jones\"\u003eLaura Jones\u003c/a\u003e (no relation to Trey or the author), a researcher with the \u003ca href=\"https://www.kcl.ac.uk/giwl\"\u003eGlobal Institute for Women’s Leadership at King's College London\u003c/a\u003e authored \u003ca href=\"https://www.kcl.ac.uk/news/women-have-been-marginalised-in-covid-19-media-coverage\"\u003ea report\u003c/a\u003e that shows how women—journalists and experts—have been involved in coronavirus media coverage. \u003c/p\u003e\n\u003cp\u003eTo find out, \u003ca href=\"https://www.kcl.ac.uk/giwl/research/covid-media-analysis\"\u003ethe study\u003c/a\u003e used Wikidata and Wikipedia’s API to identify the gender and occupation of 54,636 unique people who had been mentioned in a vat of news content sourced from \u003ca href=\"https://www.eventregistry.org/\"\u003eEvent Registry's\u003c/a\u003e \u003ca href=\"https://newsapi.ai/\"\u003eAPI\u003c/a\u003e, an AI-driven media intelligence platform, during the 2020 pandemic. \u003c/p\u003e\n\u003cp\u003eThanks to the information stored in Wikidata, Jones was able to identify most of the unique people mentioned in the news coverage – experts and journalists. The majority of media coverage about the pandemic was written by male journalists, while one out of five expert voices who were interviewed about the pandemic were female, Jones concluded.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.sciencestories.io/welcome\"\u003eScience Stories.io\u003c/a\u003e uses Wikidata and other linked data projects to visualise stories about women in science and academia. By aggregating images, structured data, and prose at scale, Science Stories.io generates hundreds of multimedia biographical portraits of historical and contemporary notable women. \u003c/p\u003e\n\u003cp\u003eWhile \u003ca href=\"https://scholia.toolforge.org/\"\u003eScholia\u003c/a\u003e pulls data from Wikidata to create visual profiles on items including chemicals, species, and people.\u003c/p\u003e\n\n                                                                                                 \u003ch3\u003eQuick links\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://www.kcl.ac.uk/giwl/research/covid-media-analysis\"\u003e\u003cstrong\u003eCovid Media Analysis Report\u003c/strong\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.sciencestories.io/\"\u003e\u003cstrong\u003eScience Stories.io\u003c/strong\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://scholia.toolforge.org/\"\u003e\u003cstrong\u003eScholia\u003c/strong\u003e\u003c/a\u003e\u003c/p\u003e\n\n                                                                     \n                            \u003cp\u003e\u003cstrong\u003e3. Discover relationships through Wikidata's query service\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eGet a sense of what’s in Wikidata, and how this may aid your data storytelling, through querying. The Wikidata \u003ca href=\"https://query.wikidata.org/\"\u003equery service\u003c/a\u003e is free and available online. You’ll need to use \u003ca href=\"https://www.w3.org/TR/sparql11-query/\"\u003eSPARQL\u003c/a\u003e, a variation of \u003ca href=\"https://www.w3schools.in/sql/\"\u003eSQL\u003c/a\u003e, the relational database management system. \u003c/p\u003e\n\u003cp\u003eWhether you are already familiar with SPARQL or just getting started, there are an abundance of tutorials and training videos to learn. The query service also has examples. Run an example yourself for fun by pressing the blue “play” button. There are also volunteer users \u003ca href=\"https://www.wikidata.org/wiki/Wikidata:Request_a_query\"\u003ewho are willing to run queries for you\u003c/a\u003e.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eWikidata query service includes example queries.\u003c/p\u003e\n\n                                                                      \n                            \u003cp\u003eYou can modify examples or write a query from scratch. Query results can be visualised, shared, downloaded, or embedded. It's worth running a query before you use the API or download a data dump. \u003c/p\u003e\n\u003cp\u003eWhen it comes to an effort like Golding’s project on “fake news” websites, the query could be the first red flag that the data just isn’t there. For instance, a query for instances of “fake news” websites in Wikidata reveals less than a dozen. \u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://query.wikidata.org/embed.html#%23fake%20news%20websites%0ASELECT%20%3Fitem%20%3FitemLabel%20%0AWHERE%20%0A%7B%0A%20%20%3Fitem%20wdt%3AP31%20wd%3AQ27881073.%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22%5BAUTO_LANGUAGE%5D%2Cen%22.%20%7D%0A%7D\"\u003eTry it\u003c/a\u003e—and keep in mind the results from your query will be as of the date you run the query, not as of mine, nor as of Goldings. Right now, there’s no way to share a hyperlink to a historical version of a query). Part of the problem, as Golding found, are idiosyncratic classifications. Some items are instances of websites, others are online newspapers. \u003c/p\u003e\n\u003cp\u003eAnother query example is a “Timeline of Death by Burning.” \u003ca href=\"https://w.wiki/uoM\"\u003e(Try it)\u003c/a\u003e. I modified the query by substituting the cause of death (P509) from burning (Q468455) to decapitation (Q204933). \u003ca href=\"https://w.wiki/uoK\"\u003e(Try it)\u003c/a\u003e. Both rendered grisly timelines showcasing a long history of these particular forms of death. \u003c/p\u003e\n\u003cp\u003eMy next modification reveals a limitation of the dataset. I wanted to create a timeline of women who are murdered in gender-based violence. This has a name, femicide (Q1342425). Again, grisly, I know—but also important. I expected the number might be lower than reality. But I was not prepared for a timeline of one. One femicide. \u003ca href=\"https://w.wiki/uoK\"\u003e(Try it)\u003c/a\u003e. \u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.linkedin.com/in/liamwyatt/?originalSubdomain=it\"\u003eLiam Wyatt\u003c/a\u003e, who manages the Wikicite programme for the Wikimedia Foundation, said this is a typical pitfall. “You have to caveat any query result with 'as far as Wikidata knows,'” he explained in a phone interview. \u003c/p\u003e\n\u003cp\u003eFor my query, it’s possible there are other femicides documented in Wikidata. Categorised as instances of homicide, or domestic violence. But there is invisibility yet. For instance, the \u003ca href=\"https://www.theguardian.com/world/2020/jul/23/turkey-outrage-rising-violence-against-women\"\u003emurder of Pınar Gültekin by her ex-boyfriend\u003c/a\u003e last year made headlines around the world. \u003ca href=\"https://www.aljazeera.com/news/2020/7/22/protests-in-turkey-over-brutal-murder-of-young-woman\"\u003eWomen took to the streets in Turkey\u003c/a\u003e to protest. \u003c/p\u003e\n\u003cp\u003eAnd there was a much-debated social media hashtag campaign, \u003ca href=\"https://www.instagram.com/explore/tags/challengeaccepted/\"\u003e#challengeaccepted\u003c/a\u003e, to raise awareness about femicide. \u003c/p\u003e\n\u003cp\u003eWhile there is a \u003ca href=\"https://en.wikipedia.org/wiki/Murder_of_P%C4%B1nar_G%C3%BCltekin\"\u003eWikipedia article in English about the murder\u003c/a\u003e, and \u003ca href=\"https://www.wikidata.org/wiki/Q97584637\"\u003ea Wikidata item for the event\u003c/a\u003e, Gültekin—not to mention the manner of her death—is not included as a human in Wikidata.\u003c/p\u003e\n\n                                                                                                 \u003ch3\u003eQuick links\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://wdqs-tutorial.toolforge.org/\"\u003e\u003cstrong\u003eWikidata Query Service Tutorial\u003c/strong\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/A_gentle_introduction_to_the_Wikidata_Query_Service\"\u003e\u003cstrong\u003eGentle Introduction to the Wikidata Query Service\u003c/strong\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.wikidata.org/wiki/Wikidata:Request_a_query\"\u003e\u003cstrong\u003eRequest a Query\u003c/strong\u003e\u003c/a\u003e\u003c/p\u003e\n\n                                                                                                \u003cp\u003e\"Wikidata is now powerful and important, but still esoteric and incomplete,” said Wyatt, on the ambiguities of the current state of Wikidata. “It’s a bit of a wild west. Journalists who can get in on the ground floor, on this wave while it’s still picking up speed, they will really be in a position to ride the momentum.” \u003c/p\u003e\n\u003cp\u003eA promising project, when we remember \u003ca href=\"https://www.wikidata.org/wiki/Q1425577\"\u003ethat promise\u003c/a\u003e, according to Wikidata, is also known as liability. That is, to be “held morally or legally responsible for action or inaction.” Use it freely and be mindful to not substitute this dataset for news judgement. \u003c/p\u003e\n\u003cp\u003eAs Last Moya writes in \u003ca href=\"https://www.researchgate.net/publication/338962408_Data_Journalism_and_the_Panama_Papers_New_Horizons_for_Investigative_Journalism_in_Africa\"\u003e“Data Journalism in the Global South”\u003c/a\u003e, data can aid journalism in speaking truth to power provided “journalistic agency and not data is King.”\u003c/p\u003e\n\n                                                                                                                                \u003cp\u003eMonika Sengul-Jones, PhD, is a freelance researcher, writer and expert on digital cultures and media industries. She was the OCLC Wikipedian-in-Residence in 2018-19. In 2020, she is co-leading \u003ca href=\"https://artandfeminism.org/initiatives/current-initiatives/reading-together/\"\u003eReading Together: Reliable Sources and Multilingual Communities\u003c/a\u003e, an Art+Feminism project on reliable sources and marginalised communities funded by WikiCred. \u003ca href=\"https://twitter.com/monikajones\"\u003e@monikajones\u003c/a\u003e, \u003ca href=\"https://monikasjones.com\"\u003ewww.monikasjones.com\u003c/a\u003e\u003c/p\u003e\n\n                                                                                                                              \n                            \u003cp\u003eThanks to Molly Brind'Amour (University of Virginia), Will Kent (Wiki Education Foundation), Lane Rasberry (University of Virginia), and Houcemeddine Turki (Wikimedia Tunisia) for speaking with me for background research for this story.\u003c/p\u003e\n\n                                                                      \n                            \u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://datajournalism.com/read/longreads/data-visualisation-by-hand\"\u003eData visualisation by hand: drawing data for your next story\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://datajournalism.com/read/longreads/hypothesis-data-journalism\"\u003eA data journalists guide to building a hypothesis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://datajournalism.com/read/longreads/data-sonification\"\u003eMaking numbers louder: telling stories with sound\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://datajournalism.com/read/longreads/conflict-reporting-with-data\"\u003eConflict reporting with data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://datajournalism.com/read/longreads/harnessing-wikipedias-superpowers-for-journalism\"\u003eHarnessing Wikipedia's superpowers for journalists\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n                                              \n                ","contentSnippet":"A decade ago, let’s say you wanted to know the population of the metropolitan area of Accra, Ghana using the open web. For a quick answer, you might look at a Wikipedia article’s infobox on Accra. \nThere would be a number. Let’s say you used another language version, French Wikipedia. You might get a second number. With a search engine, a third number. \nEach might be correct, contextually. Of course, people are born, die, move, and boundaries are being negotiated. Any population statistic is bound to be out-of-date from the moment it’s collected. \nBut the variation—and lag—in updates on data points like population across Wikipedias, not to mention elsewhere on the web, has frustrated open access semantic web advocates, working for more machine-readable linked data online. Because the inconsistency isn't an issue of controversial records or out-of-date datasets. Rather, it's a problem of unlinked data. \nEnter Wikidata, in 2012. A machine and human-readable linked knowledge base that straddles the best of both. Humans can edit. Machines can read. \nUpdate the population of a city in Wikidata; insert the linked identifier into article pages—and bada-bing, bada-boom–when the linked database is updated, all the identifiers running the information also update. The population of Accra, Ghana can be consistent no matter where you look. \nWikidata is a sister project to the better-known crowdsourced encyclopedia, Wikipedia—which has benefits for data journalists. \nAnd both are part of the Wikimedia movement, whose mission is to bring \"free educational content to the world.\" \nBut unlike Wikipedia, which at 20 years old is recognised for being surprisingly reliable despite predictions that the end of Wikipedia is near, Wikidata is best known for having a promising future—that hasn't quite arrived. Illustrated, for example, by the fact that Accra's population is still unlinked on Wikipedia (the English Wikipedia article's infobox references the census, not Wikidata).\nComputers are often very fast, but generally very dumb, so in a system of explicit representation of information, you have to tell them everything.\nHow can data journalists use the Wikimedia movement's linked knowledge base data?\nThe first step is discerning the difference between the promise of the project and how it works today. Elisabeth Giesemann, from Wikimedia Deutschland, recently gave a talk for journalists and explained that Wikidata is actualising the vision of a semantic web touted by Sir Tim Berners-Lee, creator of the world wide web. \nBerners-Lee similarly champions Wikidata. He co-founded the Open Data Institute, which recently honoured WikiData with a special award. \nThough there’s evidence that Wikidata is already ushering in a new era of linked data—with the dataset being incorporated into commercial technologies such as Amazon's Alexa and Google's knowledge graph—there are limitations, including whether or not web pages link to Wikidata. \nThe project suffers from the biases and vandalism that plague other Wikimedia projects. Including gender gaps in the contributor base—the majority of the volunteer editors are male. And the majority of the data is from—and about—the Northern hemisphere. The project is young, Giesemann emphasises. \nAs a concept, one might compare Wikidata to a busy train station. There are millions of links between data points and interlinks to other open datasets. \nDenny Vrandečić, who designed Wikidata and worked for Google for six years as an ontologist before joining the Wikimedia Foundation last summer, said Wikidata connects to 40,000 other databases, including Wikipedia, DBpedia, Library of Congress, German Bibliotech, and VIAF.\n“[It’s] a backbone of the web of data,” said Kat Thornton, a researcher at Yale University Library with expertise in linked data. “If you are interested in data, it is better than search. It would be oversimplifying Wikidata to call it search. [There] you are matching the string, Wikidata’s web of knowledge is far more powerful than string matching.”\nA cropped visualisation of Wikidata's position in the linked open data, which according to Kat Thornton, a researcher at Yale University Library, is the backbone of the web.\nHow Wikidata is designed to work\nLike other linked data projects, Wikidata models information using the Resource Description Framework (RDF). This model expresses data in semantic triples. Subject --\u003e predicate --\u003e object. For example, Accra is an instance of a city. \nAn item can be a definite object—a specific book, person, event, or place. Or a concept—transgender, optimism, or promise. Items are identified with a “Q” and a unique number. \nAccra is Q3761. City is Q515. Predicates are labelled with a “P” and a number. Instance of is P31. Relationships between items are stored as statements. Accra is an instance of a city. Q3761--\u003eP31--\u003eQ515. \n“Computers are often very fast, but generally very dumb, so in a system of explicit representation of information, you have to tell them everything. Tokyo is a city. The sky is up. Water is wet,” wrote Trey Jones, a software engineer for the Wikimedia Foundation, in a recent article on linked data.\nDiagram visualising linked data in triples, using the item for Accra, Ghana as an example.\nHowever, many data are factually contentious. When is someone dead? Most births and deaths are cut and dry—unless you are Terri Schiavo. \nHow about Taiwan? The instance of a sovereign country or the territory of another country like Sudan, Palestine, Crimea, Northern Ireland. The list goes on (and it does).\nHelpfully, Wikidata allows for ambiguity. Taiwan is an instance of a country. Taiwan is an instance of a territory. Both statements can exist in the item. \nThere’s also room for references, which enable the reuse of linked data by improving error detection. But contributors can make statements without them. \nThis lowers barriers to entry for data donations and new contributions, but can mean that \"inaccurate data, or messy database imports, such as peerage or vandalism, are a challenge,\" said Jim Hayes, a volunteer Wikidata contributor and Wikimedia D.C. member in an email interview.\nAs a concept, one might compare Wikidata to a busy train station. There are millions of links between data points and interlinks to other open datasets.\nAs a result, there are partialities in the available linked data. Some organisations have already donated data. \nThere are 25,000 items for Flemish paintings in Wikidata, thanks to 2015 data donation from a collective of Flemish museums and galleries. But other topics—such as the cultural heritage of nations in the Global South—are left wanting. \nThis is a topic Mohammed Sadat Abdulai, a co-lead of the non-profit organisation Art+Feminism and community communication manager with Wikimedia Deutschland, deals with daily. \nHe said in a recent phone call that Wikidata’s eurocentrism can materialise not only through the presence or absence of data, but also in the subtle way that data is modelled.\n“If you come from a different way of thinking, you will find it is difficult to model your way of thinking with Wikidata,” he said. He gave the example of name etymologies. “There are Ghanian names in Dagbani that are meaningful through their connection to days of the week,” said Abdulai. “Atani means a female born on a Monday. But this meaning is not easy to model using subclasses in Wikidata.”\nAbdulai strives to expand representation of Ghana with Wikidata, but his experience suggests there can be linguistic ghettos. “It is a good thing Wikidata is flexible,” he said. “You can find your own way of modelling. But since it is not conventional, you end up working in your own little space.”\nQuick links\nAn introduction to Wikidata \nDonate data to Wikidata\nProbe differences in titles and coverage to understand possible regional differences and sources on the topic. This screenshare video uses the article on the 2020 women’s strike against Polish abortion law as an example. It demonstrates how to find the Wikidata link and access multiple language versions of a Wikipedia article.\nThree ways data journalists can bring Wikidata into their data storytelling\n1. As a shortcut between Wikipedia versions\nOne easy way to use Wikidata is as a node of connection between Wikipedias, which you can also use for data journalism. The Wikidata item is the node of connection between articles in different language Wikipedias. This shortcut can help you check for variation in existing coverage, and pry for new angles. \nFor instance, the 2020 women's strike in Poland against the abortion law has articles in 17 Wikipedias in different languages, each a slightly different version providing coverage of the strike. To quickly dig into details of variation on Wikipedias, and the narratives they index, use Wikidata to toggle between article versions.\nIf we’re going to call other sites untrustworthy, we can’t just say “trust us” as the reason why.\n2. Use Wikidata at scale. The API is available and the interface is multilingual.\nThis requires caution, however. Barrett Golding can attest. When the pandemic hit last year, Golding—a former NPR producer and freelance data journalist—launched Iffy.news. Designed for researchers and journalists, the site contains indexes and lists on sources of mis/disinformation. \nGolding uses large-scale data harvesting to showcase whether or not a website has a reputation for fact-checking, based on credibility rankings from databases such as Media Bias/Fact Check. “If we’re going to call other sites untrustworthy, we can’t just say “trust us” as the reason why. So each Iffy site links to the failed fact-checks that make that site unreliable,” Golding explained. \nMore recently, he began accessing information from Wikidata and Wikipedia to cross-check the reliability of websites and online sources, thanks to a grant from WikiCred. (For disclosure, I am also working on a project on reliable sources and Wikipedia funded by Wikicred). \nThat’s where things fell apart. The data were too piecemeal. \nInfowars, a well-known instance of “fake news,” is described as such in its English Wikipedia article. Wikipedia editors have also blacklisted the website from being used as a reliable source in citations, according to the hand-updated list of Perennial sources. \nBut these classifications didn’t make it to Wikidata. Infowars was just an instance of news satire and a website. (That is, until two weeks ago, when the item was edited to include as an instance of “fake news”). \nThe takeaway for data journalists? Be aware that large-scale data harvesting from Wikidata’s API can scrape out nuance at scale, rather the other way around.\nQuick links\nHow to contribute to Wikidata \nWikidata API for Python\nIffy.news \nWikipedia Perennial Sources\nLeverage Wikidata's strengths \nThere are Wikidata storytelling success stories, which often include using the dataset in conjunction with other data. Laura Jones (no relation to Trey or the author), a researcher with the Global Institute for Women’s Leadership at King's College London authored a report that shows how women—journalists and experts—have been involved in coronavirus media coverage. \nTo find out, the study used Wikidata and Wikipedia’s API to identify the gender and occupation of 54,636 unique people who had been mentioned in a vat of news content sourced from Event Registry's API, an AI-driven media intelligence platform, during the 2020 pandemic. \nThanks to the information stored in Wikidata, Jones was able to identify most of the unique people mentioned in the news coverage – experts and journalists. The majority of media coverage about the pandemic was written by male journalists, while one out of five expert voices who were interviewed about the pandemic were female, Jones concluded.\nScience Stories.io uses Wikidata and other linked data projects to visualise stories about women in science and academia. By aggregating images, structured data, and prose at scale, Science Stories.io generates hundreds of multimedia biographical portraits of historical and contemporary notable women. \nWhile Scholia pulls data from Wikidata to create visual profiles on items including chemicals, species, and people.\nQuick links\nCovid Media Analysis Report\nScience Stories.io\nScholia\n3. Discover relationships through Wikidata's query service\nGet a sense of what’s in Wikidata, and how this may aid your data storytelling, through querying. The Wikidata query service is free and available online. You’ll need to use SPARQL, a variation of SQL, the relational database management system. \nWhether you are already familiar with SPARQL or just getting started, there are an abundance of tutorials and training videos to learn. The query service also has examples. Run an example yourself for fun by pressing the blue “play” button. There are also volunteer users who are willing to run queries for you.\nWikidata query service includes example queries.\nYou can modify examples or write a query from scratch. Query results can be visualised, shared, downloaded, or embedded. It's worth running a query before you use the API or download a data dump. \nWhen it comes to an effort like Golding’s project on “fake news” websites, the query could be the first red flag that the data just isn’t there. For instance, a query for instances of “fake news” websites in Wikidata reveals less than a dozen. \nTry it—and keep in mind the results from your query will be as of the date you run the query, not as of mine, nor as of Goldings. Right now, there’s no way to share a hyperlink to a historical version of a query). Part of the problem, as Golding found, are idiosyncratic classifications. Some items are instances of websites, others are online newspapers. \nAnother query example is a “Timeline of Death by Burning.” (Try it). I modified the query by substituting the cause of death (P509) from burning (Q468455) to decapitation (Q204933). (Try it). Both rendered grisly timelines showcasing a long history of these particular forms of death. \nMy next modification reveals a limitation of the dataset. I wanted to create a timeline of women who are murdered in gender-based violence. This has a name, femicide (Q1342425). Again, grisly, I know—but also important. I expected the number might be lower than reality. But I was not prepared for a timeline of one. One femicide. (Try it). \nLiam Wyatt, who manages the Wikicite programme for the Wikimedia Foundation, said this is a typical pitfall. “You have to caveat any query result with 'as far as Wikidata knows,'” he explained in a phone interview. \nFor my query, it’s possible there are other femicides documented in Wikidata. Categorised as instances of homicide, or domestic violence. But there is invisibility yet. For instance, the murder of Pınar Gültekin by her ex-boyfriend last year made headlines around the world. Women took to the streets in Turkey to protest. \nAnd there was a much-debated social media hashtag campaign, #challengeaccepted, to raise awareness about femicide. \nWhile there is a Wikipedia article in English about the murder, and a Wikidata item for the event, Gültekin—not to mention the manner of her death—is not included as a human in Wikidata.\nQuick links\nWikidata Query Service Tutorial\nGentle Introduction to the Wikidata Query Service\nRequest a Query\n\"Wikidata is now powerful and important, but still esoteric and incomplete,” said Wyatt, on the ambiguities of the current state of Wikidata. “It’s a bit of a wild west. Journalists who can get in on the ground floor, on this wave while it’s still picking up speed, they will really be in a position to ride the momentum.” \nA promising project, when we remember that promise, according to Wikidata, is also known as liability. That is, to be “held morally or legally responsible for action or inaction.” Use it freely and be mindful to not substitute this dataset for news judgement. \nAs Last Moya writes in “Data Journalism in the Global South”, data can aid journalism in speaking truth to power provided “journalistic agency and not data is King.”\nMonika Sengul-Jones, PhD, is a freelance researcher, writer and expert on digital cultures and media industries. She was the OCLC Wikipedian-in-Residence in 2018-19. In 2020, she is co-leading Reading Together: Reliable Sources and Multilingual Communities, an Art+Feminism project on reliable sources and marginalised communities funded by WikiCred. @monikajones, www.monikasjones.com\nThanks to Molly Brind'Amour (University of Virginia), Will Kent (Wiki Education Foundation), Lane Rasberry (University of Virginia), and Houcemeddine Turki (Wikimedia Tunisia) for speaking with me for background research for this story.\nData visualisation by hand: drawing data for your next story\nA data journalists guide to building a hypothesis\nMaking numbers louder: telling stories with sound\nConflict reporting with data\nHarnessing Wikipedia's superpowers for journalists","guid":"https://datajournalism.com/read/longreads/the-promise-of-wikidata","isoDate":"2021-02-10T08:05:00.000Z","blogTitle":"DataJournalism.com"}},"__N_SSG":true},"page":"/[...slug]","query":{"slug":["the-promise-of-wikidata"]},"buildId":"0XYe7aucOTe3b0iK50JEi","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>